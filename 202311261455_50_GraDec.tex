% 梯度下降
% keys 梯度 下降 优化 更新
% license Xiao
% type Tutor

\textbf{梯度下降}（Gradient decent），或称\textbf{最速下降}，是一种一阶优化算法，是神经网络中最基本最常用的优化方法。

梯度下降法搜索可能的权值假设空间，从而找到能够拟合训练样本的最佳权值 [1]。梯度下降法为误差反向传播算法提供了基础。而后者是多层神经网络训练的基础算法。

假设有一个没有阈值的线性神经元，其输入向量为$\bvec x$，输出为$y_o$，权值向量为$\bvec w$。正如基本的感知机训练算法一样，为了能够实现权值更新，要定义一个衡量神经元输出值与样本实际值之间误差，即训练误差（Training error）。通常可以采用最小二乘：
\begin{equation}
E=\frac{1}{2}(y-y_o)^2=\frac{1}{2}[y-y_o(\bvec x)]^2=\frac{1}{2}[y-y_o(\bvec w \bvec x)]^2~
\end{equation}



% \subsection{程序实践}


\subsubsection{参考文献}
\begin{enumerate}
\item T. M. Mitchell, Machine learning. 1997.
\item 周志华. 机器学习[M]. 北京：清华大学出版社. 2016: 97
\item I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016: 174.
\end{enumerate}