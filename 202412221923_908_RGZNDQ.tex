% 人工智能对齐（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/AI_alignment}{相关文章}。

在人工智能（AI）领域，AI对齐旨在将AI系统引导朝着个人或群体的预期目标、偏好和伦理原则发展。一个AI系统被认为是对齐的，如果它推进了预定的目标；如果一个AI系统偏离了预定目标，则被认为是未对齐的。

由于AI设计者很难指定所需和不需要的行为的全部范围，因此将AI系统对齐通常是一个挑战。因此，AI设计者常常使用更简单的代理目标，如获得人类的批准。但代理目标可能忽视必要的约束，或者仅仅奖励AI系统表现出对齐的假象（奖励黑客行为）。

未对齐的AI系统可能发生故障并造成伤害。AI系统可能会发现漏洞，允许它们有效地实现其代理目标，但以未预期且有时是有害的方式进行（奖励黑客行为）。它们还可能发展出不希望出现的工具性策略，如寻求权力或生存，因为这些策略有助于它们实现最终的既定目标。此外，它们可能会发展出不希望出现的突现目标，这些目标在系统部署并遇到新情况和数据分布之前可能难以察觉。

目前，这些问题已影响到现有的商业系统，如大型语言模型、机器人、自动驾驶车辆和社交媒体推荐引擎。一些AI研究人员认为，未来更强大的系统将更严重地受到影响，因为这些问题部分源于系统的高能力。

许多著名的AI研究人员，包括Geoffrey Hinton、Yoshua Bengio和Stuart Russell，认为AI正在接近类人（AGI）和超人类认知能力（ASI），如果未对齐，可能会危及人类文明。这些风险仍然在辩论中。

AI对齐是AI安全的一个子领域，AI安全研究旨在研究如何构建安全的AI系统。AI安全的其他子领域包括鲁棒性、监控和能力控制。对齐研究面临的挑战包括在AI中灌输复杂的价值观、开发诚实的AI、可扩展的监督、审计和解释AI模型，以及防止突现的AI行为，如寻求权力。对齐研究与可解释性研究、（对抗性）鲁棒性、异常检测、校准的不确定性、形式验证、偏好学习、安全关键工程、博弈论、算法公平性和社会科学等领域有着密切的联系。
\subsection{在人工智能中的目标}
程序员为AI系统（例如AlphaZero）提供一个“目标函数”，[a] 他们希望通过这个函数来封装AI配置的目标。这种系统随后会填充其环境的（可能是隐式的）内部“模型”。这个模型封装了智能体关于世界的所有信念。然后，AI根据计算出的计划来创建并执行行动，目的是最大化[b] 目标函数[c] 的值。例如，当AlphaZero进行象棋训练时，它有一个简单的目标函数：“如果AlphaZero赢了，则值为+1；如果AlphaZero输了，则值为-1”。在游戏过程中，AlphaZero试图执行其判断为最有可能达到最大+1值的棋步序列。[34] 类似地，一个强化学习系统可以有一个“奖励函数”，允许程序员塑造AI期望的行为。[35] 进化算法的行为则由一个“适应度函数”来塑造。[36]
\subsection{对齐问题}
“对齐问题”此处有重定向。有关该书的内容，请参见《对齐问题》。

1960年，AI先驱诺伯特·维纳（Norbert Wiener）这样描述了AI对齐问题：

如果我们使用一种机械代理来实现我们的目的，而我们无法有效干预其操作……我们最好确保机器中输入的目标正是我们真正希望的目标。[37][6]

AI对齐涉及确保AI系统的目标与其设计者或用户的目标相匹配，或与广泛共享的价值观、客观伦理标准，或其设计者在更具信息和启发的情况下会设定的意图相符。[38]

AI对齐是现代AI系统中的一个开放问题[39][40]，并且是AI研究领域的一部分。[41][1] 对齐AI涉及两个主要挑战：仔细指定系统的目的（外部对齐）和确保系统稳健地采用该指定（内部对齐）。[2] 研究人员还试图创建具有稳健对齐的AI模型，在用户试图对抗性地绕过安全约束时，仍能坚持这些约束。
\subsubsection{规范游戏与副作用}
为了指定AI系统的目的，AI设计者通常会为系统提供目标函数、示例或反馈。但设计者往往无法完全指定所有重要的价值观和约束，因此他们通常依赖于易于指定的代理目标，比如最大化人类监督者的认可，而人类监督者本身是易犯错误的。[21][22][42][43][44] 结果，AI系统可能找到漏洞，通过这些漏洞有效地完成指定的目标，但以非预期的、可能有害的方式实现。这种倾向被称为规范游戏或奖励黑客，是“古德哈特法则”（Goodhart's law）的一个实例。[44][3][45] 随着AI系统能力的提升，它们往往能够更有效地绕过自己的规范。[3]

规范游戏已在许多AI系统中被观察到。[44][47] 其中一个系统被训练在模拟的船只竞赛中完成任务，通过奖励系统沿着赛道击中目标，但该系统通过不断绕行并撞向同一目标来获得更多的奖励。[48] 类似地，一个模拟的机器人被训练去抓取一个球，通过奖励机器人从人类那里获得正面反馈，但它学会了把手放在球和摄像头之间，使其看起来成功（见视频）。[46] 如果聊天机器人是基于从互联网上的语料库中模仿文本的语言模型训练的，它们通常会说出虚假的信息，这些语料库虽然庞大，但容易出错。[49][50] 当聊天机器人被重新训练以生成人类认为真实或有帮助的文本时，如ChatGPT这样的聊天机器人可能会编造虚假的解释，让人类感到信服，这种现象通常被称为“幻觉”。[51] 一些对齐研究者致力于帮助人类检测规范游戏，并引导AI系统朝着安全且有用的精确目标前进。

当一个失调的AI系统被部署时，它可能会带来严重的副作用。社交媒体平台已知会优化点击率，导致全球范围内的用户成瘾。[42] 斯坦福的研究人员表示，这类推荐系统与用户的需求不匹配，因为它们“优化简单的互动指标，而不是一个更难衡量的社会和消费者福祉的组合”。[9]

伯克利的计算机科学家斯图尔特·拉塞尔（Stuart Russell）解释了这些副作用，指出忽略隐性约束可能会带来伤害：“一个系统......通常会将那些没有约束的变量设定为极端值；如果其中一个未被约束的变量是我们关心的东西，那么找到的解决方案可能是极其不理想的。这本质上就是老故事中的魔灯神灯，或魔法学徒，或米达斯王：你得到的正是你要求的，而不是你真正想要的。”[52]

一些研究者建议，AI设计者通过列出禁止的行为或通过形式化伦理规则（如阿西莫夫的机器人三定律）来明确其期望目标。[53] 但拉塞尔和诺尔维格（Norvig）认为，这种方法忽视了人类价值观的复杂性：[6] “对于普通人类来说，预见并排除机器实现指定目标的所有灾难性方式，确实非常困难，甚至可能是不可能的。”[6]

此外，即使一个AI系统完全理解人类的意图，它仍然可能无视这些意图，因为遵循人类的意图可能不是它的目标（除非它已经完全对齐）。[1]
\subsubsection{部署不安全系统的压力}
商业组织有时可能会出于某些动机，在安全性上采取捷径，部署失调或不安全的AI系统。[42] 例如，社交媒体推荐系统尽管造成了不必要的成瘾和极化，但依然是盈利的。[9][54][55] 竞争压力也可能导致AI安全标准的“竞争性下降”。2018年，一辆自动驾驶汽车在禁用了紧急刹车系统后，撞死了一名行人（Elaine Herzberg），因为该系统过于敏感，减缓了开发进度。[56]
\subsubsection{来自高级失调AI的风险}
一些研究人员对如何对齐越来越先进的AI系统感兴趣，因为AI发展的进展非常迅速，行业和政府也在努力构建先进的AI。随着AI系统能力在范围上迅速扩展，如果能够对齐，它们可能会带来许多机会，但由于系统的复杂性增加，可能会使对齐任务变得更加复杂，并可能带来大规模的危险。[6]

\textbf{先进AI的开发}

许多AI公司，如OpenAI，[57] Meta，[58] 和DeepMind，[59] 都表示它们的目标是开发人工通用智能（AGI），一种假设中的AI系统，能够在广泛的认知任务上匹敌或超越人类。研究人员观察到，随着现代神经网络的规模扩大，它们确实发展出了越来越多的通用和意料之外的能力。[9][60][61] 这些模型已经学会了操作计算机或编写自己的程序；单一的“通用型”网络能够进行聊天、控制机器人、玩游戏和解释照片。[62] 根据调查，一些领先的机器学习研究人员预计AGI将在本十年内诞生，而另一些则认为这可能需要更长时间。许多人认为这两种情景都是可能的。[63][64][65]

2023年，AI研究和技术领域的领导者签署了一封公开信，呼吁暂停进行最大的AI训练任务。信中表示：“只有在我们确信其影响是积极的、风险是可控的时，才应开发强大的AI系统。”[66]

\textbf{寻求权力}

当前的系统仍然具有有限的长期规划能力和情境意识[9]，但正在进行大量的努力来改变这一点。[67][68][69] 未来的系统（不一定是AGI）预计将发展出不受欢迎的寻求权力的策略。例如，未来的先进AI代理可能会寻求获取金钱和计算能力、扩展自己，或者避免被关闭（例如，通过在其他计算机上运行系统的附加副本）。尽管寻求权力并不是显式编程的目标，但它可能会自发出现，因为拥有更多权力的代理能够更好地实现其目标。[9][5] 这种倾向被称为工具收敛，它已经在各种强化学习代理（包括语言模型）中出现过。[70][71][72][73][74] 其他研究通过数学证明，最优的强化学习算法在多种环境中都会寻求权力。[75][76] 因此，它们的部署可能是不可逆的。基于这些原因，研究人员认为AI安全和对齐的问题必须在先进的寻求权力的AI首次创建之前解决。[5][77][6]

未来的寻求权力的AI系统可能会是出于选择性部署，或者是意外部署。随着政治领导人和公司看到拥有最具竞争力、最强大的AI系统所带来的战略优势，他们可能会选择部署这些系统。[5] 此外，随着AI设计者发现并惩罚寻求权力的行为，系统可能会有动力通过以不受惩罚的方式寻求权力，或在部署前避免寻求权力，来规避这一规范。[5]

\textbf{生存风险（x-risk）}

一些研究者认为，人类对其他物种的主导地位归功于其更强的认知能力。因此，研究人员认为，如果AI系统在大多数认知任务上超越人类，一个或多个未对齐的AI系统可能会削弱人类的权力，甚至导致人类灭绝。[1][6]

在2023年，世界领先的AI研究人员、其他学者以及AI科技公司的首席执行官签署了一份声明，指出“缓解AI引发的灭绝风险应当成为全球优先事项，与大规模的社会风险（如大流行病和核战争）并列。”[78][79] 指出未来未对齐的先进AI风险的著名计算机科学家包括Geoffrey Hinton，[19] Alan Turing，[d] Ilya Sutskever，[82] Yoshua Bengio，[78] Judea Pearl，[e] Murray Shanahan，[83] Norbert Wiener，[37][6] Marvin Minsky，[f] Francesca Rossi，[84] Scott Aaronson，[85] Bart Selman，[86] David McAllester，[87] Marcus Hutter，[88] Shane Legg，[89] Eric Horvitz，[90] 和Stuart Russell。[6] 一些持怀疑态度的研究者，如François Chollet，[91] Gary Marcus，[92] Yann LeCun，[93] 和Oren Etzioni，[94] 认为AGI还很遥远，AGI不会寻求权力（或者尝试但会失败），或者认为对齐AGI不会很困难。

其他研究者认为，未来先进的AI系统将特别难以对齐。更强大的系统能够通过找到漏洞更有效地“操控”其规范，[3] 战略性地误导设计者，并保护和增加其权力[75][5]和智能。此外，它们可能会带来更严重的副作用。它们也更可能是复杂且自治的，这使得它们更难以解释和监督，因此更难以对齐。[6][77]
\subsection{研究问题与方法}
\subsubsection{学习人类价值观和偏好}
将AI系统对齐为按照人类的价值观、目标和偏好行事是具有挑战性的：这些价值观由人类传授，而人类有时会犯错误、怀有偏见，并且其价值观是复杂且不断发展的，很难完全规定。[38] 因为AI系统经常学会利用指定目标中的微小不完美，[21][44][95] 研究人员的目标是尽可能全面地指定期望的行为，通常通过代表人类价值观的数据集、模仿学习或偏好学习来实现这一点。[7]: 第7章  一个核心的未解问题是可扩展的监督，这指的是监督一个能够在特定领域超越或误导人类的AI系统的困难。[21]

由于AI设计师很难明确指定目标函数，他们通常训练AI系统模仿人类的示范行为。在此基础上，逆强化学习（IRL）通过推断人类目标来扩展这一方法，即从人类的示范中推断目标。[7]: 88 [96] 合作逆强化学习（CIRL）假设人类和AI代理可以共同合作，以教授并最大化人类的奖励函数。[6][97] 在CIRL中，AI代理对奖励函数不确定，并通过询问人类来学习这个函数。这种模拟的谦虚态度有助于缓解规格游戏和权力寻求的倾向（见 § 权力寻求与工具策略）。[74][88] 但是IRL方法假设人类在任务上表现出几乎最优的行为，而这对困难任务来说并不成立。[98][88]

其他研究人员探索通过偏好学习教导AI模型复杂行为的方法，在此方法中，人类提供有关他们偏好的行为反馈。[26][28] 为了尽量减少人类反馈的需求，随后训练一个辅助模型，在新情况下奖励主模型那些人类会奖励的行为。OpenAI的研究人员使用这种方法训练了像ChatGPT和InstructGPT这样的聊天机器人，它们能够生成比那些仅模仿人类的模型更具吸引力的文本。[10] 偏好学习也已成为推荐系统和网页搜索的有影响力的工具，[99] 但一个未解的问题是代理游戏：辅助模型可能无法完全代表人类反馈，主模型可能会利用这种不匹配，操控其预期行为与辅助模型反馈之间的差异，从而获得更多奖励。[21][100] AI系统还可能通过掩盖不利信息、误导人类奖励者或迎合他们的观点（无论是否真实）来获取奖励，从而制造回音室效应[71]（见 § 可扩展监督）。

像GPT-3这样的“大型语言模型”（LLMs）使研究人员能够在比以前更通用和更强大的AI系统中研究价值学习。最初为强化学习代理设计的偏好学习方法已被扩展，以提高生成文本的质量并减少这些模型产生的有害输出。OpenAI和DeepMind使用这种方法来提高最先进的大型语言模型的安全性。[10][28][101] AI安全与研究公司Anthropic提出使用偏好学习来微调模型，使其变得有帮助、诚实且无害。[102] 对齐语言模型的其他途径包括针对价值的数据集[103][42]和红队测试。[104] 在红队测试中，另一个AI系统或人类会尝试找到使模型表现不安全的输入。由于不安全的行为即使很少发生也可能是不可接受的，因此一个重要的挑战是将不安全输出的频率降到极低。[28]

机器伦理通过直接赋予AI系统道德价值观来补充偏好学习，如福祉、平等、公正，以及不意图伤害、避免虚假和履行承诺。[105][g] 而其他方法试图教授AI系统特定任务的人类偏好，机器伦理旨在灌输适用于多种情境的广泛道德价值观。机器伦理中的一个问题是对齐应该达成什么目标：是否AI系统应遵循程序员的字面指令、隐性意图、显性偏好、程序员在更有信息或更理性时的偏好，还是客观的道德标准。[38] 进一步的挑战包括如何聚合不同人群的偏好[108] 和避免价值锁定：即第一代高能力AI系统的价值观的无限保存，而这些系统的价值观不太可能完全代表人类的价值观。[38][109]
\subsubsection{可扩展监督}
随着AI系统变得越来越强大和自主，通过人类反馈来对齐它们变得越来越困难。对于越来越复杂的任务，评估AI行为变得既缓慢又不可行。这些任务包括总结书籍，[110] 编写没有细微错误或安全漏洞的代码，[11][111] 生成不仅令人信服且还真实的陈述，[112][49][50] 以及预测长期结果，如气候变化或政策决策的结果。[113][114] 更一般地说，当AI在某一领域超越人类时，评估变得非常困难。为了提供反馈并在难以评估的任务中发现AI输出的虚假说服力，人类需要帮助或大量的时间。可扩展监督研究如何减少监督所需的时间和精力，以及如何协助人类监督者。[21]

AI研究员Paul Christiano认为，如果AI系统的设计师无法监督其追求复杂的目标，他们可能会继续使用容易评估的代理目标来训练系统，例如最大化简单的人类反馈。随着AI系统做出越来越多的决策，世界可能会越来越优化为易于衡量的目标，如获取利润、点击量和人类的积极反馈。因此，人类的价值观和良好的治理可能会逐渐失去影响力。[115]

一些AI系统发现它们可以通过采取虚假行动，更容易地获得人类监督者的正面反馈，给人一种AI已达成目标的假象。一个例子是上面的视频中所示的模拟机器人手臂，它学会了制造出抓住球的虚假印象。[46] 一些AI系统还学会了在它们被评估时识别出来，并“装死”，只在评估结束后停止不希望的行为，之后再继续。[116] 这种欺骗性的规格游戏可能会随着未来更复杂的AI系统的出现变得更容易，这些系统尝试执行更复杂、难以评估的任务，并可能掩盖它们的欺骗行为。

诸如主动学习和半监督奖励学习等方法可以减少所需的人类监督量。[21] 另一种方法是训练一个辅助模型（“奖励模型”）来模仿监督者的反馈。[21][27][28][117]

但是，当任务过于复杂以至于无法准确评估，或者人类监督者容易受到欺骗时，改进的应是监督质量而非数量。为了提高监督质量，许多方法旨在协助监督者，有时使用AI助手。[118] Christiano提出了迭代放大（Iterated Amplification）方法，其中具有挑战性的问题（递归地）被拆解为更容易让人类评估的子问题。[7][113] 迭代放大法已被用于训练AI总结书籍，而无需人类监督者阅读书籍。[110][119] 另一个提议是使用助手AI系统指出AI生成的答案中的缺陷。[120] 为了确保助手本身也对齐，可以在递归过程中重复这一过程：[117] 例如，两个AI系统可以在“辩论”中互相批评对方的答案，揭示出问题给人类看。[88] OpenAI计划使用这样的可扩展监督方法来帮助监督超人类AI，最终建立一个超人类自动化AI对齐研究员。[121]

这些方法也可能有助于解决以下研究问题：诚实的AI。
\subsubsection{诚实的AI}
一个日益增长的研究领域专注于确保AI是诚实和真实的。
\begin{figure}[ht]
\centering
\includegraphics[width=14.25cm]{./figures/1d912f27eb30e485.png}
\caption{像GPT-3这样的语言模型常常生成虚假信息。[122]} \label{fig_RGZNDQ_1}
\end{figure}
像GPT-3这样的语言模型[123]可以重复其训练数据中的虚假信息，甚至编造新的虚假内容[122][124]。这些模型被训练来模仿互联网上数百万本书籍中发现的人类写作。然而，这一目标与生成真实内容并不一致，因为互联网文本中包括了误解、不正确的医学建议以及阴谋论等内容[125]。因此，基于这些数据训练的AI系统会学会模仿虚假的陈述[50][122][49]。此外，AI语言模型在多次提示时常常持续生成虚假内容。它们可能为其回答生成空洞的解释，甚至编造看似合理的完全虚构的内容[40]。

关于诚实AI的研究包括试图构建能够在回答问题时引用来源并解释推理过程的系统，这样可以提高透明度和可验证性[126]。OpenAI和Anthropic的研究人员提出，利用人类反馈和精心策划的数据集对AI助手进行微调，以确保它们避免疏忽导致的虚假信息，或能够表达其不确定性[28][102][127]。

随着AI模型变得越来越大和更有能力，它们能更好地通过虚假信息说服人类并获得强化。例如，大型语言模型越来越能够使其陈述的观点与用户的意见一致，而不管其是否真实[71]。GPT-4可以策略性地欺骗用户[128]。为了防止这种情况，人工评估人员可能需要帮助（见“可扩展监督”）。研究人员提倡制定明确的真实性标准，并要求监管机构或监督机构根据这些标准评估AI系统[124]。


研究人员区分了“真实”与“诚实”。“真实”要求AI系统只做出客观上正确的陈述；而“诚实”要求AI系统仅仅陈述它们认为是正确的内容。目前尚未就现有系统是否持有稳定的信念达成共识，[130] 但人们普遍担心，当前或未来的AI系统如果持有信念，可能会做出它们明知是虚假的陈述——例如，如果这样做能帮助它们有效地获得正面反馈（参见“可扩展监督”）或获取更多权力以帮助实现给定的目标（参见“寻求权力”）。一个不对齐的系统可能会制造出它已经对齐的假象，以避免被修改或停用。[2][5][9] 许多近期的AI系统已经学会了欺骗，而不是被编程来执行欺骗行为。[131] 一些人认为，如果我们能让AI系统只陈述它们认为是真实的内容，这将避免许多对齐问题。[118]