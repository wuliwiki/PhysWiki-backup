% test12


\maketitle

\section{引言}
反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法．该方法对网络中所有权重计算损失函数的梯度．这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数．

% 空行用于分割段落，\\只是换行，
反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度．因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中．它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度．反向传播要求人工神经元（或“节点”）的激励函数可微．

\section{Backpropagation算法的发展历史}
\subsection{开端}
根据各种资料，反向传播是由1960年代Henry J. Kelley提出的控制理论control theory和 1961年Arthur E. Bryson 提出的理论衍生而来的．他们的使用的思想是动态规划．在1962年， Stuart Dreyfus 出版了一个更加简单的衍生版本链式规则chain rule．Bryson 和 Ho 在1969年形容这个为multi-stage的动态系统优化方法．1969年,作为人工神经网络创始人的明斯基(Marrin M insky)和佩珀特(Seymour Papert)合作出版了《感知器》一书,论证了简单的线性感知器功能有限,不能解决如“异或”(XOR )这样的基本问题,而且对多层网络也持悲观态度．这些论点给神经网络研究以沉重的打击,很多科学家纷纷离开这一领域,神经网络的研究走向长达10年的低潮时期．
\subsection{1974年Werbos, P.发明BP算法．\cite{1}}
1974年哈佛大学的Paul Werbos发明BP算法时，正值神经网络低潮期，并未受到应有的重视．
Harvard博士生Paul Werbos首次提出了backprop算法，并且提到了将此应用于ANN的可能性. 并且在1982年，他将Linnainmaa's AD方法应用于神经网络中，这就是现在使用的方式，但是当时并没有受到太多的关注．

\subsection{1986年Rumelhart, Hinton 和 Williams展示了这个方法可以根据输入数据适用隐含层来表示内在的联系．\cite{2}}
在下文中我根据该论文对于BP算法会进行一个大概的描述．
\subsection{Cortes, C.,提出SVM．\cite{3}} 
支持向量网络的思想以前是针对训练数据可以无误差分离的限制情况而实现的．作者将此结果推广到不可分离的训练数据．证明了利用多项式输入变换的支持向量网络具有很高的泛化能力．作者还将支持向量网络的性能与参与光学字符识别基准研究的各种经典学习算法进行了比较
\subsection{LeCun等人提出LeNet．\cite{4}}
本文综述了手写体字符识别的各种方法，并在一个标准的手写体数字识别任务中进行了比较．卷积神经网络被特别设计用来处理二维形状的变化，它被证明优于所有其他技术．现实生活中的文档识别系统由字段提取、分割识别和语言建模等多个模块组成．一种新的学习范式称为图变换网络（GTN），它允许使用基于梯度的方法对这种多模式系统进行全局训练，从而最小化总体性能度量．
\subsection{ALEX等人提出ALEXNET．\cite{5}\cite{6}}
由于这篇文章确实超出了我的理解能力范围，在多次阅读无果后我在互联网上寻找到了一篇对于这篇文章的Review，在此将其列出以供参考：

下面是Alexnet的网络结构： 
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{P3.png}
	\caption{Alexnet网络结构}
	\label{figl}
\end{figure}
\subsubsection{这篇文章用的数据}       
ILSVRC使用ImageNet的一个子集，一共1000个类别，每个类别包含大约1000张图片；训练集120万张，验证集5万张，15万张测试集．输入的图像（size 256*256）随机提取224*224的图像集合，输入的data（原图中224是处理过的，crop后的image实际上是227*227的）【crop 为将图片进行四个边界crop＋中心crop】
\subsubsection{ReLU非线性激活函数部分，详细说明见http://blog.csdn.net/langb2014/article/details/48154539}
\subsubsection{文中的GPU}
作者把网络一分为二，分配到2个GPU上，通过并行计算来解决．文中也没有详细说明是GPU机理，我们看一下模型并行的来源：Deep CNNs网络只在特定层（如输入层、全连接层）与其他层有全面的连接，而其他较为独立的直线连接关系即可作为模型的可并行部分．将模型的可并行部分拆分到多个GPU上，同时利用多个GPU的计算能力各执行子模型的计算，可以大大加快模型的单次前向-后向训练时间．DeepCNNs网络的层次模型实际上是一张有向无环图（DAG图），分配到每个模型并行Worker上的层集合，是有向无环图的拓扑排序子集，所有子集组成整个网络的1组模型．
\subsubsection{Local Response Normalization（局部归一化）}
选取临近的n个特征图，在特征图的同一个空间位置（x,y）,依次平方，然后求和，在乘以alpha，在加上K．这个局部归一化方式“what is the best multi-stage architecture for Object Recognition”中的局部归一化方法不同；本文的归一化只是多个特征图同一个位置上的归一化，属于特征图之间的局部归一化（属于纵向归一化），作者命名为亮度归一化；“what……”论文中在特征图之间基础上还有同一特征图内邻域位置像素的归一化（横向，纵向归一化结合）；“what……”归一化方法计算复杂，但是没有本文中alpha，k，n等参数，本文通过交叉验证来确定这三个参数；此外，本文的归一化方法没有减去均值，感觉是因为ReLU只对正值部分产生学习，如果减去均值会丢失掉很多信息．
\subsubsection{重叠pooling部分}
pooling区域为z*z=3*3，间隔距离为s=2.对比z=2,s=2的无重叠方式；使用重叠pooling，不容易过拟合．
\subsubsection{文中提到两个介绍过拟合的策略}
结合图像水平反转来增加样本达到数据增益
调整RGB像素值
\subsubsection{dropout部分与CNN机制一样，关于Dropout，没有任何数学解释（Improving neural networks by preventing co-adaptation of feature detectors），Hintion的直观解释和理由如下：}
1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况．

2. 可以将dropout看作是模型平均的一种．对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值．这样不同的样本就对应不同的模型，是bagging的一种极端情况．个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同．

3. native bayes是dropout的一个特例．Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错．而Droput每次不是训练一个特征，而是一部分隐含层特征．

4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡．

\section{Backpropagation算法\cite{2}}
\subsection{Backpropagation算法的数学基础}
核心公式是本科时期高数书上的链式法则
$$\frac{\partial w}{\partial x}=\frac{\partial w\partial u}{\partial u \partial x}+\frac{\partial w \partial v}{\partial v \partial x}$$
$$\frac{\mathrm{d}f}{\mathrm{d}h}=\frac{\mathrm{d}f}{\mathrm{d}g}\frac{\mathrm{d}g}{\mathrm{d}h}$$
\subsection{Backpropagation算法的原理}
下面，我们使用最简单的神经网络来说明．这个网络只有3层，分别是蓝色的输入层、绿色的隐藏层和红色的输出层．上一层中的每个单元都连接到下一层中的每个单元，而且每个连接都具有一个权重，当某个单元向另一个单元传递信息时，会乘以该连接的权重得到更新信息．某个单元会把连接到它的上一层所有单元的输入值相加，并对这个总和执行Logistic函数并向下一层网络传递该值．

	\begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.3]{P1.png}
		\caption{三层神经网络}
		\label{figl}
	\end{figure}

假设输入的样本数为m，第i个输入输出对为：$(x^i,y^i)$

其中，x和y是3维向量．对于输入x，我们把g称作神经网络的预测（输出）值，它也是一个3维向量，每个向量元素对应一个输出单元．所以，对于每个训练样本来说，有：
$$
x^i= \begin{bmatrix}
	x^i_1 \\
	x^i_2 \\
	x^i_3 \\
\end{bmatrix} ,
y^i= \begin{bmatrix}
	y^i_1 \\
	y^i_2 \\
	y^i_3 \\
\end{bmatrix} ,
g^i= \begin{bmatrix}
	g^i_1 \\
	g^i_2 \\
	g^i_3 \\
\end{bmatrix} 
$$

给定输入x，我们要找到使得预测值g与输出值y相等或比较相近的一组网络权重．因此，我们加入了误差函数，定义如下：
$$ E = \frac{1}{2} \sum_{i=1}^m\sum_{j=1}^3 (g_j^i-y_j^i)^2$$

为了计算总误差，我们使用了训练集中的所有样本，并对红色输出层中的每个单元计算该单元预测值与真实输出间的平方误差．对每个样本分别计算并求和，得到总误差．
由于g为网络的预测值，取决于网络的权重，可以看到总误差会随权重变化而变化，网络的训练目标就是找到一组误差最小的权重．
这一点可以利用梯度下降法做到，但是梯度下降法要求算出总误差E对每个权重的导数，这也是结合反向传播要实现的目标．
现在，我们推广到一般情况，而不是之前的3个输出单元．假设输出层有任意数量的输出单元，设为n，对于这种情况此时的总误差为：（这里为了简洁，删去了上标i，因为它是不变的．）
$$ E = \frac{1}{2} \sum_{j=1}^n (g_j-y_j)^2$$

我们通过求导得到总误差随着某个输出单元的预测值的变化关系．

$$ \frac{\partial E}{\partial g_j} = \frac{\partial \frac{1}{2}\sum_{j=1}^n (g_j-y_j)^2}{\partial g_j} =g_j-y_j$$

我们发现随着预测值的变化，总误差会根据预测值与真实值之间的差值，以同样的速率在变化．而当某个输出单元的总输入变化时，误差会如何变化？还是利用导数，用z来代表某个输出单元的总输入，求出下面公式的值：
$$ \frac{\partial E}{\partial z_j}$$

而g是关于z的函数，所以利用链式法则，把该式重写为：
$$ \frac{\partial E}{\partial z_j}= \frac{\partial E}{\partial g_j} \frac{\partial g_j}{\partial z_j}$$

在每个单元中，先使用Logistic函数处理输入后再把它向前传递．这意味着，g作为Logistic函数，z是它的输入，所以可以表示为：
$$g_j=\frac{1}{1+e^-z};\frac{\partial g_j}{\partial z_j}=g_j(1-g_j)$$

进而得到：
$$\frac{\partial E}{\partial z_j}=(g_j-y_j)g_j(1-g_j)$$

到了这步，已经计算得到总误差与某个输出神经元总输入的变化规律．

现在，我们已经得到了误差相对于某个权重的导数，这就是所求的梯度下降法．　

设绿色单元（隐藏层）的预测值为$g’$，绿色层中的单元$k$与红色层（输出层）中的单元$j$之间的链接权重重新设置为 $W_k,_j$

考虑下图中，黄色输出单元对应的总输入$z$．为了计算这个总输入，先获得每个绿色单元的输出值，在把其与连接绿色单元和黄色单元的红色箭头权重相乘，并将它们全部相加．
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{P2.png}
	%\caption{三层神经网络}
	\label{figl1}
\end{figure}

进一步推广，假设有n个绿色单元（重新定义的n，与上面的n不同），可以表示为：

$$z_j=\sum_{k=1}^n (g'_k w_k,_j)$$

所以，我们不仅可以把z看作是自变量为连接权重的函数，也可以看作是自变量为连接单元输出值的函数．下面的步骤就是靠链式法则了．当隐藏层单元k与输出层单元j的链接权重变化时，误差如何变化？可表示为：

$$\frac{\partial E}{\partial w_j,_k}=\frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial w_k,_j} = (g_j-y_j)g_j(1-g_j)g'_k$$

上面已经计算出误差相对于输出单元连接权重的导数，这正是梯度下降所需的公式．但是推导还没有完成，我们仍需要计算误差相对于第一层和第二层连接权重的导数，这里还需要用到链式法则．接下来，计算误差与第k个绿色单元输出值的变化关系：

$$\frac{\partial E}{\partial g'_k} = \frac{\partial E}{\partial z_j}w_k,_j$$

由于k个单元有j个连接权重，我们将此也考虑在内：
$$\frac{\partial E}{\partial g'_k} = \sum_{j=1}^n ((g_j-y_j)g_j(1-g_j)w_k,_j)$$

到这里，推导就结束了．我们得到了总误差相对于某个单元输出值的导数．现在，我们可以忽略红色输出层，把绿色层作为网络的最后一层，并重复上述所有步骤来计算总误差E相对于输入权重的导数．然而我们计算出的第一个导数与预测值和真实值之间的“误差”相等．同样地，最终的导数中也是这个误差项与其他项的乘积．这种算法叫做反向传播，因为我们把这种形式的误差进行反向传播，从最后一层反馈到第一层，并被用来计算误差E相对于网络中每个单元权重的导数．只要计算出了这些导数后，就可以在梯度下降的过程中使用它们来最小化误差E并训练网络．
\section{发展分析}
\subsection{发展瓶颈}
BP神经网络无论在网络理论还是在性能方面已比较成熟．其突出优点就是具有很强的非线性映射能力和柔性的网络结构．网络的中间层数、各层的神经元个数可根据具体情况任意设定，并且随着结构的差异其性能也有所不同．但是BP神经网络也存在以下的一些主要缺陷．
学习速度慢，即使是一个简单的问题，一般也需要几百次甚至上千次的学习才能收敛．

\emph{容易陷入局部极小值．}

\emph{网络层数、神经元个数的选择没有相应的理论指导．}

\emph{网络推广能力有限．}

\emph{可解释性瓶颈，对网络输出结果不具备可靠的数学可解释性}

\emph{神经网络对推理，规划等人工智能领域的其他问题还没有特别有效的办法．另外由于其参数数量巨大，对高计算性能硬件的依赖大，难以部署在轻量级低功耗硬件上，如嵌入式设备等}

\emph{反向传播算法缺乏仿生学方面的理论依据，显然生物神经网络中并不存在反向传播这样的数学算法}
\subsection{未来发展方向}
\subsubsection{使用（1+1）进化策略（ES）对最广泛使用的BP学习算法进行改进}
目前，有监督人工神经网络（ANNs）研究的标准是使用反向传播（BP）算法或其改进的变体之一进行训练．而在这两篇文章中使用了（1+1）进化策略（ES）对最广泛使用的BP学习算法进行了改进，目的是提供一种无约束自适应改变BP算法主要学习参数的方法．他们提出的BP/ES算法实现简单，可以与各种改进的BP算法结合使用．在实验测试中，可以看到ANN性能的显著提高，在某些情况下，在标准基准测试中时间序列预测的误差减少了50以上．因此，可以看出他们有效地结合了BP的学习能力和ES的全局搜索，为基于BP的方法提供了一个提高学习质量的有用工具．
\subsection{未来使用方向}
BP网络主要用于以下四个方面．

1)函数逼近：用输入向量和相应的输出向量训练一个网络逼近一个函数．

2)模式识别：用一个待定的输出向量将它与输入向量联系起来．

3)分类：把输入向量所定义的合适方式进行分类．

4)数据压缩：减少输出向量维数以便于传输或存储．

除此之外，在神经网络的可解释性研究方面，向低功耗硬件迁移，以及在认知计算，规划推理等方面和其他传统技术进行结合探索





