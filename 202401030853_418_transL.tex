% Transformers 库
% keys Python|机器学习|库
% license Usr
% type Note

\begin{issues}
\issueDraft
\end{issues}

transformers 库是要使用 \href{https://huggingface.co}{HuggingFace 网站}的已经训练好的 torch 网络参数或 tensorflow 网络参数必备的一个包（现在访问 huggingface 网站需要科学上网）。

安装可以使用 pip 命令直接安装：\verb`pip install transformers`，使用 huggingface 网站提供的数据集或神经网络参数需要保证电脑里已经装有 PyTorch。（CPU 版的 torch 可以使用命令 \verb`pip install torch` 安装。安装包较大，建议使用国内 pypi 的镜像站。）

特别的，有的网络参数（.h5 文件）则是 tensorflow 的，那么需要保证电脑已经装有 tensorflow 包。

对于网站上的每个网络参数，介绍一些常见的文件：
\begin{enumerate}
\item \verb`readme.md` 一类文件：介绍文档。
\item \verb`license`： 这文件存储开源协议。
\item \verb`config.json`：这个文件存储了网络的一些配置。
\item 分词配置：\verb`tokenizer.json`、\verb`tokenizer_config.json`、\verb`vocab`，其中 \verb`vocab` 文件可能是 txt 或 json 格式。这些文件给分词器使用。
\item \verb`pytorch_model.bin`：存储了 PyTorch 网络的训练好的参数。
\item \verb`tf_model.h5`：存储了 tensorflow 网络的训练好的参数。
\end{enumerate}

对于其中的 \verb`config.json`，我们可以通过其看出这个网络的一些配置，一个典型的配置文件以及各配置意义如下：
\begin{lstlisting}[language=json]
{
  "architectures": [
    "BertForSequenceClassification"# 模型的名称
  ],
  "attention_probs_dropout_prob": 0.1,# 注意力机制的 dropout，默认为 0.1
  "directionality": "bidi",# 文字编码方向采用 bidi 算法
  "hidden_act": "gelu",# 编码器内激活函数，默认"gelu"，还可为"relu"、"swish"或 "gelu_new"
  "hidden_dropout_prob": 0.1,# 词嵌入层或编码器的 dropout，默认为 0.1
  "hidden_size": 768,# 编码器内隐藏层神经元数量，默认 768
  "initializer_range": 0.02,# 神经元权重的标准差，默认为 0.02
  "intermediate_size": 3072,# 编码器内全连接层的输入维度，默认 3072
  "layer_norm_eps": 1e-12,# layer normalization 的 epsilon 值，默认为 1e-12
  "max_position_embeddings": 512,# 模型使用的最大序列长度，默认为 512
  "model_type": "bert",# 模型类型是 bert
  "num_attention_heads": 12,# 编码器内注意力头数，默认 12
  "num_hidden_layers": 12,# 编码器内隐藏层层数，默认 12
  "pad_token_id": 0,# pad_token_id 未找到相关解释
  "pooler_fc_size": 768,# 下面是 pooler 层的参数，本质是个全连接层，作为分类器解决序列级的NLP任务
  "pooler_num_attention_heads": 12,# pooler 层注意力头，默认12
  "pooler_num_fc_layers": 3,# pooler 连接层数，默认 3
  "pooler_size_per_head": 128,# 每个注意力头的 size
  "pooler_type": "first_token_transform",# pooler 层类型，网上介绍很少
  "type_vocab_size": 2,# 词汇表类别，默认为 2
  "vocab_size": 21128 // 词汇数，bert默认 30522，这是因为bert以中文字为单位进入输入
}
\end{lstlisting}