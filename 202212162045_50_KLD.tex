% KL散度（相对熵）
% keys Kullback–Leibler divergence 熵 统计距离


\textbf{$KL$散度}（Kullback–Leibler divergence， 缩写KLD）是一种统计学度量，表示的是一个概率分布相对于另一个概率分布的差异程度，在信息论中又称为\textbf{相对熵}．

设离散概率空间$X$上有两个概率分布$P$和$Q$，那么$P$相对于$Q$的$KL$散度定义如下：
\begin{equation}
D_{KL}(P||Q)=\sum_{x\in X}P(x)ln(\frac{P(x)}{Q(x)})=\sum_{x\in X}P(x)(ln(P(x))-ln(Q(x)))
\end{equation}

对于连续性随机变量，设概率空间$X$上有两个概率分布$P$和$Q$，其概率密度分别为$p$和$q$，那么，$P$相对于$Q$的$KL$散度定义如下：
\begin{equation}
D_{KL}(P||Q)=\int_{-\infty}^{+\infty}p(x)ln(\frac{p(x)}{q(x)})dx
\end{equation}


参考文献：
\begin{enumerate}
\item https://en.wikipedia.org/wiki/Kullback-Leibler_divergence
\end{enumerate}