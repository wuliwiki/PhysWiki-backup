% 最大期望算法
% license CCBYSA3
% type Wiki

（本文根据 CC-BY-SA 协议转载自原搜狗科学百科对英文维基百科的翻译）

在统计学中，期望最大化算法(EM)是一种迭代方法，用于寻找统计模型中参数的最大似然或最大后验估计，其中模型依赖于未观察到的潜变量。EM迭代在执行求期望(E)步骤和最大化(M)步骤之间交替进行，求期望(E)步骤创建了一个函数，是用参数当前估计值所估计得到的对数似然的期望的函数，最大化(M)步骤计算参数，使其能够最大化在E步骤中计算得到的对数似然的期望。然后这些参数估计值被用于确定下一个E步骤中的潜变量分布。

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/66edbd8e9bf77549.png}
\caption{Old Faithful eruption数据集(R语言中典型的数据集)上的EM聚类过程。随机初始化的模型(由于轴之间的规模不同，显示的是两个非常扁平又宽的球体)用于拟合观测数据。在第一次迭代中，模型的变化非常可观，但之后就在两步之间间歇收敛。由ELKI可视化。} \label{fig_ZDQW_1}
\end{figure}

\subsection{历史}

亚瑟·登普斯特、楠·莱尔德和唐纳德·鲁宾在1977年的一篇经典论文中解释了EM算法并给出了它的名字。[1]他们指出，这种方法由早期作者“在特殊情况下提出了很多次”。最早的方法之一是由塞德里克·史密斯提出的用于估计等位基因频率的基因计数法。[2]罗尔夫·桑德伯格在与佩尔·马丁-洛夫和安德斯·马丁·洛夫合作之后，[3][4][5][6][7][8][9]在他的学位论文和其他几篇论文中发表了关于指数族的EM方法的非常详细的论述。[10][11][12]登普斯特-莱尔德-鲁宾在1977年的论文中推广了这种方法，并对更广泛的一类问题进行了收敛性分析。除开早期的创新发现之外，登普斯特-莱尔德-鲁宾在《皇家统计学会杂志》上发表的具有创新性的论文在皇家统计学会会议上受到热烈讨论，桑德伯格称这篇论文“非常出色”。登普斯特-莱尔德-鲁宾论文将EM方法确立为统计分析的重要工具。

登普斯特-莱尔德-鲁宾算法的收敛性分析是有缺陷的，正确的收敛性分析由吴建福于1983年发表。[13]吴的证明在指数簇之外建立了EM方法的收敛性，正如登普斯特-莱尔德-鲁宾所声称的那样。[13]

\subsection{介绍}

EM算法用于在方程不能直接求解的情况下找到统计模型的(局部)最大似然参数。典型情况下，这些模型除了未知参数和已知可观测到的数据之外，还涉及潜变量。也就是说，要么数据中存在缺失值，要么可以通过假设存在更多未观察到的数据点来更简单地建立模型。例如，可以通过假设每个观察到的数据点具有相应的未观察到的数据点或潜在变量，指定每个数据点所属的混合成分，来更简单地描述混合模型。

寻找最大似然解通常需要对所有未知值、参数和潜变量函数取似然函数的导数，同时求解所得方程。在包含潜变量的统计模型中，这通常是不可能的。相反，结果通常是一组互锁方程，其中参数的解需要知道潜变量的值，反之亦然，如果将一组方程代入另一组方程则会产生不可解方程。

EM算法是从观察到有一种方法可以用数值方法解决这两组方程开始的。可以简单地为两组未知数中的一组选取任意值，用它们来估计另一组，再用这些新的值来找到第一组更好的估计，然后在两者之间不断交替迭代，直到结果值都收敛到不动点。这是否有效还不清楚，但可以证明，在这种情况下它是有效，并且可能性的导数在该点(任意接近)为零，这意味着该点要么是极大值，要么是鞍点。[13] 一般来说，可能会出现多个极大值，但不能保证会找到全局最大值。某些似然也存在奇点，即无意义的最大值。 例如，这样的一个解：在混合模型中，EM可以找到的方法包括将其中一个分量的方差设置为零，并且将同一分量的平均参数设置为等于其中一个数据点。

\subsection{描述}

在生成观测数据集合 $X$ 的统计模型，一组未观察到的潜在数据或缺失值 $Z$ ，以及似然函数 

$(\theta; X, Z) = p(X, Z | \theta)$
中的未知参数向量 $\theta$ ，最大似然估计是通过最大化观测数据的边缘似然来确定的：

$(\theta; X) = p(X | \theta) = \int p(X, Z | \theta) dZ$

而，这个量通常是难以处理的（例如，如果 $Z$ 是一个事件序列，那么数值的数量随着序列长度呈指数增长，因求积的精确值将是非常困难的）。

EM算法通过迭代应用这两个步骤来寻找边缘似然的最大似然估计：

求期望步骤：定义 $Q(\theta | \theta^{(t)})$ 对数似然函数的期望值为 $\theta$ ，就目前的条件分布而言 $Z$ 考虑到 $X$ 和参数的前估计 $\theta^{(t)}$(d ) :

$Q(\theta | \theta^{(t)}) = E_{Z|X,\theta^{(t)}} [\log L(\theta; X, Z)]$

最大化步骤(M步): 查找最大化此数量的参数：

$\theta^{(t+1)} = \arg \max_{\theta} Q(\theta | \theta^{(t)})$

用EM的典型模型是将 $Z$ 作为一个潜在变量，表示在一组组中的成员：

\begin{enumerate}
\item 观测到的数据点 $X$ 可以是离散的（取有限或无限集合的值）或连续的（取或不可数无限集合的值）。与每个数据点相关联的可以是观察是否。
\item 缺失的值(又名潜变量) $Z$ 是离散的，从固定数量的值中提取，每个观察单位有一个潜在变量。
\item 参数是连续的，并且有两种：与所有数据点相关联的参数，以及与潜在变量的特定值相关联的参数(即，与对应潜在变量具有该值的所有数据点相关联的参数
\end{enumerate}

然而，也可以将EM应用于其他类型的模型。

动机如下。如果参数的值 $\theta$ 已知，通常情况下潜变量 $Z$ 的值可以通过最大化所有可能值的对数似然来找到，要么简单地迭代 $Z$ 或者通过隐马尔可夫模型的鲍姆-韦尔奇算法来计算。相反，如果我们知道潜变量 $Z$ 的值，我们要找到参数 $\theta$ 的估计值相当容易，通常通过简单地根据相关联的潜在变量的值对观察到的数据点进行分组，并对每个组中的点的值进行平均或其他的操作。这提出了一种迭代算法，在这两种情况下 $\theta$ 和 $Z$ 未知：

\begin{enumerate}
\item 首先，初始化参数 $\theta$ 一些随机值。
\item 在给定 $\theta$ 的情况下，计算每个 $Z$ 可能值的概率。
\item 然后，使用刚刚计算得到的 $Z$ 的值来计算更好的参数估计 $\theta$ 。
\item 重复步骤2和3，直到收敛。
\end{enumerate}

刚刚描述的算法单调地接近成本函数的局部最小值。

\subsection{性能}



\subsection{正确性的证明}



\subsection{作为最大化-最大化过程}



\subsection{应用}



\subsection{滤波和平滑电磁算法}



\subsection{变体}



\subsubsection{9.1 α-EM算法}



\subsection{与变分贝叶斯方法的关系}



\subsection{几何解释}



\subsection{例子}

\subsubsection{12.1 混合高斯分布}



\subsubsection{12.2 断尾回归和归并回归}



\subsection{替代算法}



\subsection{参考文献}
