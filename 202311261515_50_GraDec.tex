% 梯度下降
% keys 梯度 下降 优化 更新
% license Xiao
% type Tutor

\textbf{梯度下降}（Gradient decent），或称\textbf{最速下降}，是一种一阶优化算法，是神经网络中最基本最常用的优化方法。

梯度下降法搜索可能的权值假设空间，从而找到能够拟合训练样本的最佳权值 [1]。梯度下降法为误差反向传播算法提供了基础。而后者是多层神经网络训练的基础算法。

假设有一个没有阈值的线性神经元，其输入向量为$\bvec x$，输出为$y_o$，权值向量为$\bvec w$。正如基本的感知机训练算法一样，为了能够实现权值更新，要定义一个衡量神经元输出值与样本实际值之间误差，即\textbf{训练误差}（Training error），在现代深度学习文献中，也称为\textbf{损失函数}（Loss function）。通常可以采用最小二乘：
\begin{equation}\label{eq_GraDec_1}
E(\bvec w)=\frac{1}{2}\sum_i(y-y_o)^2=\frac{1}{2}\sum_i[y-y_o(\bvec x_i)]^2=\frac{1}{2}\sum_i[y-y_o(\bvec w \bvec x_i)]^2~
\end{equation}
其中，$i$表示第$i$个样本。

\autoref{eq_GraDec_1} 的含义是先计算出神经元对每个样本输入值产生的输出与样本输出值之间的误差，然后平方，最后把每个平方后的值相加求总和除以$2$，作为模型当前的训练误差。此时，由于在训练期间，训练样本本身是固定不变的，而模型的权值是变化的，是须要更新的。因此，训练误差是模型权值的函数。训练算法的实质就是，在一定的训练数据下，按照一定规则，反复调整权值（模型参数），从而使得训练误差达到最小。

在梯度下降法中，参数更新规则是基于误差函数的梯度。


% \subsection{程序实践}


\subsubsection{参考文献}
\begin{enumerate}
\item T. M. Mitchell, Machine learning. 1997.
\item 周志华. 机器学习[M]. 北京：清华大学出版社. 2016: 97
\item I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016: 174.
\end{enumerate}