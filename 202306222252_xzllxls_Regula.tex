% 正则化
% 正则化

\textbf{正则化}（Regularization）是机器学习中用于减少泛化误差（测试误差），从而缓解过度拟合的设计策略。当使用正则化策略减少泛化误差时，可能会增大训练误差。

\textbf{参数范数惩罚}是最常用的正则化策略之一。传统机器学习方法就有很多使用，而在当今的深度学习中也应用广泛。参数范数惩罚的主要思想是给目标函数$J$添加一个参数范数惩罚项$\Omega(\theta)$，限制模型的学习能力，从而减少过度拟合的发生。设$J'$为正则化后的目标函数，则有：
\begin{equation}
J'(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(\theta)
\end{equation}
其中，$\alpha\in[0,\infty)$是权衡范数惩罚项$\Omega$和标准目标函数$J(X;\theta)$相对贡献的超参数。将$\alpha$设为$0$表示没有正则化。

\subsection{$L^2$参数正则化}

$L^2$参数正则化指的是用参数的$2$范数作为惩罚项，即把公式$(1)$中的$\Omega(\theta)$写成参数的$2$范数形式。数学表示是：$\Omega(\theta)=\frac{1}{2}||w||_2^2$.
在优化代价函数时，会使得$\Omega(\theta)$有极小化的趋势，因此，该正则化策略会使得权值趋向于原点。

采用了$L_2$正则化策略的代价函数表示为：
\begin{equation}
J'(\theta;X,y)=J(\theta;X,y)+\alpha\frac{1}{2}||w||_2^2~
\end{equation}


有的文献称$L^2$正则化为\textbf{岭回归}。


\subsection{$L^1$参数正则化}