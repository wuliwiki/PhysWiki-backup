% 蒙特卡洛树搜索算法
% keys 蒙特卡洛|搜索
% license Usr
% type Tutor

\begin{issues}
\issueDraft
\issueTODO
\end{issues}

% 暂且把 UCB 与 蒙特卡洛树搜索算法 合并，如果需要可以单开一篇详细介绍 UCB。

蒙特卡洛树是一种不同于 Alpha-Beta 剪枝的优化搜索方法。他与 Alpha-Beta 剪枝相比，更好的适用于诸如围棋一类的大型对弈游戏（即搜索空间过大，但搜索过程需要兼顾“探索”与“最优化”，尽量得到全局最优解，避免陷入局部最优解）。但与 Alpha-Beta 剪枝相比，由于过程中采用蒙特卡洛方法（可以理解为抽样检测），搜索的结果并不保证一定是最优解。

值得一提的是，\textbf{蒙特卡洛树搜索算法不是蒙特卡洛算法}。

蒙特卡洛树搜索限定于解决 Combinatorial Game，例如一般的围棋、象棋等等。

\subsection{UCB 算法}
UCB，upper confidence bound，置信度上界，是蒙特卡洛树搜索中启发式搜索的一种常用方式，下面先来考察 UCB 算法。

简单来说采用 UCB 的蒙特卡洛树搜索是一种（类）启发式的搜索，大体思路是：在选择子节点的时候，
\begin{enumerate}
\item 优先考虑未搜索过的；
\item 如果都搜索过就根据给每个节点的“赋分”来选择，这个“赋分”是与这个节点的最终获胜概率正相关、与这个子节点访问的的次数负相关的。
\end{enumerate}
我们可以看到第 $2$ 条就是为了兼顾“探索”与“优化”。与最终获胜概率正相关为了保证“优化”、与访问次数负相关为了兼顾“探索”。





UCB 算法是用来评估多臂老虎机问题的，这个问题一般描述如下：

你面前有 $n$ 个老虎机，每次可以拉下一个老虎机的拉杆。每次拉拉杆都有一定的概率获得 $1$ 块钱。拉下每个拉杆获得奖金的概率不同。
麻烦的是，你事先并不知道这些概率分别是多少。好在你一共能按100次按钮，每次除了有可能获得奖励外，还能积累一些经验。不过每按一次按钮，你都会懊悔（regret），懊悔的程度为最优按钮（也就是概率最大的那个）的奖励概率减去本次按下的按钮的奖励概率。请问，有什么办法能让你在按完所有按钮后懊悔的总和最小。