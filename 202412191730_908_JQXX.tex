% 机器学习（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Machine_learning}{相关文章}。

\textbf{机器学习（ML）}是人工智能的一个研究领域，关注于开发和研究能够从数据中学习并对未见过的数据进行泛化的统计算法，从而在没有明确指令的情况下执行任务。[1] 深度学习领域的进展使得神经网络在性能上超越了许多先前的方法。[2]

机器学习应用于许多领域，包括自然语言处理、计算机视觉、语音识别、电子邮件过滤、农业和医学。[3][4] 将机器学习应用于商业问题的领域被称为预测分析。

统计学和数学优化（数学编程）方法构成了机器学习的基础。数据挖掘是一个相关的研究领域，专注于通过无监督学习进行探索性数据分析（EDA）。[6][7]

从理论角度来看，可能大致正确（PAC）学习为描述机器学习提供了一个框架。
\subsection{历史}  
“机器学习”这一术语由IBM员工、计算机游戏和人工智能领域的先驱亚瑟·塞缪尔（Arthur Samuel）于1959年创造。[8][9] 在这一时期，“自我学习计算机”这个同义词也曾被使用。[10][11]

尽管最早的机器学习模型是在1950年代由亚瑟·塞缪尔发明的，该程序用于计算每方在跳棋中的获胜概率，但机器学习的历史可以追溯到几十年来人类对研究人类认知过程的渴望和努力。[12] 1949年，加拿大心理学家唐纳德·赫布（Donald Hebb）出版了《行为的组织》（The Organization of Behavior）一书，在书中他提出了通过神经元之间特定交互形成的理论神经结构。[13] 赫布关于神经元相互作用的模型为人工智能和机器学习算法在节点（或计算机用来传输数据的人工神经元）下如何工作奠定了基础。[12] 其他研究人类认知系统的学者也为现代机器学习技术做出了贡献，包括逻辑学家沃尔特·皮茨（Walter Pitts）和沃伦·麦卡洛克（Warren McCulloch），他们提出了早期的神经网络数学模型，旨在开发模拟人类思维过程的算法。[12]

到1960年代初，雷神公司（Raytheon）开发了一种实验性的“学习机器”，名为Cybertron，它采用打孔带存储，用于分析声纳信号、心电图和语音模式，使用的是基础的强化学习。它通过人工操作员/教师反复“训练”以识别模式，并配备了一个“错误”按钮，用于在做出错误决策时促使其重新评估。[14] 1960年代有关机器学习的代表性书籍之一是尼尔森（Nilsson）的《学习机器》一书，主要讨论了用于模式分类的机器学习。[15] 与模式识别相关的兴趣持续到1970年代，正如Duda和Hart在1973年所描述的那样。[16] 1981年，有报告讨论了使用教学策略，使人工神经网络学习从计算机终端识别40个字符（26个字母、10个数字和4个特殊符号）。[17]

汤姆·M·米切尔（Tom M. Mitchell）提出了机器学习领域算法的广泛引用的正式定义：“如果一个计算机程序在经验E的基础上，针对某些任务类别T，通过性能度量P，在T类别中的任务执行表现有所提高，则该计算机程序可以说是从经验E中学习。”[18] 这个关于机器学习所涉及任务的定义提供了一个基本的操作性定义，而不是从认知角度来定义该领域。这一概念沿袭了阿兰·图灵（Alan Turing）在其论文《计算机器与智能》中的提议，其中“机器能思考吗？”的问题被“机器能做我们（作为思维实体）能够做的事吗？”所取代。[19]

现代机器学习有两个目标。一是根据已开发的模型对数据进行分类；另一目的是根据这些模型对未来的结果进行预测。一个专门用于数据分类的假设性算法，可能会使用计算机视觉技术，结合监督学习来训练算法识别癌变的痣。用于股票交易的机器学习算法可能会向交易员提供未来潜在的预测。[20]
\subsection{与其他领域的关系}  
\subsubsection{人工智能}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/108658dec604aeb3.png}
\caption{机器学习作为人工智能的一个子领域[21]} \label{fig_JQXX_1}
\end{figure}
作为一项科学事业，机器学习源自于对人工智能（AI）的探索。在人工智能作为学术学科的早期，一些研究者希望让机器从数据中学习。他们试图通过各种符号方法来解决这个问题，以及当时被称为“神经网络”的方法；这些方法大多是感知机和其他模型，后来被发现实际上是统计学中广义线性模型的再发明。[22] 概率推理也被应用，特别是在自动化医疗诊断中。[23]: 488

然而，随着对逻辑、知识为基础的方法的日益重视，人工智能与机器学习之间出现了分歧。概率系统面临数据获取和表示的理论和实践问题。[23]: 488 到1980年，专家系统已经主导了人工智能，统计学也不再受到青睐。[24] 虽然符号/知识为基础的学习方法在人工智能中仍然有所研究，推动了归纳逻辑编程（ILP）的发展，但更多的统计学研究现在已经脱离了人工智能的范畴，转向了模式识别和信息检索。[23]: 708–710, 755 神经网络研究在同一时期也被人工智能和计算机科学放弃。这一领域也被从其他学科的研究者所延续，包括约翰·霍普菲尔德、戴维·鲁梅哈特和杰弗里·辛顿。他们的主要成功是在1980年代中期重新发明了反向传播算法。[23]: 25

机器学习（ML）在1990年代重新组织并被认定为独立的学科，开始蓬勃发展。该领域的目标从实现人工智能转向解决具有实际性质的可解问题。它的重点从人工智能继承的符号方法转向了借鉴统计学、模糊逻辑和概率论的方法和模型。[24]
\subsubsection{数据压缩}  
机器学习与数据压缩之间有着紧密的联系。一个系统，如果能够根据整个历史数据预测序列的后验概率，就可以用于最优的数据压缩（通过对输出分布使用算术编码）。反过来，一个最优的压缩器也可以用于预测（通过找到在给定前历史数据的情况下最能压缩的符号）。这种等价性被用作使用数据压缩作为“通用智能”基准的理由。[25][26][27]  

另一种观点认为，压缩算法隐式地将字符串映射到隐式特征空间向量中，基于压缩的相似性度量计算这些特征空间中的相似性。对于每个压缩器 C(.)，我们定义一个关联的向量空间 ℵ，使得 C(.) 将输入字符串 x 映射到向量范数 ||~x||。由于空间限制，无法对所有压缩算法的特征空间进行详尽的检查；因此，选择检查三种典型的无损压缩方法：LZW、LZ77 和 PPM。[28]  

根据 AIXI 理论，这一点在 Hutter 奖中有更直接的解释，x 的最佳压缩是生成 x 的最小软件。例如，在该模型中，zip 文件的压缩大小包括 zip 文件和解压缩软件，因为没有这两者就无法解压，但可能存在一个更小的组合形式。  

由 AI 驱动的音频/视频压缩软件的例子包括 NVIDIA Maxine、AIVC。[29] 可以执行 AI 驱动图像压缩的软件的例子包括 OpenCV、TensorFlow、MATLAB 的图像处理工具箱（IPT）和高保真生成图像压缩。[30]  

在无监督机器学习中，K-means 聚类可以通过将相似的数据点分组成簇来压缩数据。该技术简化了处理缺乏预定义标签的大规模数据集，广泛应用于图像压缩等领域。[31]  

数据压缩旨在减少数据文件的大小，从而提高存储效率并加快数据传输速度。K-means 聚类是一种无监督机器学习算法，用于将数据集划分为指定数量的簇 k，每个簇由其点的质心表示。这个过程将大规模数据集压缩为一个更紧凑的代表性点集。特别在图像和信号处理方面，K-means 聚类通过将一组数据点替换为其质心，帮助减少数据量，同时大大减少所需的存储空间，保留原始数据的核心信息。[32]  

大语言模型（LLMs）也能实现无损数据压缩，正如 DeepMind 在其 Chinchilla 70B 模型中的研究所示。DeepMind 开发的 Chinchilla 70B 有效地压缩了数据，超越了传统方法，如便携式网络图形（PNG）图像和无损音频压缩编码（FLAC）音频。它成功地将图像和音频数据压缩至其原始大小的 43.4\% 和 16.4\%。[33]
\subsubsection{数据挖掘}  
机器学习和数据挖掘通常使用相同的方法，并且有很大的重叠，但尽管机器学习侧重于基于从训练数据中学习到的已知特征进行预测，数据挖掘则侧重于发现数据中（之前）未知的特征（这是数据库中知识发现的分析步骤）。数据挖掘使用了许多机器学习方法，但其目标不同；另一方面，机器学习也使用数据挖掘方法作为“无监督学习”或作为预处理步骤来提高学习者的准确性。这两个研究领域之间的许多混淆（尽管它们通常有独立的会议和期刊，ECML PKDD 是一个主要的例外）来自于它们所采用的基本假设：在机器学习中，性能通常是根据重现已知知识的能力来评估的，而在知识发现和数据挖掘（KDD）中，关键任务是发现以前未知的知识。在已知知识的评估中，未加信息的（无监督）方法通常会被其他监督方法超越，而在典型的 KDD 任务中，由于训练数据不可用，监督方法不能使用。

机器学习还与优化密切相关：许多学习问题被表述为在训练集示例上最小化某个损失函数。损失函数表示训练中模型的预测与实际问题实例之间的差异（例如，在分类中，人们希望为实例分配标签，而模型被训练来正确预测一组示例的预先分配标签）。[34]
\subsubsection{泛化}  
描述各种学习算法的泛化能力是当前研究的一个活跃话题，特别是对于深度学习算法。
\subsubsection{统计学}  
机器学习和统计学在方法上密切相关，但在主要目标上有所不同：统计学通过样本推断总体，而机器学习则发现可泛化的预测模式。[35] 根据迈克尔·I·乔丹（Michael I. Jordan）的说法，机器学习的思想，从方法论原则到理论工具，在统计学中有着悠久的历史。[36] 他还提出了“数据科学”这一术语，作为一个占位符来称呼整个领域。[36]

传统的统计分析需要事先选择一个最适合研究数据集的模型。此外，分析时仅包括基于以往经验认为重要或理论上相关的变量。相比之下，机器学习并不依赖于预先结构化的模型；相反，数据通过检测潜在的模式来塑造模型。使用更多的变量（输入）来训练模型，最终模型的准确性将更高。[37]

莱奥·布雷曼（Leo Breiman）区分了两种统计建模范式：数据模型和算法模型，[38] 其中“算法模型”指的就是像随机森林（Random Forest）这样的机器学习算法。

一些统计学家已经采纳了机器学习的方法，形成了他们称之为统计学习的结合领域。[39]
\subsubsection{统计物理}  
源自无序系统深厚物理学的分析和计算技术可以扩展到大规模问题，包括机器学习，例如，用于分析深度神经网络的权重空间。[40] 因此，统计物理学在医学诊断领域找到了应用。[41]
\subsection{理论}  
主要文章：计算学习理论和统计学习理论  
学习者的核心目标之一是从经验中进行泛化。[5][42] 在此语境下，泛化指的是学习机器在经历过学习数据集后，能够在新的、未见过的示例/任务上准确执行的能力。训练示例来自某些通常未知的概率分布（被认为代表事件发生的空间），学习者必须构建一个关于该空间的通用模型，使其能够在新的案例中做出足够准确的预测。

机器学习算法及其性能的计算分析是理论计算机科学的一个分支，称为计算学习理论，通过可能近似正确学习（PAC）模型进行研究。由于训练集是有限的，并且未来是不确定的，学习理论通常不能提供算法性能的保证。相反，通常会给出性能的概率边界。偏差-方差分解是量化泛化误差的一种方法。

为了在泛化的语境中获得最佳性能，假设的复杂性应与数据背后的函数复杂性相匹配。如果假设的复杂性低于函数的复杂性，则模型对数据拟合不足。如果响应中增加了模型的复杂性，则训练误差减少。但如果假设过于复杂，则模型容易过拟合，泛化能力会变差。[43]

除了性能边界，学习理论家还研究学习的时间复杂度和可行性。在计算学习理论中，如果某个计算可以在多项式时间内完成，那么它被认为是可行的。时间复杂度的结果有两种：正向结果表明某一类函数可以在多项式时间内学习；负向结果则表明某些类的函数无法在多项式时间内学习。
\subsection{方法}
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/e1c28c1201868559.png}
\caption{在监督学习中，训练数据会标注期望的答案，而在无监督学习中，模型会在未标注的数据中识别模式或结构。} \label{fig_JQXX_2}
\end{figure}
机器学习方法通常分为三大类，这些类别对应不同的学习范式，具体取决于学习系统可用的“信号”或“反馈”的性质：
\begin{itemize}
\item 监督学习：计算机被提供一组输入示例及其对应的期望输出，这些输出由“教师”提供，目标是学习一个通用规则，将输入映射到输出。
\item 无监督学习：学习算法没有标签，必须自行在输入中找到结构。无监督学习可以是一个独立的目标（发现数据中的隐藏模式），也可以是实现某个目标的手段（特征学习）。
\item 强化学习：计算机程序与一个动态环境进行交互，必须执行特定目标（例如驾驶车辆或与对手玩游戏）。在导航问题空间时，程序会收到类似奖励的反馈，它会尽量最大化这些反馈。[5]
\end{itemize}
尽管每种算法都有其优点和局限性，但没有一种算法适用于所有问题。[44][45][46]
\subsubsection{监督学习}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/7c5b1f1fd0da91d3.png}
\caption{支持向量机是一种监督学习模型，它将数据分成由线性边界分隔的区域。在这里，线性边界将黑色圆圈与白色圆圈分开。} \label{fig_JQXX_3}
\end{figure}
监督学习算法通过建立一个数学模型来学习一组包含输入和期望输出的数据。[47] 这些数据被称为训练数据，包含一组训练样本。每个训练样本都有一个或多个输入以及期望的输出，输出也称为监督信号。在数学模型中，每个训练样本被表示为一个数组或向量，有时称为特征向量，训练数据则表示为一个矩阵。通过对目标函数进行迭代优化，监督学习算法学习一个函数，该函数可以用来预测与新输入相关的输出。[48] 一个优化的函数可以使算法正确地确定未包含在训练数据中的输入对应的输出。当一个算法随着时间推移提高其输出或预测的准确性时，我们称其已经学会执行该任务。[18]

监督学习算法的类型包括主动学习、分类和回归。[49] 分类算法用于输出被限制为一组有限值的情况，而回归算法用于输出可能具有某个范围内任意数值的情况。例如，对于一个过滤电子邮件的分类算法，输入将是一个到达的电子邮件，输出将是该邮件应该存入的文件夹名称。回归的例子包括预测一个人的身高或未来的气温。[50]

相似度学习是与回归和分类密切相关的监督学习领域，其目标是通过使用相似度函数来学习如何判断两个对象的相似性或相关性。它在排名、推荐系统、视觉身份跟踪、面部验证和语音验证等领域有广泛应用。
\subsubsection{无监督学习}  
无监督学习算法用于发现那些没有标签、分类或归类的数据中的结构。与监督学习通过反馈来学习不同，无监督学习算法通过识别数据中的共性，并根据每个新数据中是否存在这些共性做出反应。无监督学习的核心应用包括聚类、降维和密度估计。

聚类分析是将一组观察结果分配到多个子集（称为聚类）中的过程，使得同一聚类内的观察结果在一个或多个预定标准下相似，而来自不同聚类的观察结果则是不同的。不同的聚类技术对数据的结构有不同的假设，通常通过某种相似性度量来定义，并通过内部紧密度（同一聚类内成员之间的相似性）和分离度（不同聚类之间的差异）等方式来评估。其他方法则基于估计的密度和图连接性。

一种特殊类型的无监督学习，称为自监督学习，涉及通过从数据本身生成监督信号来训练模型。
\subsubsection{半监督学习}  
半监督学习介于无监督学习（没有任何标签的训练数据）和监督学习（完全有标签的训练数据）之间。在半监督学习中，部分训练样本缺少标签，但许多机器学习研究人员发现，当无标签数据与少量有标签数据结合使用时，可以显著提高学习的准确性。

在弱监督学习中，训练标签是噪声的、有限的或不精确的；然而，这些标签通常更容易获得，从而形成更大的有效训练集。
\subsubsection{强化学习}
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/2c45fd1e87c18e90.png}
\caption{} \label{fig_JQXX_4}
\end{figure}
强化学习是机器学习的一个领域，关注软件代理如何在环境中采取行动，以最大化某种累积奖励的概念。由于其广泛的适用性，该领域在许多其他学科中也有研究，如博弈论、控制理论、运筹学、信息理论、基于仿真的优化、多智能体系统、群体智能、统计学和遗传算法。在强化学习中，环境通常表示为马尔可夫决策过程（MDP）。许多强化学习算法使用动态编程技术。强化学习算法不假设对MDP的精确数学模型有已知信息，当精确模型不可行时，便使用这些算法。强化学习算法被用于自动驾驶车辆或在与人类对手对战时学习游戏。
\subsubsection{降维}
降维是通过获得一组主变量来减少考虑的随机变量数量的过程。[56] 换句话说，它是减少特征集的维度的过程，也称为“特征数量”。大多数降维技术可以视为特征消除或特征提取。降维的常用方法之一是主成分分析（PCA）。PCA涉及将高维数据（例如，3D）转换为较小的空间（例如，2D）。流形假设提出，高维数据集沿着低维流形分布，许多降维技术都假设这一点，导致了流形学习和流形正则化的研究领域。
\subsubsection{其他类型}
还有一些方法并不完全符合这三种分类，有时同一个机器学习系统会使用多种方法。例如，主题建模、元学习。[57]

\textbf{自学习}

自学习，作为一种机器学习范式，在1982年被引入，同时提出了一种能够自学习的神经网络，名为交叉条形自适应阵列（CAA）。[58] 它是在没有外部奖励和外部教师建议的情况下进行学习。CAA自学习算法以交叉条形的方式计算关于行为和情绪（对后果情况的感受）的决策。该系统由认知与情感之间的互动驱动。[59] 自学习算法更新一个记忆矩阵 \( W = ||w(a,s)|| \)，在每次迭代中执行以下机器学习例程：
\begin{enumerate}
\item 在情境 \( s \) 中执行行为 \( a \)
\item 接收后果情境 \( s' \)
\item 计算处于后果情境中的情绪 \( v(s') \)
\item 更新交叉条形记忆 \( w'(a,s) = w(a,s) + v(s') \)
\end{enumerate}
这是一个只有一个输入（情境）和一个输出（行为或动作 \( a \)）的系统。没有来自环境的单独强化输入或建议输入。反向传播的值（次级强化）是对后果情境的情绪反应。CAA存在于两个环境中，一个是行为环境，它在其中表现，另一个是遗传环境，在这里它仅仅接收一次关于将要遇到的情境的初步情绪。收到来自遗传环境的基因组（物种）向量后，CAA在一个包含既有可取又有不可取情境的环境中学习目标导向行为。[60]

\textbf{特征学习}

特征学习（Feature learning）是一种学习算法，旨在发现训练过程中提供的输入的更好表示。[61] 经典的例子包括主成分分析（PCA）和聚类分析。特征学习算法，也称为表示学习算法，通常试图保持输入中的信息，但也会以一种方式进行转化，使其更有用，通常作为执行分类或预测之前的预处理步骤。这种技术允许重建来自未知数据生成分布的输入，同时不一定忠实于该分布下不太可能的配置。这取代了手动特征工程，并允许机器既学习特征又使用这些特征来执行特定任务。

特征学习可以是监督式学习或无监督式学习。在监督式特征学习中，特征通过标记的输入数据来学习。例子包括人工神经网络、多层感知器和监督字典学习。在无监督式特征学习中，特征通过未标记的输入数据来学习。例子包括字典学习、独立成分分析、自动编码器、矩阵分解[62]和各种形式的聚类[63][64][65]。

流形学习算法试图在低维表示的约束下进行学习。稀疏编码算法试图在学习表示稀疏的约束下进行学习，这意味着数学模型有很多零值。多线性子空间学习算法旨在直接从多维数据的张量表示中学习低维表示，而无需将它们重新塑形为更高维的向量。[66] 深度学习算法发现多层次的表示，或特征的层次结构，其中更高层次、更抽象的特征是基于（或生成）较低层次特征定义的。有人认为，智能机器是指能够学习出一个表示，解开解释观察数据的潜在变化因素的机器。[67]

特征学习的动机在于，机器学习任务（如分类）通常需要数学上和计算上便于处理的输入。然而，现实世界的数据（如图像、视频和传感数据）并未通过算法尝试定义特定的特征。另一种方法是通过检查来发现这些特征或表示，而不依赖于显式的算法。

\textbf{稀疏字典学习}

稀疏字典学习是一种特征学习方法，其中训练样本被表示为基函数的线性组合，并假设其为稀疏矩阵。这种方法是强NP难题，难以近似求解。[68] 稀疏字典学习的一个流行启发式方法是k-SVD算法。稀疏字典学习已应用于多个领域。在分类中，问题是确定一个之前未见过的训练样本属于哪个类别。对于已经构建了每个类别的字典，新训练样本会与在相应字典中稀疏表示最好的类别关联。稀疏字典学习还应用于图像去噪。其关键思想是，干净的图像补丁可以通过图像字典进行稀疏表示，而噪声则不能。[69]

\textbf{异常检测}

在数据挖掘中，异常检测，也称为离群点检测，是识别那些与大多数数据显著不同的稀有项、事件或观察结果的过程，这些异常引起了怀疑。[70] 通常，异常项代表一个问题，如银行欺诈、结构缺陷、医疗问题或文本中的错误。异常也被称为离群点、新奇、噪声、偏差和例外。[71]

特别是在滥用和网络入侵检测的背景下，感兴趣的对象往往不是稀有的对象，而是意外的活动中断。这种模式不符合将稀有对象定义为离群点的常见统计学定义。许多离群点检测方法（特别是无监督算法）在这种数据上会失败，除非进行适当的聚合。相反，聚类分析算法可能能够检测到这些模式所形成的微聚类。[72]

异常检测技术通常分为三大类。[73] 无监督异常检测技术在一个未标记的测试数据集中检测异常，假设数据集中的大多数实例是正常的，通过寻找与数据集其他部分最不匹配的实例。监督异常检测技术要求一个已经标记为“正常”和“异常”的数据集，并通过训练分类器来检测异常（与许多其他统计分类问题的主要区别在于离群点检测的固有不平衡性）。半监督异常检测技术从给定的正常训练数据集中构建一个表示正常行为的模型，然后测试测试实例是否可能由该模型生成。

\textbf{机器人学习}

机器人学习受到多种机器学习方法的启发，从监督学习、强化学习[74][75]到最终的元学习（例如MAML）。

\textbf{关联规则}

关联规则学习是一种基于规则的机器学习方法，用于发掘大规模数据库中变量之间的关系。其目的是通过某种“有趣性”度量来识别数据库中发现的强规则。[76]

基于规则的机器学习是指任何识别、学习或演化“规则”的机器学习方法，用于存储、操作或应用知识。基于规则的机器学习算法的定义特征是识别和利用一组关系规则，这些规则共同代表系统所捕获的知识。这与其他常见的机器学习算法形成对比，后者通常识别一个单一的模型，这个模型可以普遍应用于任何实例，以进行预测。[77] 基于规则的机器学习方法包括学习分类系统、关联规则学习和人工免疫系统。

基于强规则的概念，Rakesh Agrawal、Tomasz Imieliński 和 Arun Swami 引入了关联规则，用于发现大型事务数据（如超市销售点系统记录的数据）中产品之间的规律。[78] 例如，在超市销售数据中发现的规则 
{
洋葱,
土豆
}
⇒
{
汉堡包
}  
表示如果顾客同时购买洋葱和土豆，他们很可能还会购买汉堡肉。此类信息可作为决定营销活动（如促销定价或产品摆放）的依据。除了市场购物篮分析，关联规则现在还广泛应用于包括网页使用挖掘、入侵检测、连续生产和生物信息学等应用领域。与序列挖掘不同，关联规则学习通常不考虑事务内或事务间项目的顺序。

学习分类器系统（LCS）是一类基于规则的机器学习算法，结合了发现组件（通常是遗传算法）和学习组件，执行监督学习、强化学习或无监督学习。它们寻求识别一组上下文相关的规则，这些规则共同存储并以分段的方式应用知识，以进行预测。[79]

归纳逻辑编程（ILP）是一种利用逻辑编程作为输入示例、背景知识和假设的统一表示的规则学习方法。给定已知背景知识的编码和一组作为逻辑事实数据库表示的示例，ILP系统将推导出一个假设的逻辑程序，该程序涵盖所有正例且没有负例。归纳编程是一个相关领域，它考虑任何类型的编程语言来表示假设（不仅仅是逻辑编程），如函数式程序。

归纳逻辑编程在生物信息学和自然语言处理中特别有用。Gordon Plotkin 和 Ehud Shapiro 为逻辑环境中的归纳机器学习奠定了初步的理论基础。[80][81][82] Shapiro 在1981年构建了他们的第一个实现（模型推理系统）：一个Prolog程序，通过正负示例归纳推导逻辑程序。[83] 这里的“归纳”一词指的是哲学中的归纳法，指提出一个理论来解释观察到的事实，而不是数学归纳法，即证明某个属性适用于一个有序集的所有成员。
\subsection{模型}
机器学习模型是一种数学模型，一旦在给定的数据集上“训练”完成，就可以用于对新数据进行预测或分类。在训练过程中，学习算法通过迭代调整模型的内部参数，以最小化预测中的误差。[84] 从广义上讲，"模型"这个术语可以指代多个层次的具体性，从一类模型及其相关的学习算法，到一个完全训练的模型，其中所有的内部参数都已调优。[85]

已使用并研究了多种类型的模型用于机器学习系统，选择最合适的模型来完成任务被称为模型选择。
\subsubsection{人工神经网络}
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/30018251155614e0.png}
\caption{人工神经网络是一个互联的节点群体，类似于大脑中庞大的神经元网络。在这里，每个圆形节点代表一个人工神经元，箭头表示从一个人工神经元的输出到另一个人工神经元的输入的连接。} \label{fig_JQXX_5}
\end{figure}
人工神经网络（ANNs），或称连接主义系统，是一种计算系统，灵感来源于构成动物大脑的生物神经网络。这些系统通过考虑示例来“学习”执行任务，通常不需要编程任何特定任务的规则。

ANN 是基于一组相互连接的单元或节点（称为“人工神经元”）的模型，这些人工神经元大致模拟生物大脑中的神经元。每个连接，类似于生物大脑中的突触，可以将信息（“信号”）从一个人工神经元传递到另一个。接收到信号的人工神经元可以处理该信号，然后向与其连接的其他人工神经元发送信号。在常见的 ANN 实现中，人工神经元之间的连接上的信号是一个实数，且每个人工神经元的输出是其输入的总和通过某个非线性函数计算得出的。人工神经元之间的连接称为“边”。人工神经元和边通常具有一个权重，在学习过程中会进行调整。权重会增加或减少连接处信号的强度。人工神经元可能具有一个阈值，只有当信号总和超过该阈值时，信号才会发送。通常，人工神经元会被聚集成层。不同的层可能会对其输入执行不同类型的变换。信号从第一层（输入层）传递到最后一层（输出层），可能在经过多个层的处理后。

ANN 方法最初的目标是以与人脑相同的方式解决问题。然而，随着时间的推移，研究重点转向了执行特定任务，这导致了与生物学的偏离。人工神经网络已被应用于多种任务，包括计算机视觉、语音识别、机器翻译、社交网络过滤、棋盘游戏和视频游戏、以及医学诊断。

深度学习由人工神经网络中的多个隐层组成。这种方法试图模拟人脑如何处理光和声音，转化为视觉和听觉。深度学习的一些成功应用包括计算机视觉和语音识别。[86]
\subsubsection{决策树}
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/c4bfbb6b46e01f2b.png}
\caption{一棵显示泰坦尼克号乘客生存概率的决策树} \label{fig_JQXX_6}
\end{figure}
决策树学习使用决策树作为预测模型，通过观察某个项目的特征（在树枝上表示）来推导该项目的目标值（在叶子节点上表示）。它是统计学、数据挖掘和机器学习中常用的预测建模方法。目标变量可以取离散值的树模型称为分类树；在这些树结构中，叶子节点代表类别标签，树枝表示导致这些类别标签的特征组合。目标变量可以取连续值（通常是实数）的决策树称为回归树。在决策分析中，决策树可以用来直观且明确地表示决策和决策过程。在数据挖掘中，决策树描述了数据，但生成的分类树可以作为决策过程的输入。
\subsubsection{支持向量机}   
支持向量机（SVMs），也称为支持向量网络，是一类相关的监督学习方法，广泛应用于分类和回归任务。给定一组训练样本，每个样本被标记为属于两个类别中的一个，SVM训练算法构建一个模型，预测新的样本是否属于某一类别。SVM训练算法是一个非概率性的二元线性分类器，尽管存在像Platt缩放这样的技术，可以将SVM应用于概率分类场景。除了执行线性分类外，SVM还可以通过所谓的核技巧高效地执行非线性分类，将输入隐式映射到高维特征空间。
\subsubsection{回归分析}
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/ff90a5240f3131d3.png}
\caption{线性回归在数据集上的示意图} \label{fig_JQXX_7}
\end{figure}
回归分析包含了多种统计方法，用于估计输入变量与其相关特征之间的关系。其最常见的形式是线性回归，其中绘制一条直线，以数学标准（如普通最小二乘法）最佳拟合给定的数据。后者通常通过正则化方法扩展，以减轻过拟合和偏差，例如岭回归。在处理非线性问题时，常用的模型包括多项式回归（例如，Microsoft Excel中用于趋势线拟合的回归）、逻辑回归（常用于统计分类）或甚至核回归，通过利用核技巧将输入变量隐式映射到更高维度的空间，从而引入非线性。
\subsubsection{贝叶斯网络}

贝叶斯网络、信念网络或有向无环图模型是一种概率图模型，通过有向无环图（DAG）表示一组随机变量及其条件独立性。例如，贝叶斯网络可以表示疾病与症状之间的概率关系。给定症状后，可以利用该网络计算各种疾病存在的概率。存在高效的算法来执行推理和学习。用于建模变量序列（如语音信号或蛋白质序列）的贝叶斯网络称为动态贝叶斯网络。能够表示并解决不确定性下决策问题的贝叶斯网络的一般化形式称为影响图。