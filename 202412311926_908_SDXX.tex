% 深度学习（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Deep_learning}{相关文章}。

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/b06d12296b7d8e2d.png}
\caption{在深度学习中以多层抽象表示图像[1]} \label{fig_SDXX_1}
\end{figure}
深度学习是机器学习的一个子领域，专注于利用神经网络执行分类、回归和表示学习等任务。该领域受到生物神经科学的启发，核心在于将人工神经元堆叠成多层，并通过“训练”使其能够处理数据。“深度”一词指的是网络中使用了多层结构，这些层数从三层到几百甚至几千层不等。深度学习的方法可以是监督学习、半监督学习或无监督学习。[2]

常见的深度学习网络架构包括全连接网络（Fully Connected Networks）、深度信念网络（Deep Belief Networks）、循环神经网络（Recurrent Neural Networks）、卷积神经网络（Convolutional Neural Networks）、生成对抗网络（Generative Adversarial Networks）、Transformer以及神经辐射场（Neural Radiance Fields）。这些架构已应用于计算机视觉、语音识别、自然语言处理、机器翻译、生物信息学、药物设计、医学图像分析、气候科学、材料检测以及棋盘游戏程序等领域，并在许多情况下取得了与人类专家相当甚至超越的表现。[3][4][5]

早期形式的神经网络受到生物系统中信息处理和分布式通信节点的启发，特别是人脑。然而，当前的神经网络并不旨在模拟生物体的大脑功能，在这一方面通常被认为是低质量的模型。[6]
\subsection{概述}
大多数现代深度学习模型基于多层神经网络，例如卷积神经网络和Transformer，但它们也可能包含命题公式或潜变量，这些变量在深度生成模型中按层次组织，如深度信念网络和深度玻尔兹曼机中的节点。[7]

从根本上说，深度学习指的是一类机器学习算法，其特点是使用层次化的结构将输入数据逐步转换为更加抽象和复合的表示。例如，在图像识别模型中，原始输入可能是图像（表示为像素张量）。第一层表示可能试图识别基本形状（如线条和圆形），第二层可能组合和编码边缘排列，第三层可能编码鼻子和眼睛，第四层可能识别出图像中包含一张人脸。

深度学习的一个关键特点是，模型可以自主学习最佳特征及其所在的层级。在深度学习出现之前，机器学习技术通常需要通过手动设计特征工程来将数据转换为更适合分类算法操作的形式。而在深度学习方法中，特征不是手动设计的，模型会从数据中自动发现有用的特征表示。这并不意味着无需手动调整，例如，层数和每层大小的变化会提供不同程度的抽象。[8][2]

“深度学习”中的“深度”指的是数据被转换的层数。更确切地说，深度学习系统具有较大的信用分配路径（CAP，Credit Assignment Path）深度。CAP是从输入到输出的一系列转换过程，描述了输入和输出之间的潜在因果关系。对于前馈神经网络，CAP深度等于网络的深度，即隐藏层的数量加一（因为输出层也被参数化）。对于信号可能在某一层多次传播的循环神经网络，CAP深度潜在无限。[9] 虽然没有统一公认的深度门槛来区分浅层学习与深度学习，但大多数研究者认为深度学习的CAP深度大于2。研究表明，CAP深度为2的模型在理论上是一个通用逼近器，能够模拟任何函数。[10] 超过这一深度，更多的层不会提高网络作为函数逼近器的能力。然而，深度模型（CAP > 2）能够比浅层模型提取出更好的特征，因此额外的层数有助于有效地学习特征。

深度学习架构可以通过逐层的贪婪方法构建。[11] 深度学习能够解开抽象，提取出哪些特征有助于提升性能。[8]

深度学习算法可以应用于无监督学习任务。这是一个重要的优势，因为无标签数据比有标签数据更为丰富。可以通过无监督方式训练的深度结构包括深度信念网络等。[8][12]

“深度学习”这一术语由Rina Dechter在1986年引入机器学习领域，[13] 并由Igor Aizenberg及其同事于2000年引入人工神经网络领域，最初用于描述布尔阈值神经元。[14][15] 不过，其出现的历史显然更加复杂。[16]
\subsection{解释}
深度神经网络通常通过\textbf{通用逼近定理}[17][18][19][20][21]或\textbf{概率推断}[22][23][8][9][24]进行解释。

经典的\textbf{通用逼近定理}涉及具有单个有限大小隐藏层的前馈神经网络逼近连续函数的能力。[17][18][19][20] 1989年，George Cybenko首次证明了具有S形激活函数的前馈网络的通用逼近能力，[17] 这一结果在1991年被Kurt Hornik推广到多层前馈网络架构。[18] 最近的研究还表明，通用逼近定理同样适用于诸如Kunihiko Fukushima提出的修正线性单元（ReLU）等无界激活函数。[25][26]

针对深度神经网络的通用逼近定理则关注网络的宽度有限，但深度可以增加的情况下的逼近能力。Lu等人[21]证明，如果深度神经网络的宽度严格大于输入维度，且激活函数为ReLU，那么该网络可以逼近任何勒贝格可积函数；如果宽度小于或等于输入维度，则深度神经网络不具有通用逼近能力。

\textbf{概率解释}[24]来源于机器学习领域，涉及推断[23][7][8][9][12][24]以及与训练和测试相关的优化概念，分别对应于拟合和泛化。更具体地说，概率解释将激活非线性视为累积分布函数。[24] 概率解释促成了在神经网络中引入\textbf{dropout}作为正则化方法。这一解释由Hopfield、Widrow和Narendra等研究人员提出，并通过Bishop等人的综述文章得到推广。[27]
\subsection{历史}
\subsubsection{1980年之前}
人工神经网络（ANN）分为两种类型：\textbf{前馈神经网络（FNN）}或\textbf{多层感知机（MLP）}和\textbf{循环神经网络（RNN）}。RNN的连接结构中包含循环，而FNN则没有。

1920年代，\textbf{威廉·伦茨（Wilhelm Lenz）}和\textbf{恩斯特·伊辛（Ernst Ising）}提出了\textbf{伊辛模型（Ising model）}，[28][29] 其本质上是一种非学习型RNN架构，由类神经元的阈值元素组成。1972年，\textbf{天利俊一（Shun'ichi Amari）}将这一架构改造成自适应的。[30][31] 他的学习型RNN在1982年由\textbf{约翰·霍普菲尔德（John Hopfield）}重新发表。[32] 其他早期的循环神经网络包括\textbf{中野薰（Kaoru Nakano）}在1971年发表的模型。[33][34] 早在1948年，\textbf{艾伦·图灵（Alan Turing）}就已经在未发表的论文《智能机器》（Intelligent Machinery）中提出了与人工进化和学习型RNN相关的思想。[35][31]

1958年，\textbf{弗兰克·罗森布拉特（Frank Rosenblatt）}提出了感知机（Perceptron），这是一个包含三层的MLP：输入层、具有随机权重的隐藏层（不参与学习）和输出层。他在1962年的书中进一步介绍了感知机的变体和计算机实验，包括一种四层感知机的版本，其“终端前网络”（preterminal networks）具有自适应权重。[37] 书中引用了\textbf{R.D. Joseph}于1960年提出的一个类似系统[38]，但Joseph的学习算法并不实用，最终被遗忘。

第一个可用的深度学习算法是1965年\textbf{亚历克谢·伊瓦赫宁科（Alexey Ivakhnenko）}和Lapa提出的\textbf{数据处理组方法（Group Method of Data Handling, GMDH）}，它可以训练任意深度的神经网络。[39] 1971年的一篇论文描述了用这种方法训练的八层深度网络。[41] 该方法通过逐层回归分析进行训练，并使用单独的验证集剪枝多余的隐藏单元。节点的激活函数是柯尔莫哥洛夫-加博尔多项式（Kolmogorov-Gabor polynomials），因此这些是最早的带有乘性单元或“门”的深度网络。[31]

第一个使用\textbf{随机梯度下降（Stochastic Gradient Descent, SGD）}训练的多层感知机（MLP）由\textbf{天利俊一}于1967年提出。[43] 他的学生Saito的计算机实验显示，一个五层MLP通过两个可调节层学会了非线性可分模式的内部表示。[31] 随着硬件和超参数调节的发展，端到端的随机梯度下降已成为当前主流的训练技术。

1969年，\textbf{福岛邦彦（Kunihiko Fukushima）}引入了\textbf{修正线性单元（ReLU, Rectified Linear Unit）}激活函数。[25][31] ReLU已成为深度学习中最流行的激活函数。[44]

卷积神经网络（CNN）的深度学习架构，包含卷积层和下采样层，起源于1979年\textbf{福岛邦彦}提出的新认知机（Neocognitron），尽管当时未使用反向传播进行训练。[45][46]

\textbf{反向传播（Backpropagation）}是一种高效应用由\textbf{戈特弗里德·威廉·莱布尼茨（Gottfried Wilhelm Leibniz）}于1673年提出的链式法则的技术，用于可微节点组成的网络。术语“误差反向传播”（Back-propagating errors）由\textbf{罗森布拉特}在1962年引入，[37] 但他并未实现该方法。\textbf{Henry J. Kelley}在1960年提出了控制理论背景下的连续版本反向传播。[48] 现代形式的反向传播首次由\textbf{Seppo Linnainmaa}在1970年的硕士论文中发表。[49][50][31] \textbf{G.M. Ostrovski}等人在1971年重新发表了该方法。[51][52] \textbf{Paul Werbos}在1982年将反向传播应用于神经网络，[53] 但他的1974年博士论文中尚未描述该算法。[52] 1986年，\textbf{David E. Rumelhart}等人推广了反向传播，但未引用最初的研究。[55][56]
\subsubsection{1980年代至2000年代}
时延神经网络（TDNN）：1987年，Alex Waibel 引入了时延神经网络（TDNN），将卷积神经网络（CNN）应用于音素识别。它使用了卷积、权重共享和反向传播。[57][58]CNN早期应用：1988年，Wei Zhang将反向传播训练的CNN用于字母识别。[59] 1989年，Yann LeCun等人开发了LeNet，用于识别邮件上的手写邮政编码，训练耗时3天。[60]1990年，Wei Zhang在光学计算硬件上实现了CNN。[61] 1991年，CNN被应用于医学图像分割[62]和乳腺癌检测。[63] 1998年，LeNet-5是一个7层CNN，用于分类手写数字，并被多家银行用于识别支票上的手写数字。[64]  

RNN在1980年代得到了进一步发展，主要用于序列处理。展开后的RNN在数学上类似于深度前馈层。[28][30] 两个早期的关键RNN模型：Jordan网络（1986）[65]和Elman网络（1990）[66]，被用于认知心理学问题的研究。

1980年代，反向传播在长信用分配路径上的表现不佳。1991年，Jürgen Schmidhuber提出了一种分层RNN，通过自监督学习逐层预训练，每层RNN尝试预测自身的下一个输入。[67][68]  1993年，这种神经历史压缩器解决了一个“非常深度学习”任务，涉及1000多层展开的RNN。[69]  

1991年，Sepp Hochreiter在毕业论文中提出了LSTM，并分析了梯度消失问题。[70][71] 1995年，LSTM被正式发表。[72] LSTM能够处理需要数千离散时间步的长期记忆任务。现代LSTM架构于1999年引入“遗忘门”，成为标准RNN架构。[73]  

生成对抗网络的雏形：1991年，Schmidhuber提出了“对抗神经网络”，其中两个网络以零和博弈形式竞争。[74][75] 这一原理在2014年被用于开发生成对抗网络（GANs）。[76]  

无监督学习模型：受统计力学的启发，Terry Sejnowski、Peter Dayan 和 Geoffrey Hinton等人开发了玻尔兹曼机、受限玻尔兹曼机、赫姆霍兹机及其“唤醒-睡眠”算法。[77][78][79][80] 这些模型设计用于深度生成模型的无监督学习，但与反向传播相比计算成本更高。 生物信息学的早期应用：1988年的网络成为蛋白质结构预测的最新成果，这是深度学习在生物信息学中的早期应用。[82]  

ANN的探索：ANN（包括RNN）在语音识别中的浅层和深层学习已被研究多年。[83][84][85] 然而，早期方法未能超越基于生成模型的GMM-HMM技术。[86] 主要困难包括梯度消失[70]和神经预测模型中时间相关性较弱。[87][88]  

例外进展：1990年代末，SRI International在美国NSA和DARPA资助下，研究语音和说话人识别。1998年，Larry Heck领导的团队在NIST说话人识别基准中取得了显著成功，并在Nuance Verifier中部署，这是深度学习的首次重大工业应用。[89][91]  原始特征的探索：1990年代末，深度自编码器首次在“原始”语谱图或线性滤波特征上表现出优越性，超越了传统的梅尔倒谱特征。[90] 随后，基于原始语音波形的深度学习在大规模任务中取得了优异表现。[92]  
\subsubsection{2000年代}  
在1990年代和2000年代，神经网络进入了一段低潮期。由于人工神经网络的计算成本较高以及对生物网络连接方式缺乏理解，研究更倾向于采用任务特定的手工设计特征（如Gabor滤波器）和支持向量机（SVM）等更简单的模型。[citation needed]  

2003年，LSTM在某些任务上开始与传统语音识别器竞争。[93] 2006年，Alex Graves、Santiago Fernández、Faustino Gomez和Schmidhuber将LSTM与\textbf{连接时间分类（CTC）}结合在一起，形成了LSTM堆栈。[94][95] 2009年，LSTM成为首个在模式识别比赛中获胜的RNN，用于连接手写识别任务。[96][9]  

2006年，Geoff Hinton、Ruslan Salakhutdinov、Osindero和Teh发表了关于\textbf{深度信念网络（DBN）}的论文。[97][98] 该模型被开发用于生成式建模。训练过程包括先训练一个受限玻尔兹曼机（RBM），然后冻结这一层，再在其上训练另一层，以此类推，最后可选地使用监督反向传播进行微调。[99] 深度信念网络可以建模高维概率分布，例如MNIST图像的分布，但收敛速度较慢。[100][101][102]  

根据Yann LeCun的说法，2000年代早期，CNN已经处理了估计10\%至20\%的美国手写支票。[103] 深度学习在大规模语音识别中的工业应用始于2010年前后。  

2009年，NIPS举办了“深度学习与语音识别”研讨会，旨在解决基于深度生成模型的语音识别的局限性，并探索在更强硬件和大规模数据集支持下神经网络的实用性。当时认为，通过深度信念网络（DBN）对深度神经网络（DNN）进行生成式预训练可以克服神经网络的主要困难。然而，研究发现，若使用包含大量上下文相关输出层的大型DNN，仅通过大规模训练数据和直接反向传播即可显著降低错误率，相较于当时的高斯混合模型（GMM）/隐马尔可夫模型（HMM）和其他生成式模型系统，这种方法效果更佳。[104]两种系统产生的识别错误性质截然不同，[105] 为将深度学习集成到已有高效语音解码系统提供了技术见解。[23][106][107] 2009年至2010年左右的分析对比了基于GMM和其他生成式语音模型与DNN模型的表现，激发了工业界对深度学习在语音识别中投资的兴趣。[105] 这一分析表明，判别式DNN与生成式模型之间的误差率差距小于1.5\%。[104][105][108]2010年，研究人员将深度学习从TIMIT扩展到大词汇量语音识别。他们通过基于决策树构建的上下文相关HMM状态，采用DNN的大型输出层，从而实现这一突破。[109][110][111][106]  
\subsubsection{深度学习革命}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/19d3951b7fd96363.png}
\caption{深度学习是机器学习的一个子集，而机器学习是人工智能（AI）的一个子集。} \label{fig_SDXX_2}
\end{figure}
深度学习革命始于基于卷积神经网络（CNN）和GPU的计算机视觉。

尽管通过反向传播训练的CNN已经存在了几十年，且基于GPU实现的神经网络（包括CNN）也已应用多年，[112][113] 但要在计算机视觉领域取得进展，还需要更快的GPU上的CNN实现。后来，随着深度学习的普及，专门用于深度学习的硬件以及算法优化得以发展。[114]

推动深度学习革命的一项关键进步是硬件的发展，特别是GPU。早期的一些工作可以追溯到2004年。[112][113] 在2009年，Raina、Madhavan 和 Andrew Ng 报告了一个由30块Nvidia GeForce GTX 280 GPU组成的集群训练的1亿参数深度信念网络，这是基于GPU的深度学习的早期演示。他们报告称训练速度提高了多达70倍。[115]

2011年，Dan Ciresan、Ueli Meier、Jonathan Masci、Luca Maria Gambardella 和 Jürgen Schmidhuber 开发的名为 DanNet 的卷积神经网络（CNN）[116][117] 首次在视觉模式识别竞赛中达到了超越人类的表现，其性能比传统方法高出3倍。[9] 随后，它还赢得了更多的竞赛。[118][119] 他们还展示了基于GPU的最大池化CNN如何显著提升性能。[3]

2012年，Andrew Ng 和 Jeff Dean 开发了一种前馈神经网络（FNN），能够仅通过观看从YouTube视频中提取的无标签图像学习识别更高层次的概念，例如猫。[120]

2012年10月，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 开发的 AlexNet[4] 在大规模 ImageNet 竞赛中以显著优势击败了浅层机器学习方法。这之后，增量改进包括由 Karen Simonyan 和 Andrew Zisserman 开发的 VGG-16 网络[121] 以及 Google 的 Inceptionv3。[122]

图像分类的成功随后扩展到更具挑战性的任务，即为图像生成描述（字幕），这通常是 CNN 和 LSTM 的结合使用。[123][124][125]

到2014年，深度学习的最新进展是训练“非常深的神经网络”，网络深度达到20至30层。[126] 但堆叠过多的层数会导致训练准确率的急剧下降，[127] 这被称为“退化问题”。[128] 在2015年，为训练非常深的网络开发了两种技术：2015年5月发布的 Highway Network，以及2015年12月发布的残差神经网络（ResNet）。[129] ResNet 的行为类似于一个开放门控的 Highway 网络。

大约在同一时间，深度学习开始对艺术领域产生影响。早期的例子包括 Google DeepDream（2015）和神经风格迁移（2015），[130] 这两者都基于预训练的图像分类神经网络，例如 VGG-19。

生成对抗网络（GAN）由 Ian Goodfellow 等人于2014年提出，[131]（基于 Jürgen Schmidhuber 的人工好奇原则[74][76]），在2014至2018年期间成为生成建模的最新技术。Nvidia 的 StyleGAN（2018）基于 Tero Karras 等人提出的 Progressive GAN，[133] 实现了卓越的图像质量。在该方法中，GAN 的生成器以金字塔方式从小规模逐步扩展到大规模。GAN 的图像生成取得了广泛的成功，并引发了关于深度伪造（deepfakes）的讨论。[134] 自那时以来，扩散模型（2015）[135] 在生成建模中超越了 GAN，例如 DALL·E 2（2022）和 Stable Diffusion（2022）等系统。

2015年，Google 的语音识别通过基于 LSTM 的模型提高了49\%，并通过智能手机上的 Google Voice Search 提供了这一功能。[136][137]

深度学习已经成为多个学科中最先进系统的一部分，尤其是在计算机视觉和自动语音识别（ASR）领域。诸如 TIMIT（ASR）和 MNIST（图像分类）等常用评估集的结果，以及一系列大词汇量语音识别任务的结果，均稳步提升。[104][138] 在语音识别方面，卷积神经网络（CNN）被 LSTM 超越，[137][139][140][141] 但在计算机视觉领域，CNN 更为成功。

Yoshua Bengio、Geoffrey Hinton 和 Yann LeCun 因“在概念和工程上的突破，使深度神经网络成为计算的关键组成部分”而被授予2018年图灵奖。[142]
\subsection{神经网络}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/749b3524c551dca7.png}
\caption{对象检测中训练神经网络的简化示例：网络通过多张已知包含海星和海胆的图像进行训练，这些图像与表示视觉特征的“节点”相关联。海星与环状纹理和星形轮廓匹配，而大多数海胆与条纹纹理和椭圆形状匹配。然而，一例具有环状纹理的海胆会在它们之间形成一个弱关联的权重。} \label{fig_SDXX_3}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/380ed3697dbafa56.png}
\caption{网络对输入图像（左图）的一次后续运行：[143] 网络正确检测到了海星。然而，由于环状纹理和海胆之间的弱关联关系，其中一个中间节点向海胆传递了一个弱信号。此外，一个未在训练中出现的贝壳由于具有椭圆形状，也向海胆输出传递了一个弱信号。这些弱信号可能导致海胆的误检（假阳性）结果。  实际上，纹理和轮廓不会由单个节点表示，而是由多个节点的关联权重模式表示。} \label{fig_SDXX_4}
\end{figure}
人工神经网络（ANN）或连接主义系统是受动物大脑中生物神经网络启发的计算系统。这类系统通过学习示例来完成任务，并逐步提高能力，通常无需特定于任务的编程。例如，在图像识别中，它们可以通过分析手动标记为“猫”或“非猫”的示例图像来学习识别包含猫的图像，并利用分析结果来识别其他图像中的猫。它们主要被用于那些难以用基于规则的传统计算机算法表达的应用场景。

人工神经网络基于一组称为人工神经元的连接单元（类似于生物大脑中的神经元）。神经元之间的每个连接（突触）都可以将信号传递到另一个神经元。接收信号的（突触后）神经元可以处理信号，然后向与其连接的下游神经元发送信号。神经元可能具有状态，通常用介于0到1之间的实数表示。神经元和突触还可能具有权重，这些权重在学习过程中会发生变化，从而增强或减弱其向下游发送信号的强度。

通常，神经元以层的形式组织。不同的层可能对输入进行不同种类的变换。信号从第一层（输入层）开始，经过多层传递，最终到达最后一层（输出层），有时可能会多次遍历这些层。

神经网络方法的最初目标是以与人脑相同的方式解决问题。随着时间的推移，研究重点转向匹配特定的心理能力，导致了一些偏离生物学的创新，如反向传播（backpropagation），即反向传递信息并调整网络以反映该信息。

神经网络已被用于多种任务，包括计算机视觉、语音识别、机器翻译、社交网络过滤、棋类和电子游戏，以及医疗诊断。

截至2017年，神经网络通常包含几千到几百万个单元以及数百万个连接。尽管这些数字比人类大脑中的神经元数量低几个数量级，这些网络在许多任务上的表现已经超越人类水平，例如面部识别或围棋游戏[144]。
\subsubsection{深度神经网络} 
深度神经网络（DNN）是一种人工神经网络，其输入层和输出层之间有多个隐藏层。[7][9] 虽然神经网络的类型多种多样，但它们总是由相同的组成部分构成：神经元、突触、权重、偏置和函数。[145] 这些组成部分整体上以一种模拟人脑功能的方式运行，并且可以像其他机器学习算法一样进行训练。[需要引用]  

例如，一个训练用来识别犬种的DNN会分析给定图像，并计算图像中狗属于某种犬种的概率。用户可以查看结果，并选择显示超过某个阈值的概率，返回相应的标签。每一个这样的数学操作都被视为一层，[146] 而复杂的DNN包含许多层，因此被称为“深度”网络。  

DNN能够建模复杂的非线性关系。DNN架构生成组合模型，其中对象被表达为原始特征的分层组合。[147] 额外的层能够从低层组合特征，从而使用较少的单元建模复杂数据，与性能相似的浅层网络相比更高效。[7] 例如，已证明稀疏多元多项式用DNN近似比用浅层网络要容易得多，其难度呈指数级降低。[148]  

深度架构包括几种基本方法的多种变体。每种架构在特定领域都取得了成功。除非在相同的数据集上进行评估，否则不总能比较多个架构的性能。[146]  

DNN通常是前馈网络，其中数据从输入层流向输出层，没有回环。一开始，DNN会创建一个虚拟神经元的映射，并为它们之间的连接分配随机数值或“权重”。权重和输入值相乘，返回一个介于0到1之间的输出。如果网络未能准确识别某一模式，一个算法会调整权重。[149] 通过这种方式，算法可以使某些参数更具影响力，直到确定正确的数学操作以完全处理数据。  

循环神经网络（RNN）允许数据在任意方向流动，用于语言建模等应用。[150][151][152][153][154] 长短期记忆（LSTM）在这些应用中尤其有效。[155][156]  

卷积神经网络（CNN）用于计算机视觉领域。[157] CNN还被应用于自动语音识别（ASR）的声学建模。[158]  
\subsubsection{挑战}
与人工神经网络（ANN）一样，未经深思熟虑训练的深度神经网络（DNN）可能会出现许多问题。两个常见问题是过拟合和计算时间过长。

由于增加了抽象层，DNN 容易发生过拟合，这使其能够建模训练数据中的罕见依赖关系。可以在训练期间采用正则化方法来对抗过拟合，例如 Ivakhnenko 的单元剪枝[41]、权重衰减（
ℓ
2
正则化，通常称为 \( \ell_2 \) 正则化）或稀疏性（
ℓ
1
正则化，通常称为 \( \ell_1 \) 正则化）。[159] 或者，可以采用随机失活（dropout）正则化，在训练期间随机从隐藏层中省略一些单元。这有助于排除罕见依赖关系。[160] 最近的一项有趣进展是研究通过估计所建模任务的内在复杂性来开发仅具足够复杂度的模型。这种方法已成功应用于多变量时间序列预测任务，例如交通预测。[161] 最后，可以通过裁剪、旋转等数据增强方法扩充较小的训练集，以减少过拟合的可能性。[162]

DNN需要考虑许多训练参数，例如规模（层数和每层的单元数）、学习率和初始权重。在参数空间中搜索最佳参数可能由于时间和计算资源的成本而不可行。各种技巧可以加快计算，例如批处理（一次对多个训练样本而非单个样本计算梯度）。[163] 多核架构（例如GPU或Intel Xeon Phi）的强大处理能力显著加快了训练速度，因为这些架构非常适合矩阵和向量计算。[164][165]

另外，工程师可能会寻找其他类型的神经网络，这些网络具有更简单和收敛的训练算法。例如，小脑模型关节控制器（CMAC）是一种神经网络。它不需要学习率或随机初始化权重。使用新的数据批次时，训练过程可以在一步内保证收敛，且训练算法的计算复杂度与涉及的神经元数量呈线性关系。[166][167]
\subsection{硬件}
自2010年代以来，机器学习算法和计算机硬件的进步促使了更高效的方法，用于训练包含多个非线性隐藏单元和非常大输出层的深度神经网络。[168] 到2019年，图形处理单元（GPU），通常带有AI专用增强功能，取代了中央处理单元（CPU），成为训练大规模商业云AI的主导方法。[169] OpenAI 估计，从 AlexNet（2012）到 AlphaZero（2017），最大深度学习项目中所使用的硬件计算量增长了30万倍，并显示出每3.4个月计算量翻倍的趋势线。[170][171]

专门设计用于加速深度学习算法的电子电路被称为深度学习处理器。这些处理器包括华为手机中的神经处理单元（NPU），[172] 以及云计算服务器中的张量处理单元（TPU），例如 Google Cloud Platform 提供的 TPU。[173] Cerebras Systems 也开发了专用于处理大型深度学习模型的系统 CS-2，其基于业内最大规模的处理器——第二代晶圆级引擎（WSE-2）。[174][175]

原子级薄半导体被认为是节能深度学习硬件的有前途的方向，在这种硬件中，相同的基础设备结构既可用于逻辑操作，也可用于数据存储。2020年，Marega 等人发表了关于利用大面积有源通道材料开发基于浮栅场效应晶体管（FGFET）的存储器逻辑设备和电路的实验结果。[176]

2021年，J. Feldmann 等人提出了一种用于并行卷积处理的集成光子硬件加速器。[177] 研究者指出，与电子设备相比，集成光子学有两个关键优势：（1）通过波长分复用结合频率梳实现大规模并行数据传输；（2）极高的数据调制速度。[177] 他们的系统每秒可以执行数万亿次乘加操作，显示出集成光子技术在数据密集型AI应用中的潜力。[177]
\subsection{应用}
\subsubsection{自动语音识别}  
大规模自动语音识别是深度学习最早也是最令人信服的成功案例之一。长短期记忆（LSTM）循环神经网络（RNN）可以处理“非常深度学习”任务，[9] 这些任务涉及包含语音事件的多秒时间间隔，这些事件被数千个离散时间步分隔开，每个时间步约为10毫秒。带遗忘门的LSTM[156] 在某些任务上与传统语音识别器具有竞争力。[93]  

语音识别的初步成功基于小规模的识别任务，这些任务使用了 TIMIT 数据集。该数据集包含来自八种主要美式英语方言的630名发音者，每位发音者朗读10句话。[178] 数据集的小规模使得可以尝试多种配置。更重要的是，TIMIT任务关注音素序列识别，与单词序列识别不同，它允许使用弱音素双元语言模型。这使得语音识别中声学建模部分的强度更容易被分析。以下列出的错误率，包括这些早期结果，都是以音素错误率（PER）的百分比形式总结的，自1991年以来一直如此。
\begin{table}[ht]
\centering
\caption\label{SDXX}
\begin{tabular}{|c|c}
\hline
\textbf{方法}&\textbf{音素错误率（PER）(\%)}\\
\hline 随机初始化的 RNN[179] & 26.1\\
\hline 贝叶斯三音素 GMM-HMM & 25.6\\
\hline 隐轨迹（生成式）模型 & 24.8\\
\hline 单音素随机初始化的 DNN & 23.4\\
\hline 单音素 DBN-DNN & 22.4\\
\hline 三音素 GMM-HMM（带 BMMI 训练）& 21.7\\
\hline 单音素 DBN-DNN（基于 fbank）& 20.7\\
\hline 卷积 DNN[180] & 20.0\\
\hline 带异构池化的卷积 DNN & 18.7\\
\hline DNN/CNN/RNN 集成模型[181] & 18.3\\
\hline 双向 LSTM & 17.8 \\
\hline 分层卷积深度 Maxout 网络[182] & 16.5 \\
\hline 
\end{tabular}
\end{table}
深度神经网络（DNN）在20世纪90年代后期首次用于说话人识别，并在2009至2011年左右应用于语音识别，同时长短期记忆网络（LSTM）在2003至2007年间逐步发展，这些进展加速了以下八个主要领域的发展：[23][108][106]  
\begin{itemize}
\item DNN 的扩展和加速训练与解码  
\item 序列判别性训练  
\item 通过深度模型进行特征处理，并深入理解其底层机制  
\item DNN 和相关深度模型的自适应  
\item 基于 DNN 和相关深度模型的多任务学习和迁移学习  
\item 卷积神经网络（CNN）及其如何设计以充分利用语音领域知识  
\item 循环神经网络（RNN）及其丰富的 LSTM 变体  
\item 其他类型的深度模型，包括基于张量的模型以及生成/判别综合深度模型
\end{itemize}  
所有主要的商业语音识别系统（例如，微软 Cortana、Xbox、Skype Translator，亚马逊 Alexa，Google Now，苹果 Siri，百度和科大讯飞语音搜索，以及一系列 Nuance 语音产品等）都基于深度学习。[23][183][184]
\subsubsection{图像识别}
一个常用的图像分类评估数据集是 MNIST 数据库。MNIST 由手写数字组成，包括 60,000 个训练样本和 10,000 个测试样本。与 TIMIT 数据集类似，它的小规模使得用户能够测试多种配置。关于该数据集的详细结果列表可用。[185]

基于深度学习的图像识别已经达到了“超人”水平，产生了比人类竞赛者更准确的结果。第一次出现这种情况是在 2011 年对交通标志的识别中，2014 年则是在对人脸的识别中。[186][187]

深度学习训练的车辆现在能够解读 360° 摄像头视角。[188] 另一个例子是面部畸形新型分析（FDNA），它用于分析与大型基因综合症数据库相关的人类畸形病例。
\subsubsection{视觉艺术处理}
与图像识别进展密切相关的是深度学习技术在各种视觉艺术任务中的日益应用。深度神经网络（DNN）已证明其在以下方面的能力，例如：
\begin{itemize}
\item 识别特定绘画的艺术风格时期[189][190]  
\item 神经风格迁移（Neural Style Transfer）：捕捉给定艺术作品的风格，并以视觉上令人愉悦的方式将其应用于任意照片或视频[189][190]  
\item 基于随机视觉输入字段生成引人注目的图像。[189][190]
\end{itemize}
\subsubsection{自然语言处理}  
自2000年代初以来，神经网络就已被用于实现语言模型。[150] 长短期记忆（LSTM）帮助改进了机器翻译和语言建模。[151][152][153]  

该领域的其他关键技术包括负采样[191] 和词嵌入（word embedding）。词嵌入（如 word2vec）可以被看作是深度学习架构中的表示层，它将一个原子词转换为该词相对于数据集中其他词的位置表示；该位置表示为向量空间中的一个点。将词嵌入作为 RNN 的输入层使网络能够使用有效的组合向量语法解析句子和短语。组合向量语法可以被视为由 RNN 实现的概率上下文无关语法（PCFG）。[192] 基于词嵌入构建的递归自动编码器可以评估句子相似性并检测改写。[192] 深度神经网络架构在短语解析、[193] 情感分析、[194] 信息检索、[195][196] 语音理解、[197] 机器翻译、[151][198] 上下文实体链接、[198] 写作风格识别、[199] 命名实体识别（标记分类）、[200] 文本分类等任务中表现出最佳效果。[201]  

最近的发展将词嵌入推广到了句子嵌入。  

Google 翻译（GT）使用了一个大型的端到端长短期记忆（LSTM）网络。[202][203][204][205] Google 神经机器翻译（GNMT）采用了基于示例的机器翻译方法，其中系统通过“从数百万示例中学习”。[203] 它一次翻译“整句，而不是片段”。Google 翻译支持一百多种语言。[203] 该网络编码了“句子的语义，而不仅仅是记住短语到短语的翻译”。[203][206] GT 在大多数语言对之间使用英语作为中间语言。[206]
\subsubsection{药物研发与毒理学}  
大比例的候选药物未能通过监管审批。这些失败通常由以下原因引起：疗效不足（目标效应不足）、不希望的相互作用（非目标效应），或未预见的毒性效应。[207][208] 研究探索了利用深度学习预测生物分子目标、[209][210] 非目标效应以及环境化学物质（如营养成分、家用产品和药物）的毒性效应。[211][212][213]  

AtomNet 是一种基于结构的深度学习系统，用于合理的药物设计。[214] AtomNet 被用于预测针对疾病目标（如埃博拉病毒[215] 和多发性硬化症[216][215]）的新型候选生物分子。  

2017年，图神经网络首次被用于预测大型毒理学数据集中分子的各种特性。[217] 2019年，生成神经网络被用来生成分子，并在实验中成功验证到小鼠实验阶段。[218][219]  
\subsubsection{客户关系管理}  
深度强化学习已被用于估算直接营销行动的潜在价值，这些行动以 RFM（最近一次消费、消费频率、消费金额）变量定义。估算的价值函数被证明可以自然地解释为客户生命周期价值（CLV）。[220]
\subsubsection{推荐系统}  
推荐系统使用深度学习为基于内容的音乐和期刊推荐中的潜在因子模型提取有意义的特征。[221][222] 多视角深度学习被应用于从多个领域学习用户偏好。[223] 该模型采用混合协同过滤和基于内容的方法，在多个任务中增强了推荐效果。  
\subsubsection{生物信息学}  
自动编码器人工神经网络（ANN）被用于生物信息学中，以预测基因本体注释和基因功能关系。[224]  

在医学信息学中，深度学习被用于基于可穿戴设备数据预测睡眠质量，[225] 以及基于电子健康记录数据预测健康并发症。[226]  

深度神经网络在根据组成蛋白质的氨基酸序列预测蛋白质结构方面表现出了无与伦比的性能。2020年，基于深度学习的系统 AlphaFold 实现了显著高于所有先前计算方法的准确性。[227][228]  
\subsubsection{深度神经网络估算}  
深度神经网络可用于估算随机过程的熵，被称为神经联合熵估算器（Neural Joint Entropy Estimator, NJEE）。[229] 这种估算提供了输入随机变量对独立随机变量影响的洞察。在实践中，DNN 被训练为一个分类器，将输入向量或矩阵 \(X\) 映射为随机变量 \(Y\) 的可能类别上的输出概率分布，给定输入 \(X\)。例如，在图像分类任务中，NJEE 将像素颜色值的向量映射为可能的图像类别上的概率分布。实际中，\(Y\) 的概率分布通过一个 Softmax 层获得，其节点数量等于 \(Y\) 的字母表大小。  

NJEE 使用连续可微的激活函数，使得满足通用逼近定理的条件。研究表明，该方法提供了一种强一致的估算器，在字母表大小较大时，其性能优于其他方法。[229]  
\subsubsection{医学图像分析}  
深度学习已被证明在医学应用中具有竞争力，例如癌细胞分类、病变检测、器官分割和图像增强。[230][231] 现代深度学习工具展现了在检测各种疾病方面的高准确性，并帮助专业人员提高诊断效率。[232][233]  
\subsubsection{移动广告}  
为移动广告找到合适的受众始终是一个挑战，因为在创建和使用目标受众群体之前，需要考虑和分析许多数据点。[234] 深度学习已被用于解释维度多、数据量大的广告数据集。在请求/投放/点击的互联网广告循环中，会收集许多数据点。这些信息可以作为机器学习的基础，用于改进广告选择。  
\subsubsection{图像修复}  
深度学习已成功应用于去噪、超分辨率、修复缺失部分（inpainting）和电影着色等逆问题。[235] 这些应用包括诸如“Shrinkage Fields for Effective Image Restoration”[236] 之类的学习方法，该方法基于图像数据集进行训练，以及 Deep Image Prior，这种方法直接在需要修复的图像上训练。  
\subsubsection{金融欺诈检测 } 
深度学习被成功应用于金融欺诈检测、税收逃避检测[237] 和反洗钱。[238]  
\subsubsection{材料科学}  
2023年11月，Google DeepMind 和劳伦斯伯克利国家实验室的研究人员宣布开发了一种名为 GNoME 的 AI 系统。该系统通过在相对较短的时间内发现了超过 200 万种新材料，对材料科学领域作出了贡献。GNoME 利用深度学习技术高效探索潜在的材料结构，在识别稳定的无机晶体结构方面取得了显著进展。系统预测通过自主机器人实验得到了验证，显示出 71\% 的成功率。这些新发现的材料数据通过 Materials Project 数据库公开，为研究人员提供了识别具有所需性能材料的机会，以用于各种应用。这一发展对科学发现和 AI 在材料科学研究中的整合具有重要意义，可能加速材料创新并降低产品开发成本。AI 和深度学习的应用表明有可能最小化甚至消除手动实验室实验，使科学家能够更专注于独特化合物的设计和分析。[239][240][241]  
\subsubsection{军事}  
美国国防部将深度学习应用于通过观察训练机器人完成新任务。[242]  
\subsubsection{偏微分方程}  
物理信息神经网络（Physics Informed Neural Networks, PINNs）已被用于以数据驱动的方式求解偏微分方程的正问题和逆问题。[243] 例如，基于 Navier-Stokes 方程重建流体流动。使用物理信息神经网络无需传统计算流体动力学（CFD）方法中常需的昂贵网格生成。[244][245]  
\subsubsection{深度向后随机微分方程方法}  
深度向后随机微分方程（Deep Backward Stochastic Differential Equation, BSDE）方法是一种结合深度学习与向后随机微分方程的数值方法。这种方法在解决金融数学中的高维问题时特别有用。通过利用深度神经网络强大的函数逼近能力，深度 BSDE 方法解决了传统数值方法在高维场景中面临的计算挑战。传统方法（如有限差分法或蒙特卡洛模拟）常因维度灾难而在计算成本上呈指数增长。而深度 BSDE 方法通过深度神经网络近似高维偏微分方程（PDE）的解，有效降低了计算负担。[246]  

此外，将物理信息神经网络（PINNs）集成到深度 BSDE 框架中进一步增强了其能力，直接将底层物理定律嵌入神经网络架构中。这确保了解不仅符合数据，还遵守控制随机微分方程的约束条件。PINNs 在利用深度学习能力的同时，尊重物理模型的约束，为金融数学问题提供了更准确且可靠的解。  
\subsubsection{图像重建}  
图像重建是通过图像相关测量重建底层图像的过程。多项研究表明，在各种应用中（例如光谱成像[247] 和超声成像[248]），深度学习方法相比于传统解析方法表现更优越。  
\subsubsection{天气预测}  
传统的天气预测系统需要求解非常复杂的偏微分方程组。GraphCast 是一种基于深度学习的模型，训练于长时间的天气数据历史，用于预测天气模式的变化。它能够在不到一分钟内以非常精细的水平预测全球多达10天的天气条件，其精度与最先进的系统相当。[249][250]  
\subsubsection{表观遗传时钟}  
表观遗传时钟是一种可以用来测量年龄的生化测试。Galkin 等人使用深度神经网络训练了一种具有前所未有精确度的表观遗传年龄时钟，该模型基于超过 6,000 个血液样本。[251] 该时钟利用来自 1000 个 CpG 位点的信息，预测患有某些疾病的人群的生物年龄高于健康对照组，例如炎症性肠病（IBD）、额颞叶痴呆、卵巢癌和肥胖。该年龄时钟计划由 Insilico Medicine 的衍生公司 Deep Longevity 于 2021 年向公众发布。
\subsection{与人类认知和大脑发育的关系}  
深度学习与认知神经科学家在20世纪90年代初提出的一类大脑发育理论（特别是新皮质发育）密切相关。[252][253][254][255] 这些发育理论通过计算模型实现，是深度学习系统的前身。这些发育模型具有一个共同特性，即大脑中各种学习动态（例如神经生长因子波动）支持一种类似于深度学习模型中神经网络的自组织机制。与新皮质类似，神经网络使用分层过滤器，其中每一层从前一层（或操作环境）获取信息，然后将其输出（以及可能的原始输入）传递给其他层。这个过程产生了一组自组织的转换器，与其操作环境高度契合。1995年的描述中指出，“...婴儿的大脑似乎在所谓的营养因子波的影响下自行组织...大脑的不同区域依次连接，一层组织在另一层成熟之前完成发育，如此循环，直到整个大脑发育成熟”。[256]  

从神经生物学的角度，研究者采用了多种方法来探讨深度学习模型的合理性。一方面，提出了几种反向传播算法的变体，以提高其处理的现实性。[257][258] 另一方面，一些研究者认为，无监督深度学习形式（如基于分层生成模型和深度信念网络的模型）可能更接近生物现实。[259][260] 在这方面，生成式神经网络模型与大脑皮层中基于采样处理的神经生物学证据有关。[261]  

尽管尚未建立人类大脑组织与深度网络中的神经编码之间的系统性比较，但已有一些类比被报道。例如，深度学习单元执行的计算可能类似于实际神经元[262] 和神经元群体的计算。[263] 同样，深度学习模型生成的表示与在灵长类视觉系统中测量的表示相似，[264] 无论是在单个神经元[265] 还是在神经群体[266] 的层面上。
\subsection{商业活动}  
Facebook 的 AI 实验室执行自动任务，例如自动为上传的图片标注其中人物的姓名。[267]  

Google 的 DeepMind Technologies 开发了一个系统，能够仅使用像素作为数据输入学习如何玩 Atari 视频游戏。2015年，他们展示了 AlphaGo 系统，该系统学习围棋并达到足以击败职业围棋选手的水平。[268][269][270] Google 翻译使用神经网络进行超过100种语言之间的翻译。  

2017年，Covariant.ai 创立，专注于将深度学习集成到工厂中。[271]  

2008年，得克萨斯大学奥斯汀分校（UT）的研究人员开发了一种名为 **TAMER**（Training an Agent Manually via Evaluative Reinforcement）的机器学习框架，该框架提出了通过与人类导师交互学习任务的新方法。[242] 最初开发为 TAMER，2018年，美国陆军研究实验室（ARL）和 UT 的研究人员合作引入了一种新算法，称为 **Deep TAMER**。Deep TAMER 使用深度学习，使机器人能够通过观察学习新任务。[242] 使用 Deep TAMER，一个机器人可以与人类训练师一起学习任务，观看视频流或亲自观察人类执行任务。随后，机器人在训练师的指导下练习任务，训练师提供诸如“做得好”或“做得不好”之类的反馈。[273]  
\subsection{批评与评论}  
深度学习吸引了来自计算机科学领域之外的一些批评和评论。
\subsubsection{理论} 
一个主要的批评集中在某些方法缺乏理论支持。[274] 在最常见的深度架构中，学习是通过已被充分理解的梯度下降实现的。然而，关于其他算法的理论（如对比散度）则不够明确，例如：“它是否收敛？如果收敛，其速度如何？它在逼近什么？” 深度学习方法通常被视为一个黑箱，大多数验证是通过经验而非理论进行的。[275]  

另一些人指出，深度学习应该被视为实现强人工智能的一步，而不是包罗万象的解决方案。尽管深度学习方法非常强大，但它们仍然缺乏实现这一目标所需的许多功能。心理学家 Gary Marcus 指出：  

现实来看，深度学习只是构建智能机器这一更大挑战的一部分。这些技术缺乏表示因果关系的方法（……），无法显而易见地进行逻辑推理，也远未能整合抽象知识，例如关于对象是什么、它们的用途及其典型用法的信息。最强大的人工智能系统（如 Watson）……将深度学习技术仅作为一个非常复杂的技术组合中的一个元素，从贝叶斯推断的统计技术到演绎推理不等。[276]  

在进一步探讨艺术敏感性可能存在于认知层次较低部分的观点时，一组关于深度（20-30层）神经网络内部状态的图形表示研究显示了视觉吸引力，这些网络试图在基本随机数据中识别其训练的图像。[277] 研究公告原文获得了超过 1,000 条评论，并一度成为《卫报》[278] 网站上访问量最高的文章。  
\subsubsection{错误}  
一些深度学习架构表现出问题行为，[279] 例如自信地将无法识别的图像归类为普通图像的已知类别（2014年）[280]，以及将正确分类的图像因微小扰动错误分类（2013年）。[281] Goertzel 假设，这些行为是由于内部表示的局限性，并认为这些局限性会阻碍它们融入异质多组件人工通用智能（AGI）架构。[279] 这些问题可能通过内部形成与观察实体和事件的图像语法分解同源的状态的深度学习架构来解决。[282] 从训练数据中学习语法（视觉或语言）等同于限制系统在语法生成规则的术语中进行常识推理，这是人类语言习得[283] 和人工智能（AI）的基本目标。[284]  
\subsubsection{网络安全威胁}  
随着深度学习从实验室走向现实，研究和经验表明人工神经网络（ANNs）容易受到攻击和欺骗。[285] 通过识别这些系统使用的模式，攻击者可以修改输入，使 ANN 发现人类观察者无法识别的匹配。例如，攻击者可以对图像进行微妙的更改，使得 ANN 发现匹配，而图像在人类看来与目标完全不同。这种操纵被称为“对抗性攻击”。[286]  

2016年，研究人员使用一个 ANN 通过试错方式修改图像以识别另一个 ANN 的焦点，从而生成欺骗它的图像。这些修改后的图像在人类眼中与原图没有差别。另一个研究小组展示了如何使用打印的篡改图像再拍摄来欺骗图像分类系统。[287] 一种防御方法是反向图像搜索，将可能的假图像提交到如 TinEye 之类的网站，寻找其其他实例。一种改进是仅使用图像的一部分进行搜索，以识别该部分可能来自的图像。[288]  

另一个研究小组展示了某些“迷幻风格”的眼镜可以欺骗面部识别系统，让其将普通人识别为名人，可能允许一个人冒充另一个人。2017年，研究人员通过在停车标志上添加贴纸，导致 ANN 错误分类它们。[287]  

然而，ANNs 也可以进一步训练以检测欺骗尝试，可能导致攻击者与防御者之间的军备竞赛，类似于目前定义恶意软件防御行业的竞争。ANNs 已被训练用来击败基于 ANN 的反恶意软件，通过反复攻击防御，使用遗传算法不断改变恶意软件，直到欺骗反恶意软件的同时保持其破坏目标的能力。[287]  

2016年，另一个研究小组展示了某些声音可以使 Google Now 语音命令系统打开特定网页，并假设这可能成为进一步攻击的跳板（例如，打开承载恶意软件的网页）。[287]  

在“数据投毒”中，虚假数据被持续偷偷加入机器学习系统的训练集中，阻止其掌握任务。[287]  
\subsubsection{数据收集伦理}  
使用监督学习训练的深度学习系统通常依赖于由人类创建和/或注释的数据。[289] 有人认为，不仅是低薪的点击工作（如亚马逊 Mechanical Turk 上的任务）经常被用于此目的，还包括通常不被识别为此类的隐性形式的人类微任务。[290] 哲学家 Rainer Mühlhoff 将生成训练数据的“人类微任务捕获”分为五种类型：(1) 游戏化（将注释或计算任务嵌入游戏流程中），(2) “陷阱和跟踪”（例如用于图像识别的 CAPTCHA 或 Google 搜索结果页面的点击跟踪），(3) 利用社交动机（例如通过 Facebook 标记人脸以获取标注的面部图像），(4) 信息挖掘（例如通过利用活动跟踪器等量化自我设备），(5) 点击工作。[290]
\subsection{参见} 
\begin{itemize}
\item 人工智能的应用  
\item 深度学习软件比较  
\item 压缩感知  
\item 可微分编程  
\item 回声状态网络  
\item 人工智能项目列表  
\item 液态状态机  
\item 机器学习研究数据集列表  
\item 储备计算  
\item 尺度空间与深度学习  
\item 稀疏编码  
\item 随机鹦鹉  
\item 拓扑深度学习 
\end{itemize} 
\subsection{参考文献}
\begin{enumerate}
\item Schulz, Hannes; Behnke, Sven (2012年11月1日). "Deep Learning". KI - Künstliche Intelligenz. 26 (4): 357–363. doi:10.1007/s13218-012-0198-z. ISSN 1610-1987. S2CID 220523562.  
\item LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). "Deep Learning" (PDF). Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442. S2CID 3074096.  
\item Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). "Multi-column deep neural networks for image classification". 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8. S2CID 2161592.  
\item Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey (2012). "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Archived (PDF) from the original on 2017-01-10. Retrieved 2017-05-24.  
\item "Google's AlphaGo AI wins three-match series against the world's best Go player". TechCrunch. 2017年5月25日. Archived from the original on 2018年6月17日. Retrieved 2018年6月17日.  
\item "Study urges caution when comparing neural networks to the brain". MIT News | Massachusetts Institute of Technology. 2022-11-02. Retrieved 2023-12-06.  
\item Bengio, Yoshua (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006. S2CID 207178999. Archived from the original (PDF) on 4 March 2016. Retrieved 3 September 2015.  
\item Bengio, Y.; Courville, A.; Vincent, P. (2013). "Representation Learning: A Review and New Perspectives". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. doi:10.1109/tpami.2013.50. PMID 23787338. S2CID 393948.  
\item Schmidhuber, J. (2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509.  
\item Shigeki, Sugiyama (2019年4月12日). Human Behavior and Another Kind in Consciousness: Emerging Research and Opportunities: Emerging Research and Opportunities. IGI Global. ISBN 978-1-5225-8218-2.  
\item Bengio, Yoshua; Lamblin, Pascal; Popovici, Dan; Larochelle, Hugo (2007). Greedy layer-wise training of deep networks (PDF). Advances in neural information processing systems. pp. 153–160. Archived (PDF) from the original on 2019-10-20. Retrieved 2019-10-06.  
\item Hinton, G.E. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.  
\item Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory. Online Archived 2016-04-19 at the Wayback Machine  
\item Aizenberg, I.N.; Aizenberg, N.N.; Vandewalle, J. (2000). Multi-Valued and Universal Binary Neurons. Science & Business Media. doi:10.1007/978-1-4757-3115-6. ISBN 978-0-7923-7824-2. Retrieved 27 December 2023.  
\item Co-evolving recurrent neurons learn deep memory POMDPs. Proc. GECCO, Washington, D. C., pp. 1795–1802, ACM Press, New York, NY, USA, 2005.  
\item Fradkov, Alexander L. (2020-01-01). "Early History of Machine Learning". IFAC-PapersOnLine. 21st IFAC World Congress. 53 (2): 1385–1390. doi:10.1016/j.ifacol.2020.12.1888. ISSN 2405-8963. S2CID 235081987.  
\item Cybenko (1989). "Approximations by superpositions of sigmoidal functions" (PDF). Mathematics of Control, Signals, and Systems. 2 (4): 303–314. Bibcode:1989MCSS....2..303C. doi:10.1007/bf02551274. S2CID 3958369. Archived from the original (PDF) on 10 October 2015.  
\item Hornik, Kurt (1991). "Approximation Capabilities of Multilayer Feedforward Networks". Neural Networks. 4 (2): 251–257. doi:10.1016/0893-6080(91)90009-t. S2CID 7343126.  
\item Haykin, Simon S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall. ISBN 978-0-13-273350-2.  
\item Hassoun, Mohamad H. (1995). Fundamentals of Artificial Neural Networks. MIT Press. p. 48. ISBN 978-0-262-08239-6.  
\item Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). The Expressive Power of Neural Networks: A View from the Width. Neural Information Processing Systems, 6231-6239. Archived 2019-02-13 at the Wayback Machine.
\item Orhan, A. E.; Ma, W. J. (2017). "Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback". Nature Communications. 8 (1): 138. Bibcode:2017NatCo...8..138O. doi:10.1038/s41467-017-00181-8. PMC 5527101. PMID 28743932.
\item Deng, L.; Yu, D. (2014). "Deep Learning: Methods and Applications" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039. Archived (PDF) from the original on 2016-03-14. Retrieved 2014-10-18.
\item Murphy, Kevin P. (24 August 2012). Machine Learning: A Probabilistic Perspective. MIT Press. ISBN 978-0-262-01802-9.
\item Fukushima, K. (1969). "Visual feature extraction by a multilayered network of analog threshold elements". IEEE Transactions on Systems Science and Cybernetics. 5 (4): 322–333. doi:10.1109/TSSC.1969.300225.
\item Sonoda, Sho; Murata, Noboru (2017). "Neural network with unbounded activation functions is universal approximator". Applied and Computational Harmonic Analysis. 43 (2): 233–268. arXiv:1505.03654. doi:10.1016/j.acha.2015.12.005. S2CID 12149203.
\item Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning (PDF). Springer. ISBN 978-0-387-31073-2. Archived (PDF) from the original on 2017-01-11. Retrieved 2017-08-06.
\item "bibliotheca Augustana". www.hs-augsburg.de.
\item Brush, Stephen G. (1967). "History of the Lenz-Ising Model". Reviews of Modern Physics. 39 (4): 883–893. Bibcode:1967RvMP...39..883B. doi:10.1103/RevModPhys.39.883.
\item Amari, Shun-Ichi (1972). "Learning patterns and pattern sequences by self-organizing nets of threshold elements". IEEE Transactions. C (21): 1197–1206.
\item Schmidhuber, Jürgen (2022). "Annotated History of Modern AI and Deep Learning". arXiv:2212.11279 [cs.NE].
\item Hopfield, J. J. (1982). "Neural networks and physical systems with emergent collective computational abilities". Proceedings of the National Academy of Sciences. 79 (8): 2554–2558. Bibcode:1982PNAS...79.2554H. doi:10.1073/pnas.79.8.2554. PMC 346238. PMID 6953413.
\item Nakano, Kaoru (1971). "Learning Process in a Model of Associative Memory". Pattern Recognition and Machine Learning. pp. 172–186. doi:10.1007/978-1-4615-7566-5_15. ISBN 978-1-4615-7568-9.
\item Nakano, Kaoru (1972). "Associatron-A Model of Associative Memory". IEEE Transactions on Systems, Man, and Cybernetics. SMC-2 (3): 380–388. doi:10.1109/TSMC.1972.4309133.
\item Turing, Alan (1948). "Intelligent Machinery". Unpublished (Later Published in Ince DC, Editor, Collected Works of AM Turing—Mechanical Intelligence, Elsevier Science Publishers, 1992).
\item Rosenblatt, F. (1958). "The perceptron: A probabilistic model for information storage and organization in the brain". Psychological Review. 65 (6): 386–408. doi:10.1037/h0042519. ISSN 1939-1471. PMID 13602029.
\item Rosenblatt, Frank (1962). Principles of Neurodynamics. Spartan, New York.
\item Joseph, R. D. (1960). Contributions to Perceptron Theory, Cornell Aeronautical Laboratory Report No. VG-11 96--G-7, Buffalo.
\item Ivakhnenko, A. G.; Lapa, V. G. (1967). Cybernetics and Forecasting Techniques. American Elsevier Publishing Co. ISBN 978-0-444-00020-0.
\item Ivakhnenko, A.G. (March 1970). "Heuristic self-organization in problems of engineering cybernetics". Automatica. 6 (2): 207–219. doi:10.1016/0005-1098(70)90092-0.
\item Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems" (PDF). IEEE Transactions on Systems, Man, and Cybernetics. SMC-1 (4): 364–378. doi:10.1109/TSMC.1971.4308320. Archived (PDF) from the original on 2017-08-29. Retrieved 2019-11-05.
\item Robbins, H.; Monro, S. (1951). "A Stochastic Approximation Method". The Annals of Mathematical Statistics. 22 (3): 400. doi:10.1214/aoms/1177729586.
\item Amari, Shun'ichi (1967). "A theory of adaptive pattern classifier". IEEE Transactions. EC (16): 279–307.
\item Ramachandran, Prajit; Barret, Zoph; Quoc, V. Le (October 16, 2017). "Searching for Activation Functions". arXiv:1710.05941 [cs.NE].
\item Fukushima, K. (1979). "Neural network model for a mechanism of pattern recognition unaffected by shift in position—Neocognitron". Trans. IECE (In Japanese). J62-A (10): 658–665. doi:10.1007/bf00344251. PMID 7370364. S2CID 206775608.
\item Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364. S2CID 206775608.
\item Leibniz, Gottfried Wilhelm Freiherr von (1920). The Early Mathematical Manuscripts of Leibniz: Translated from the Latin Texts Published by Carl Immanuel Gerhardt with Critical and Historical Notes (Leibniz published the chain rule in a 1676 memoir). Open court publishing Company. ISBN 9780598818461.
\item Kelley, Henry J. (1960). "Gradient theory of optimal flight paths". ARS Journal. 30 (10): 947–954. doi:10.2514/8.5282.
\item Linnainmaa, Seppo (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors (Masters) (in Finnish). University of Helsinki. p. 6–7.
\item Linnainmaa, Seppo (1976). "Taylor expansion of the accumulated rounding error". BIT Numerical Mathematics. 16 (2): 146–160. doi:10.1007/bf01931367. S2CID 122357351.
\item Ostrovski, G.M., Volin,Y.M., and Boris, W.W. (1971). On the computation of derivatives. Wiss. Z. Tech. Hochschule for Chemistry, 13:382–384.
\item Schmidhuber, Juergen (25 Oct 2014). "Who Invented Backpropagation?". IDSIA, Switzerland. Archived from the original on 30 July 2024. Retrieved 14 Sep 2024.
\item Werbos, Paul (1982). "Applications of advances in nonlinear sensitivity analysis" (PDF). System modeling and optimization. Springer. pp. 762–770. Archived (PDF) from the original on 14 April 2016. Retrieved 2 July 2017.
\item Werbos, Paul J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting. New York: John Wiley & Sons. ISBN 0-471-59897-6.
\item Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (October 1986). "Learning representations by back-propagating errors". Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0. ISSN 1476-4687.
\item Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams. "Learning Internal Representations by Error Propagation". David E. Rumelhart, James L. McClelland, and the PDP research group. (editors), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundation. MIT Press, 1986.
\item Waibel, Alex (December 1987). Phoneme Recognition Using Time-Delay Neural Networks (PDF). Meeting of the Institute of Electrical, Information and Communication Engineers (IEICE). Tokyo, Japan.
\item Alexander Waibel et al., Phoneme Recognition Using Time-Delay Neural Networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, Volume 37, No. 3, pp. 328–339, March 1989.
\item Zhang, Wei (1988). "Shift-invariant pattern recognition neural network and its optical architecture". Proceedings of the Annual Conference of the Japan Society of Applied Physics.
\item LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition", Neural Computation, 1, pp. 541–551, 1989.
\item Zhang, Wei (1990). "Parallel distributed processing model with local space-invariant interconnections and its optical architecture". Applied Optics. 29 (32): 4790–7. Bibcode:1990ApOpt..29.4790Z. doi:10.1364/AO.29.004790. PMID 20577468.
\item Zhang, Wei (1991). "Image processing of human corneal endothelium based on a learning network". Applied Optics. 30 (29): 4211–7. Bibcode:1991ApOpt..30.4211Z. doi:10.1364/AO.30.004211. PMID 20706526.
\item Zhang, Wei (1994). "Computerized detection of clustered microcalcifications in digital mammograms using a shift-invariant artificial neural network". Medical Physics. 21 (4): 517–24. Bibcode:1994MedPh..21..517Z. doi:10.1118/1.597177. PMID 8058017.
\item LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). "Gradient-based learning applied to document recognition" (PDF). Proceedings of the IEEE. 86 (11): 2278–2324. CiteSeerX 10.1.1.32.9552. doi:10.1109/5.726791. S2CID 14542261. Retrieved October 7, 2016.
\item Jordan, Michael I. (1986). "Attractor dynamics and parallelism in a connectionist sequential machine". Proceedings of the Annual Meeting of the Cognitive Science Society. 8.
\item Elman, Jeffrey L. (March 1990). "Finding Structure in Time". Cognitive Science. 14 (2): 179–211. doi:10.1207/s15516709cog1402_1. ISSN 0364-0213.
\item Schmidhuber, Jürgen (April 1991). "Neural Sequence Chunkers" (PDF). TR FKI-148, TU Munich.
\item Schmidhuber, Jürgen (1992). "Learning complex, extended sequences using the principle of history compression (based on TR FKI-148, 1991)" (PDF). Neural Computation. 4 (2): 234–242. doi:10.1162/neco.1992.4.2.234. S2CID 18271205.
\item Schmidhuber, Jürgen (1993). Habilitation thesis: System modeling and optimization (PDF). [permanent dead link] Page 150 ff demonstrates credit assignment across the equivalent of 1,200 layers in an unfolded RNN.
\item S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen". Archived 2015-03-06 at the Wayback Machine. Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.
\item Hochreiter, S.; et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies". In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.
\item Sepp Hochreiter; Jürgen Schmidhuber (21 August 1995), Long Short Term Memory, Wikidata Q98967430
\item Gers, Felix; Schmidhuber, Jürgen; Cummins, Fred (1999). "Learning to forget: Continual prediction with LSTM". 9th International Conference on Artificial Neural Networks: ICANN '99. Vol. 1999. pp. 850–855. doi:10.1049/cp:19991218. ISBN 0-85296-721-7.
\item Schmidhuber, Jürgen (1991). "A possibility for implementing curiosity and boredom in model-building neural controllers". Proc. SAB'1991. MIT Press/Bradford Books. pp. 222–227.
\item Schmidhuber, Jürgen (2010). "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010)". IEEE Transactions on Autonomous Mental Development. 2 (3): 230–247. doi:10.1109/TAMD.2010.2056368. S2CID 234198.
\item Schmidhuber, Jürgen (2020). "Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991)". Neural Networks. 127: 58–66. arXiv:1906.04493. doi:10.1016/j.neunet.2020.04.008. PMID 32334341. S2CID 216056336.
\item Ackley, David H.; Hinton, Geoffrey E.; Sejnowski, Terrence J. (1985-01-01). "A learning algorithm for boltzmann machines". Cognitive Science. 9 (1): 147–169. doi:10.1016/S0364-0213(85)80012-4. ISSN 0364-0213.
\item Smolensky, Paul (1986). "Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory" (PDF). In Rumelhart, David E.; McLelland, James L. (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations. MIT Press. pp. 194–281. ISBN 0-262-68053-X.
\item Peter, Dayan; Hinton, Geoffrey E.; Neal, Radford M.; Zemel, Richard S. (1995). "The Helmholtz machine". Neural Computation. 7 (5): 889–904. doi:10.1162/neco.1995.7.5.889. hdl:21.11116/0000-0002-D6D3-E. PMID 7584891. S2CID 1890561.
\item Hinton, Geoffrey E.; Dayan, Peter; Frey, Brendan J.; Neal, Radford (1995-05-26). "The wake-sleep algorithm for unsupervised neural networks". Science. 268 (5214): 1158–1161. Bibcode:1995Sci...268.1158H. doi:10.1126/science.7761831. PMID 7761831. S2CID 871473.
\end{enumerate}