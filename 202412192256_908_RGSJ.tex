% 人工神经网络（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Neural_network_(machine_learning)}{相关文章}。

\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/9fddb6b4ea5a9fd3.png}
\caption{人工神经网络是一个互相连接的节点群，灵感来源于大脑中神经元的简化模型。在这里，每个圆形节点代表一个人工神经元，箭头代表从一个人工神经元的输出到另一个人工神经元输入的连接。} \label{fig_RGSJ_1}
\end{figure}
在机器学习中，神经网络（也称为人工神经网络或神经网，缩写为ANN或NN）是一种受到动物大脑生物神经网络结构和功能启发的模型。[1][2]

一个人工神经网络由连接的单元或节点组成，这些节点被称为人工神经元，粗略地模拟大脑中的神经元。这些神经元通过边（连接）相互连接，模拟大脑中的突触。每个人工神经元接收来自连接神经元的信号，然后处理这些信号并将结果传送给其他连接的神经元。这个“信号”是一个实数，每个神经元的输出通过某个非线性函数计算，该函数作用于输入的和，这个过程被称为激活函数。每个连接的信号强度由一个权重决定，权重会在学习过程中调整。

通常，神经元会被聚合成层。不同的层可能对其输入执行不同的变换。信号从第一层（输入层）传输到最后一层（输出层），可能会经过多个中间层（隐藏层）。如果一个网络至少有两个隐藏层，通常称其为深度神经网络。[3]

人工神经网络被用于各种任务，包括预测建模、自适应控制和解决人工智能中的问题。它们能够从经验中学习，并能够从复杂且看似不相关的信息集中推导结论。
\subsection{训练}
神经网络通常通过经验风险最小化进行训练。这种方法基于优化网络参数，以最小化预测输出与给定数据集中的实际目标值之间的差异或经验风险的理念。[4] 基于梯度的方法，如反向传播，通常用于估计网络的参数。[4] 在训练阶段，人工神经网络通过标记的训练数据进行学习，通过迭代更新参数以最小化定义的损失函数。[5] 这种方法使得网络能够对未见过的数据进行泛化。
\subsection{历史}  
\subsubsection{早期工作}  
今天的深度神经网络基于200多年前统计学的早期工作。最简单的前馈神经网络（FNN）是一个线性网络，它由一层输出节点组成，节点使用线性激活函数；输入通过一系列权重直接馈送到输出。在每个节点上，输入和权重的乘积之和会被计算出来。通过对这些计算的输出与给定目标值之间的均方误差进行最小化，调整权重。这种技术已经有两个多世纪的历史，被称为最小二乘法或线性回归。它由勒让德（1805年）和高斯（1795年）用于预测行星运动，通过找到一组点的最佳线性拟合。[7][8][9][10][11]  

历史上，像冯·诺依曼模型这样的数字计算机通过执行显式指令并通过多个处理器访问内存来操作。另一方面，一些神经网络起源于试图通过联结主义框架模拟生物系统中的信息处理。与冯·诺依曼模型不同，联结主义计算不将内存和处理分开。  

沃伦·麦卡洛克和沃尔特·皮茨（1943年）考虑了一个非学习型的神经网络计算模型。[13] 这个模型为神经网络研究分裂成两种不同的方向奠定了基础。一种方向专注于生物过程，而另一种方向则专注于神经网络在人工智能中的应用。

在20世纪40年代末，D.O. Hebb[14] 提出了一个基于神经可塑性机制的学习假设，这个假设后来被称为赫布学习（Hebbian learning）。它被用于许多早期的神经网络中，例如Rosenblatt的感知器和霍普菲尔德网络。Farley和Clark[15]（1954年）使用计算机模拟了一个赫布网络。其他神经网络计算机器由Rochester、Holland、Habit和Duda（1956年）创建。[16]  

1958年，心理学家Frank Rosenblatt描述了感知器，这是一种最早实现的人工神经网络之一，[17][18][19][20] 由美国海军研究办公室资助。[21] R.D. Joseph（1960年）[22] 提到Farley和Clark（1956年）开发的早期感知器类设备：[10] "麻省理工学院林肯实验室的Farley和Clark实际上在感知器类设备的开发上先于Rosenblatt。" 然而，他们“放弃了这个课题。”感知器引发了公众对人工神经网络研究的热情，促使美国政府大幅增加资金。这为计算机科学家对感知器能够模拟人类智能的乐观预期提供了支持，进一步推动了“人工智能的黄金时代”。[23]  

最初的感知器没有自适应的隐藏单元。然而，Joseph（1960年）[22] 也讨论了具有自适应隐藏层的多层感知器。Rosenblatt（1962年）[24]: 第16节 引用并采纳了这些想法，同时也给出了H.D. Block和B.W. Knight的工作。然而，这些早期的努力并未导致隐藏单元的有效学习算法，也就是深度学习的实现未能成功。
