% Softmax 函数（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Softmax_function}{相关文章}。

Softmax 函数，也称为 softargmax1: 184  或归一化指数函数2: 198 ，能够将一个长度为 K 的实数向量转换为 K 个可能结果的概率分布。它是逻辑函数在多维空间的推广形式，常用于多项式逻辑回归中。Softmax 函数通常作为神经网络中最后一层的激活函数，用于将网络输出归一化为对各个预测类别的概率分布。
\subsection{定义}
Softmax 函数以一个长度为 $K$ 的实数向量 $\mathbf{z}$ 作为输入，并将其归一化为一个概率分布：该分布由 $K$ 个概率值组成，每个概率值与输入中对应元素的指数成正比。也就是说，在应用 Softmax 之前，向量中的某些分量可能为负，或大于 1，且它们的和不一定为 1；但在应用 Softmax 之后，每个分量都将位于区间 $(0, 1)$ 之内，并且所有分量之和为 1，因此可以将它们解释为概率。此外，输入值越大的分量，对应的概率也越大。

标准（单位）Softmax 函数$\sigma: \mathbb{R}^K \to (0,1)^K$,其中 $K > 1$，它接收一个向量$\mathbf{z} = (z_1, \dotsc, z_K) \in \mathbb{R}^K$,并计算输出向量$\sigma(\mathbf{z}) \in (0,1)^K$的每个分量，定义为：
$$
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}.~
$$
换句话说，Softmax 对输入向量 $\mathbf{z}$ 中的每个元素 $z_i$ 应用标准指数函数（即 $e^{z_i}$），然后将所有指数值归一化——即每个指数值除以所有指数值的总和。这个归一化操作保证了输出向量 $\sigma(\mathbf{z})$ 所有分量的和为 1，从而可以被解释为概率分布。

“Softmax”一词来源于指数函数对输入向量中最大值的放大作用。例如，对向量 $(1, 2, 8)$ 进行标准 Softmax 运算，其结果大约为$(0.001, 0.002, 0.997)$,也就是说，几乎所有的权重都被分配给了最大值 8 所在的位置。

一般情况下，Softmax 函数中不一定非要使用自然底数 $e$，可以使用任意大于 0 的底数 $b$。和之前一样：如果 $b > 1$，那么输入向量中较大的分量会对应较大的输出概率；并且当 $b$ 增大时，所得的概率分布将更加集中在最大值所在的位置；相反，如果 $0 < b < 1$，那么输入中较小的分量反而会对应较大的输出概率；随着 $b$ 的减小，概率分布将更多地集中在最小值所在的位置。我们可以写成如下形式：$b = e^{\beta}$，或$b = e^{-\beta}$，其中 $\beta$ 为实数。这将导致 Softmax 函数有如下表达式：
$$
\sigma(\mathbf{z})_i = \frac{e^{\beta z_i}}{\sum_{j=1}^{K} e^{\beta z_j}} \quad \text{或} \quad \sigma(\mathbf{z})_i = \frac{e^{-\beta z_i}}{\sum_{j=1}^{K} e^{-\beta z_j}}, \quad \text{其中 } i = 1, \dotsc, K~
$$
其中与 $\beta$ 的倒数成正比的值，有时被称为温度：$\beta = 1/kT$这里的 $k$ 通常取 1 或玻尔兹曼常数，$T$ 是“温度”。较高的温度（较小的 $\beta$）会使输出分布更均匀（即熵更高，更“随机”）；较低的温度（较大的 $\beta$）则会使分布更尖锐，即一个值占主导地位。

在某些领域中，底数 $b$ 是固定的，对应于某种固定的尺度；而在另一些领域中，会改变参数 $\beta$ 或 $T$ 来调整分布的形状。
\subsection{解释}
\subsubsection{平滑的 arg max}
Softmax 函数是 arg max 函数（即返回向量中最大元素索引的函数）的一个平滑近似。尽管如此，“softmax”这个名称可能具有误导性：它并不是最大值函数的平滑近似，而只是arg max的平滑版本。“softmax”一词有时也被用来指代与之密切相关的 LogSumExp 函数，而后者确实是最大值函数的平滑近似。因此，为了更准确地表达其本质，一些人更倾向于使用“softargmax”这一术语，尽管在机器学习领域，“softmax”已经是习惯用法。为避免混淆，本节中使用“softargmax”这一更清晰的表述。

与其将 arg max 看作一个输出为类别索引（如 $1, 2, \dots, n$）的函数，我们可以将其视为一个输出为独热编码的函数（假设最大值是唯一的）：
$$
\operatorname{arg,max}(z_1, \dots, z_n) = (y_1, \dots, y_n) = (0, \dots,0,1,0,\dots, 0)~
$$

