% Softmax 函数（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Softmax_function}{相关文章}。

Softmax 函数，也称为 softargmax1: 184  或归一化指数函数2: 198 ，能够将一个长度为 K 的实数向量转换为 K 个可能结果的概率分布。它是逻辑函数在多维空间的推广形式，常用于多项式逻辑回归中。Softmax 函数通常作为神经网络中最后一层的激活函数，用于将网络输出归一化为对各个预测类别的概率分布。
\subsection{定义}
Softmax 函数以一个长度为 $K$ 的实数向量 $\mathbf{z}$ 作为输入，并将其归一化为一个概率分布：该分布由 $K$ 个概率值组成，每个概率值与输入中对应元素的指数成正比。也就是说，在应用 Softmax 之前，向量中的某些分量可能为负，或大于 1，且它们的和不一定为 1；但在应用 Softmax 之后，每个分量都将位于区间 $(0, 1)$ 之内，并且所有分量之和为 1，因此可以将它们解释为概率。此外，输入值越大的分量，对应的概率也越大。

标准（单位）Softmax 函数$\sigma: \mathbb{R}^K \to (0,1)^K$,其中 $K > 1$，它接收一个向量$\mathbf{z} = (z_1, \dotsc, z_K) \in \mathbb{R}^K$,并计算输出向量$\sigma(\mathbf{z}) \in (0,1)^K$的每个分量，定义为：
$$
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}.~
$$
换句话说，Softmax 对输入向量 $\mathbf{z}$ 中的每个元素 $z_i$ 应用标准指数函数（即 $e^{z_i}$），然后将所有指数值归一化——即每个指数值除以所有指数值的总和。这个归一化操作保证了输出向量 $\sigma(\mathbf{z})$ 所有分量的和为 1，从而可以被解释为概率分布。

“Softmax”一词来源于指数函数对输入向量中最大值的放大作用。例如，对向量 $(1, 2, 8)$ 进行标准 Softmax 运算，其结果大约为$(0.001, 0.002, 0.997)$,也就是说，几乎所有的权重都被分配给了最大值 8 所在的位置。

一般情况下，Softmax 函数中不一定非要使用自然底数 $e$，可以使用任意大于 0 的底数 $b$。和之前一样：如果 $b > 1$，那么输入向量中较大的分量会对应较大的输出概率；并且当 $b$ 增大时，所得的概率分布将更加集中在最大值所在的位置；相反，如果 $0 < b < 1$，那么输入中较小的分量反而会对应较大的输出概率；随着 $b$ 的减小，概率分布将更多地集中在最小值所在的位置。我们可以写成如下形式：$b = e^{\beta}$，或$b = e^{-\beta}$，其中 $\beta$ 为实数。这将导致 Softmax 函数有如下表达式：
$$
\sigma(\mathbf{z})_i = \frac{e^{\beta z_i}}{\sum_{j=1}^{K} e^{\beta z_j}} \quad \text{或} \quad \sigma(\mathbf{z})_i = \frac{e^{-\beta z_i}}{\sum_{j=1}^{K} e^{-\beta z_j}}, \quad \text{其中 } i = 1, \dotsc, K~
$$
其中与 $\beta$ 的倒数成正比的值，有时被称为温度：$\beta = 1/kT$这里的 $k$ 通常取 1 或玻尔兹曼常数，$T$ 是“温度”。较高的温度（较小的 $\beta$）会使输出分布更均匀（即熵更高，更“随机”）；较低的温度（较大的 $\beta$）则会使分布更尖锐，即一个值占主导地位。

在某些领域中，底数 $b$ 是固定的，对应于某种固定的尺度；而在另一些领域中，会改变参数 $\beta$ 或 $T$ 来调整分布的形状。
\subsection{解释}
\subsubsection{平滑的 arg max}
Softmax 函数是 arg max 函数（即返回向量中最大元素索引的函数）的一个平滑近似。尽管如此，“softmax”这个名称可能具有误导性：它并不是最大值函数的平滑近似，而只是arg max的平滑版本。“softmax”一词有时也被用来指代与之密切相关的 LogSumExp 函数，而后者确实是最大值函数的平滑近似。因此，为了更准确地表达其本质，一些人更倾向于使用“softargmax”这一术语，尽管在机器学习领域，“softmax”已经是习惯用法。为避免混淆，本节中使用“softargmax”这一更清晰的表述。

与其将 arg max 看作一个输出为类别索引（如 $1, 2, \dots, n$）的函数，我们可以将其视为一个输出为独热编码的函数（假设最大值是唯一的）：
$$
\operatorname{arg,max}(z_1, \dots, z_n) = (y_1, \dots, y_n) = (0, \dots,0,1,0,\dots, 0)~
$$
当且仅当索引 $i$ 是向量$(z_1, \dots, z_n)$的最大值所在位置时，输出坐标 $y_i = 1$，也就是说，$z_i$是该向量的**唯一最大值**。例如，在这种编码下：$\operatorname{arg\,max}(1, 5, 10) = (0, 0, 1)$因为第三个元素是最大值。

这一表示可以推广到存在多个最大值（即多个 $z_i$ 相等且为最大值）的情况。此时，可以将值 1 平均分配给所有最大值所在的位置；形式上，对应位置取值为 $1/k$，其中 $k$ 是最大值的个数。例如：$\operatorname{arg\,max}(1, 5, 5) = (0, 1/2, 1/2)$,因为第二和第三个元素都是最大值。如果所有元素都相等，例如：$\operatorname{arg\,max}(z, \dots, z) = \left( 1/n, \dots, 1/n \right)$表示每个位置都等可能是最大值。具有多个最大值的点 $\mathbf{z}$ 被称为奇异点（singular points或 singularities），它们构成所谓的奇异集——这些是 arg max 函数不连续的点（存在跳跃不连续）；而只有一个最大值的点则称为非奇异点或常规点（non-singular或 regular points）。

根据引言中的最后一个表达式，softargmax 是 arg max函数的一个平滑近似：当$\beta \to \infty$时，softargmax 逐点收敛于 arg max。也就是说，对于任意固定的输入向量 $\mathbf{z}$，当 $\beta \to \infty$ 时，有：$\sigma_{\beta}(\mathbf{z}) \to \operatorname{arg\,max}(\mathbf{z})$。然而，softargmax 不以一致方式收敛于 arg max。这意味着不同的输入点收敛速度不同，甚至可能非常缓慢。实际上，softargmax 是连续的，而 arg max 在 奇异集（即两个或多个坐标相等的位置）上是不连续的。由于连续函数的一致极限也是连续函数，而 arg max 不连续，因此 softargmax 不可能以一致方式收敛于它。其不一致收敛的原因在于：当输入中两个坐标接近相等（其中一个略大于另一个）时，arg max 的输出会发生剧烈跳跃（从一个位置变到另一个）。例如：
$\sigma_\beta(1, 1.0001) \to (0, 1)$，$\sigma_\beta(1, 0.9999) \to (1, 0)$，而对于完全相等的输入 $(1, 1)$，无论 $\beta$ 为多少，都有：$\sigma_\beta(1, 1) = (1/2, 1/2)$。这说明：越接近奇异点 $(x, x)$，收敛速度越慢。尽管如此，在非奇异点集（即最大值唯一）上，softargmax 会在紧集上收敛。这是一种更强的逐点收敛形式，适用于没有不连续跳变的区域。

相反地，当$\beta \to -\infty$时，softargmax 会以相同的方式收敛到 arg min，这时的奇异集是具有两个或多个最小值的点。在热带分析的语言中，softmax 被视为对 arg max 和 arg min 的一种变形或“量化”。具体而言，这种变形是将 max-plus 半环（对应 arg max）或 min-plus 半环（对应 arg min）替换为 对数半环。通过取极限来恢复 arg max 或 arg min 的过程，被称为“热带化”或“去量化”。

同样地，对于任意固定的 $\beta$，如果某个输入 $z_i$ 相对于温度$T = 1/\beta$来说远大于其他输入分量，那么 Softmax 的输出就会近似于 arg max。例如，当温度为 1 时，输入间的差值为 10 被认为是“很大”的：
$$
\sigma(0, 10) := \sigma_1(0, 10) = \left( 1/(1 + e^{10}), e^{1/10}/({1 + e^{10})}\right) \approx (0.00005, 0.99995)~
$$
在这种情况下，输出几乎完全集中在最大值对应的位置上。

但如果输入差值相对于温度来说较小，则输出就不会接近 arg max。例如，当温度为 100 时，差值 10 相对较小：
$$
\sigma_{1/100}(0, 10) = \left( \frac{1}{1 + e^{1/10}}, \frac{e^{1/10}}{1 + e^{1/10}} \right) \approx (0.475, 0.525)~
$$
此时两个分量的概率接近平均，远不如高温差下那么“明确”。随着$\beta \to \infty$，意味着温度$T = \frac{1}{\beta} \to 0$，此时即便是很小的输入差异，相对于趋近于零的温度也变得“巨大”，这为 Softmax 在极限情形下趋于 arg max 提供了另一种解释。
