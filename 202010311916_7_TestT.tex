% TestTensor

\pentry{Matrix\upref{Mat}}

The word "tensor" was first translated in Structural Mechanics, used to describe the properties of stress, therefore translated as “张力”. The term continues in use today, except the translation now becomes “张量”. Today we realize that the stress tensor is only a particular case for a more general mathematical object, tenosr. This object appears everywhere in physics frequently. We will define the concept of tensor from the fundamental sense, which is the contemporary perspective. We will also demonstrate how a tensor is represented by a matrix or a high dimensional matrix. 

This section is a step-by-step guiding textbook, rather than a list of concepts and theorems. To better understand this section, it is recommended that the reader have solid understanding of linear algebra. 

The definition we use in this section is the most fundamental version. No additional structures or constraints are put onto it; also, Einstein's Summation Agreement is not applied yet, so as to be friendly for beginners. For the further concepts like covariant or contravariant tensors, please continue to the subsequent sections. 



\subsection{Introduction}

Students in physics often ask a question: "what is a tensor?" Usually the students are actually asking "why the moment of intertia, the mass, the stress, the electrical polarization are tensors? What do they have in common? " To understand the essential common property of all these seemingly different quantaties, we have to clarify the very fundamental definition of a tensor. That is, to answer the real question, we have to answer the literal question. So let's take some time to focus on the mathematical term itself, before we natrually demonstrate why those things are all tensors. 

A tensor is a multi-linear map. Particularly, a second-order tensor can be interpreted as a map from two vectors to a scalor, or a map from a vector to another vector. The scalor here refers to an element in the field that defines the vector space; in physics, the field is usually the set of real numbers or complex numbers. 

Many physical quantities can be interpreted as linear maps. As an example, mass is a second-order tensor, which maps a force vector to an acceleration vector; the moment of inertia is a second-order tensor, which maps an angular velocity vector to an angular momentum vector. Tensors of different orders may behave differently, but they all share one property: the linearity. When you encounter tensors in the future, I suggest you take some time and think "what quantities does this tensor map from and to"; it helps a deeper understanding. 



\subsection{1-linear function, 2-linear function and their representation}

It is known that given 2 vector spaces $V$ and $W$, we can define a linear map from $V$ to $W$. In this case, we have only 1 independent and 1 dependent variables. A multi-linear map, say, a k-linear map, maps a series of vector spaces $V_1, V_2, \cdots, V_k$ to $W$, so that if only the element in one $V_i$ varies, the map varies in the way a linear map does. 

The simplest case is when $W$ is a 1-dimensional space. In this case, the vector space $W$ is also the field (the real numbers). If the image of a map is some set of numbers, then we have a particular term for such a map, \textbf{function}. This is the same function that you may have seen in high school, as the maps involved in high school are mostly from real numbers to real numbers. Now, since we make $W$ the set of real numbers, we just call the map from $V_1, V_2, \cdots, V_k$ to $W$ a \textbf{linear function}. Now, we are going to introduce the concepts of multi-linear maps, tensors and their representations, step by step, starting with linear functions. 

\subsubsection{Representing linear functions with vectors}

\begin{definition}{Linear functions}
Given the n-dimensional vector space $V$ on $\mathbb{R}$, we call $f:V\rightarrow \mathbb{R}$ a \textbf{linear function} from $V$ to $\mathbb{R}$, if and only if $f$ satisfies the linearity: for any vectors $\bvec{v}_1, \bvec{v}_2\in V$ and scalors $a_1, a_2\in\mathbb{R}$, we have: $a_1f(\bvec{v}_1)+a_2f(\bvec{v}_2)=f(a_1\bvec{v}_1+a_2\bvec{v}_2)$．

\end{definition}


If we think of $\mathbb{R}$ as a 1-dimensional vector space, then $f$ is a linear map from $V$ onto this vector space. Therefore, if we take any base of $V$ and determine what real numbers $f$ maps the base vectors to, then we can calculate $f(\bvec{v})$ for any vector $\bvec{v}\in V$. 

Now suppose a base of $V$ is $\{\bvec{e}_i\}^n_{i=1}$. If the base vector $\bvec{e}_i$ is mapped to $f(\bvec{e}_i)=m_i\in\mathbb{R}$, then for any vector $\bvec{v}=a_1\bvec{e}_1+a_2\bvec{e}_2+\cdots+a_n\bvec{e}_n$, we can calculate by linearity: 

\begin{equation}
\begin{aligned}
f(a_1\bvec{e}_1+a_2\bvec{e}_2+\cdots+a_n\bvec{e}_n)&=f(a_1\bvec{e}_1)+f(a_2\bvec{e}_2)+\cdots+f(a_n\bvec{e}_n)\\&=m_1a_1+m_2a_2+\cdots+m_na_n
\end{aligned}
\end{equation}


$m_1a_1+m_2a_2+\cdots+m_na_n$can be interpreted as the inner product of $\bvec{m}$ and $\bvec{v}$, where$\bvec{m}=m_1\bvec{e}_1+m_2\bvec{e}_2+\cdots+m_n\bvec{e}_n$．

That is, every linear function $f$ corresponds to a vector $\bvec{m}$, so that $f(\bvec{v})=\bvec{m}\cdot\bvec{v}$．The coordinate of the vector $\bvec{m}$, in the base $\{\bvec{e}_i\}_{i=1}^n$, is $(m_1, m_2, \cdots, m_n)^T$. If we define another base, the coordinate usually changes, but the vector is still the same. 

Since a linear function is a map from a vector space to real numbers, we also call it a 1-linear function. It is to distinguish it from other multi-linear functions, like the 2-linear function we are going to focus on now. 

\subsubsection{Representing 2-linear funcions with matices}

Let's stick to the n-dimensional vector space $V$, except now we take 2 copies of $V$ to construct the map $f:V\times V\rightarrow\mathbb{R}$．

\begin{definition}{2-linear function}
We call $f:V\times V\rightarrow\mathbb{R}$ a 2-linear function, if and only if for any fixed vector $\bvec{v}_0$, both $f(\bvec{v}_0, \bvec{v})$ and $f(\bvec{v}, \bvec{v}_0)$ are 1-linear functions for $\bvec{v}$. In particular, we call $f$ a 2-linear function from $(V, V)$ onto $\mathbb{R}$. 
\end{definition}

An explicit expression for a 2-linear function is as follows: 

\begin{equation}\label{TestT_eq2}
\begin{aligned}
f(a_1\bvec{v}_1+a_2\bvec{v}_2, b_1\bvec{u}_1+b_2\bvec{u}_2)&=a_1f(\bvec{v}_1, b_1\bvec{u}_1+b_2\bvec{u}_2)+a_2f(\bvec{v}_2, b_1\bvec{u}_1+b_2\bvec{u}_2)\\&=a_1b_1f(\bvec{v}_1, \bvec{u}_1)+a_2b_1f(\bvec{v}_2, \bvec{u}_1)+a_1b_2f(\bvec{v}_1, \bvec{u}_2)+a_2b_2f(\bvec{v}_2, \bvec{u}_2)
\end{aligned}
\end{equation}


In order to represent this map, we have to determine the bases for the two copies of $V$. They don't have to be the same base. Let's assume the base for the first $V$ is $\{\bvec{e}_i\}^n_{i=1}$, and for the second is $\{\bvec{e}_i'\}^n_{i=1}$. Now vectors in the two vector spaces can be represented by column matrices, and the 2-linear function by a square matrix: (example with a 2-dimesional $V$)
\begin{equation}\label{TestT_eq1}
\begin{aligned}
&\bvec{v}=a_1\bvec{e}_1+a_2\bvec{e}_2\rightarrow  \bvec c_v=\pmat{a_1\\a_2}\\
&\bvec{u}=b_1\bvec{e}'_1+b_2\bvec{e}'_2\rightarrow  \bvec c_u=\pmat{b_1\\b_2}\\
&f\rightarrow \bvec{M}=\pmat{f(\bvec{e}_1, \bvec{e}'_1)&f(\bvec{e}_1, \bvec{e}'_2)\\f(\bvec{e}_2, \bvec{e}'_1)&f(\bvec{e}_2, \bvec{e}'_2)}\\
\end{aligned}
\end{equation}


That way, we have $f(\bvec{v}, \bvec{u})= \bvec c_v^T\bvec{M} \bvec c_u$．Note that $\bvec c_v^T$ means the transpose of the matix $\bvec c_v$\footnote{If we use other bases, then the representations of $\bvec{v}$, $\bvec{u}$ and $f$ may be different matrices, but the calculation of $\bvec c_v^T\bvec{M} \bvec c_u$ doesn't change}．

\begin{exercise}{}
Think of the coordinate of a vector as a column matrix, then use to the laws of matrix calculation and substitute \autoref{TestT_eq1} into \autoref{TestT_eq2} to verify that $f(\bvec{v}, \bvec{u})= \bvec c_v^T\bvec{M} \bvec c_u$．
\end{exercise}


The example we just talked about is for 2-dimensional $V$. For n-dimensional $V$, $\bvec{M}$ becomes a $n\times n$ matrix, where the element in the ith row and jth column is $m_{ij}=f(\bvec{e}_i, \bvec{e}'_j)$．

Let's take a look at a simple 2-linear function. 

\begin{example}{Power}
The possible forces exerted on a point mass in the 3-dimensional space, make a 3-dimensional real vector space $V$. The possible velocities of this point mass alsk make a $V$\footnote{Recall: 2 vector spaces are isomorphic (equivalent) if and only if they are defined over a same field and have the same dimensions. }．Represent the force on the point by $\bvec{F}$, the velocity of the point by $\bvec{v}$, then the power of the force on the point is $P$, a 2-linear function: $P=\bvec{F}\cdot\bvec{v}$．If we determine the bases for the force space and the velocity space, we can also represent $\bvec{F}$ and $\bvec{v}$ as column matrices $c_F$ and $c_v$, respectively. Then, there is a matrix $\bvec{P}$ so that $P=c^T_F\bvec{P}c_v$. Here, $\bvec{P}$ is a 2-linear function that maps a force and a velocity to a power. If we choose the bases in the way: the base forces are along the positive directions of $x$, $y$ and $z$ with magnitude $1N$, and the base velocities are along the positive directions of $x$, $y$ and $z$ with magnitude $1\opn{m/s}$, then for these two bases, $\bvec{P}=\pmat{1&0&0\\0&1&0\\0&0&1}$；If we choose the base forces to be along the positive directions of $y$, $x$ and $z$, then $\bvec{P}$ becomes $\pmat{0&1&0\\1&0&0\\0&0&1}$. If, \textbf{again}, we change the base velocities to be along the positive directions of $x$, $z$ and $y$, then $\bvec{P}$ becomes $\pmat{0&0&1\\1&0&0\\0&1&0}$．
\end{example}



We can see that functiuon of power, $P$, maps the two vectors, force and velocity, to a real number, power. If we choose different base for the space of forces and the space of velocities, the matrix representation of $P$ also differs. These matrices all represent the properties of $P$ in corresponding bases. Compare this fact to the linear map that you learned from linear algebra. The matrix representation of the same linear map may vary as the bases for the vector spaces vary; similarly, the tensor $P$ doesn't change as the bases change, but the matrix representation of it does. 

\subsubsection{Review on matrix calculations}

In order to conveniently generalize the concept of 2-linear functions, we require the concept of matrix whose elements are vectors. We will briefly review the matrix calculations before proceeding with vector matrices. 

Matrix calculation is just an arrangement of addition and multiplication. Although we usually deal with matrices whose elements are numbers, we can apply the same arrangement to any set whose elements can "add" and "multiply". 

\begin{example}{Examples for vector matrices}

\begin{itemize}
%
\item We can multiply a vector with a number (the scalor product). If $\bvec{v}_i$ is a vector, $a_i$ is a number, then $\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}$ is a row matrix of 3 vectors，$\pmat{a_1\\a_2\\a_3}$ is a column matrix of 3 numbers. Use the scalor product as the multiplication, we have: $\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}\pmat{a_1\\a_2\\a_3}=(a_1\bvec{v}_1+a_2\bvec{v}_2+a_3\bvec{v}_3)$ is a vector, and $\pmat{a_1\\a_2\\a_3}\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}=\pmat{a_1\bvec{v_1}&a_1\bvec{v_2}&a_1\bvec{v_3}\\a_2\bvec{v_1}&a_2\bvec{v_2}&a_2\bvec{v_3}\\a_3\bvec{v_1}&a_3\bvec{v_2}&a_3\bvec{v_3}}$ is a square matrix of 9 vectors. 
\item We can multiply two vectors with inner product. Using this multiplication, $\pmat{\bvec{v}_1&\bvec{v}_2\\\bvec{v}_3&\bvec{v}_4}\cdot\pmat{\bvec{v}_5\\\bvec{v}_6}=\pmat{\bvec{v}_1\cdot\bvec{v}_5+\bvec{v}_2\cdot\bvec{v}_6\\\bvec{v}_3\cdot\bvec{v}_5+\bvec{v}_4\cdot\bvec{v}_6}$ is a column matrix of 2 numbers. 
\item We can multiply 3-dimensional vectors with cross product. Using this multiplication, $\pmat{\bvec{v}_1&\bvec{v}_2\\\bvec{v}_3&\bvec{v}_4}\times\pmat{\bvec{v}_5\\\bvec{v}_6}=\pmat{\bvec{v}_1\times\bvec{v}_5+\bvec{v}_2\times\bvec{v}_6\\\bvec{v}_3\times\bvec{v}_5+\bvec{v}_4\times\bvec{v}_6}$ is a column matrix of 2 vectors. 
%
\end{itemize}
\end{example}

\subsubsection{Representing 2-linear functions with vector matrices}

Suppose we have a 2-linear function $f:V^2\rightarrow\mathbb{R}$. It has a matrix representation under the base $\{\bvec{e}_i\}_{i=1}^n$: (for convenience, suppose the 2 copies of $V$ share this base)
\begin{equation}
\pmat{m_{ij}}=\pmat{m_{11}&m_{12}&\cdots&m_{1n}\\ m_{21}&m_{22}&\cdots&m_{2n}\\ \vdots&\vdots&\ddots&\vdots\\ m_{n1}&m_{n2}&\cdots&m_{nn}}
\end{equation}
If we take every row as a row matrix, namely $\bvec{m}_i^T=\pmat{m_{i1}&m_{i2}&\cdots&m_{in}}$, then $f$ can also be expressed as "a row matrix of row matrices"" 

\begin{equation}
\bvec{M}=\pmat{\bvec{m}_1^T&\bvec{m}_2^T&\cdots&\bvec{m}_n^T}
\end{equation}

For any vectors $\bvec{v}, \bvec{u}\in V$ if their coordinates under the base are $c_v$ and $c_u$, then $\bvec{M}c_v$ is a row matrix, and $\bvec{M}c_vc_u$ is a number in $\mathbb{R}$. 

Hence, the row matrix of row matrices, $\bvec{M}$, represents the 2-linear function $f$. We can express this representation explicitly: $f(\bvec{v}, \bvec{u})=\bvec{M}\bvec{v}\bvec{u}$\footnote{Here, we treat $\bvec{m}_1^T$ as an element from $\bvec{M}$，and vector $\bvec{v}, \bvec{u}$ as column matrices, then apply the matrix multiplication defined by scalor product. }．

You may have noticed that the expression starts to look complicated. Good news is that in subsequent sections we will introduce Einstein's Summation Agreement, which simplifies the expressions by a lot. 

\subsection{Tensors are linear functions}

A 2-linear function is also named a \textbf{second-order tensor}. Similarly, a 1-linear function is a \textbf{first-order tensor}. Further more, a real number (or anything from the field) is a \textbf{zero-order tensor}. This is one of the definition of tensors in modern mathematics: a tensor of order k is a k-linear function. 

But a tensor does NOT only map things to numbers; that is, tensors are more than linear functions; they are linear maps, as we are discussing as follows. 

\subsection{Reflections on 2-linear functions}
Let's observe a 2-linear function once again. We define a 2-linear function as a map from 2 vectors to 1 number, but in the example of "representing a 2-linear function with a vector matrix", we also realize that a 2-linear function can also map 1 vector to 1 linear function. In the case of the example, $f$ maps $c_v$ onto $\bvec{M}c_v$上; if we think of $\bvec{M}c_v$ as a whole, we notice that it is a 1-linear function of $c_u$. Now that a 1-linear funtion is interpreted as a vector, we can also interpret a 2-linear function as a map from 1 vector to 1 vector. 

To wrap up, a 2-linear function is a map \textbf{from 2 vectors to 1 number}, \textbf{from 1 vector to a 1-linear function}, or \textbf{from 1 vector to 1 vector}. Does the last interpretation look familiar? Yes, in this context, a 2-linear function is a "linear transformation" or a "linear map from one vector space to another", as we have studied a lot in linear algebra. 

With the new interpretations, it is possible to make more examples for 2-linear maps. 

\begin{example}{Moment of inertia}


The angular velocity is defined as a vector, with the angular speed as its magnitude and the direction defined with the righ-hand-rule. Given a reference point, the angular momentum of a particle is defined as $\bvec{r}\times m\bvec{v}$, where $\bvec{r}$ is the displacement vector from the reference point to the location of the particle, $m$ is the mass of the particle, and $\bvec{v}$is the velocity of the particle. 一个质点系统的角动量，定义为所有质点的角动量之和．

For a sphere whose mass distributes uniformly, it is highly symmetric. Now if it has angular velocity $\bvec{\omega}$, then its angular momentum is also along the direction of $\bvec{\omega}$. As result we have: $\bvec{L}=I\bvec{\omega}$, where $\bvec{L}$ is the angular momentum of the sphere, and the \textbf{number} $I$ 被称为转动惯量．

但是，如果一个刚体是$x$轴上一对对称质点构成的，它们的质量都是$m$，到原点的距离都是$r$，那么当这个刚体的角速度在$z$方向上时，角动量非零，且也在$z$方向上；当角速度在$x$方向上时，角动量为零．也就是说，转动惯量一般没法简单表示为一个实数$I$，而应该用一个$2$-线性函数来表示．在这个例子中，在给定的直角坐标系上，转动惯量表示为一个矩阵
\begin{equation}
\pmat{mr^2&0&0\\ 0&0&0\\ 0&0&0}
\end{equation}

拿出你的手机，把它近似看成一个长方体．取一个直角坐标系，使得三个坐标轴分别垂直于手机的正面、侧面和顶面，那么你的手机的转动惯量就可以表示成
\begin{equation}
\pmat{a&0&0\\ 0&b&0\\ 0&0&c}
\end{equation}
其中通常有$a>b>c$，因为你的手机的厚度从薄到厚依次是正面、侧面和顶面对应的方向．如果选用其它坐标系，这个转动惯量通常就是另一个矩阵的样子，但不管选什么坐标系表示手机的转动惯量矩阵，它们都是相似的．

事实上，转动惯量都应该是双线性函数而非一个数，即使对于前面所说的球体，也应该把它的转动惯量写为
\begin{equation}
\pmat{I&0&0\\ 0&I&0\\ 0&0&I}
\end{equation}
\end{example}

接下来一个例子也是极为常用的二阶张量．

\begin{example}{电极化率}
我们简单介绍一下电极化率的概念．如果你已经熟悉电极化率的概念，可以跳过介绍部分；如果你想了解电极化率的更多相关知识，请参考电动力学相关词条．

电偶极子是指一对电荷量相同的正负电荷构成的微观系统，它有一个性质叫电偶极矩，是一个向量．如果两个电荷分别带电荷$\pm q$，而从负电荷到正电荷的位移是$\bvec{r}$，那么我们定义此电偶极矩就是$\bvec{p}=q\bvec{r}$．当然，宏观上我们可以认为这两个电荷位置实际上是重合的，但仍然具有电偶极矩$\bvec{p}$，也就是所谓的“理想电偶极子”．

一些不对称的分子中，电子云的电荷中心可能和原子核的电荷中心不重合，一个这样的分子就可以看成是一个电偶极子．如果我们把一定区域里的电偶极子的电偶极矩都按照向量加法加起来，得到的总向量再除以这个区域的体积，我们就得到这个区域力电偶极矩的密度；当选定区域趋于一个点时，还能得到电偶极矩在一个点上的密度．当然，这里假设电偶极矩是连续的，因为宏观实验上所说的点，其实并不是严格意义上的点，也包含了大量的电偶极子．

一般情况下，即使有大量电偶极子存在，物质也不会显出电性，因为各个地方的电偶极子都互相抵消了，使得电偶极矩的密度\footnote{电动力学中，将电偶极矩的密度称为电极化强度（polarization）．}处处为零．但是如果物质处在电场下，就不一定了．正电荷趋于往电场方向移动，负电荷趋于往电场方向的负方向移动，于是电偶极子就趋于让自己的方向和电场方向平行．当然，电偶极子的长度也会发生变化，改变电偶极矩的大小，但是这种改变对电偶极矩密度的改变相比转向带来的改变，微不足道\footnote{对于没有自带电偶极矩的分子，或者单独的原子，转向效应是不存在的，这个时候只剩下电偶极矩被拉伸带来的效应了．}．当原本指向均匀地分布在各个方向上的电偶极子开始指向同一个方向的时候，各个点上就会出现非零的电偶极矩了，物质也开始显出电性．




\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/TestT_1.pdf}
\caption{电场引起电偶极矩变化的示意图．左图中物质里的电偶极子方向随机均匀分布，整体上不显电性；右图中物质里分布着水平向右的电场，在电场作用下电偶极子们都指向水平向右了，出现了水平向右的电偶极矩密度，宏观上开始表现出电性．} \label{TestT_fig1}
\end{figure}




电偶极矩密度不为零的时候，物质里会出现束缚电荷，但这不是我们目前关心的．我们目前关心的是，电偶极矩密度是怎么被电场影响的．

对于可以自由运动的电偶极子，比如水分子，这个规律其实非常简单：记$\bvec{P}$是某一点的电偶极矩密度，$\bvec{E}$是物质内的电场分布，那么就有$\bvec{P}=\epsilon\bvec{E}$，其中$\epsilon$是一个实数．也就是说，$\bvec{P}$总是和$\bvec{E}$平行，并且大小之比恒为$\epsilon$．这个时候，我们可以说水具有电极化张量$\bvec{M}$，使得$\bvec{P}=\bvec{M}\bvec{E}$．如果按照国际单位制，把$\bvec{E}$和$\bvec{P}$的基都选为同一组直角坐标系上的单位向量，那么水的电极化张量就可以表示为
\begin{equation}
\pmat{\epsilon&0&0\\ 0&\epsilon&0\\ 0&0&\epsilon}
\end{equation}
这是\textbf{单位矩阵的$\epsilon$倍}．

但是电极化张量并非总是这么简单的．如果在一个晶体里，固有的电偶极子都是指向水平方向的，但是水平向右和向左的电偶极子互相抵消，总体上还是不显电性；同时，这些电偶极子无论如何都被限制在水平方向上，那么它们就无法转向．当晶体中存在水平向右的电场$\bvec{E}$的时候，水平向右的电偶极子被拉长、向左的被缩短，于是出现了水平向右的电偶极矩密度$\bvec{P}=\epsilon\bvec{E}$．如果晶体中的电场不在水平方向，那么由于电偶极矩密度还是限制在了水平方向上，因此电场和电偶极矩密度不再平行了．按照国际单位制，把$\bvec{E}$和$\bvec{P}$的基都选为同一组直角坐标系上的单位向量，那么这个晶体里的电极化张量就变成了
\begin{equation}
\pmat{\epsilon&0&0\\ 0&0&0\\ 0&0&0}
\end{equation}
它\textbf{不再是单位矩阵的$\epsilon$倍}．

实际情况下，由于晶格对晶体中电荷在不同方向上的运动有不同程度的阻碍，晶体里的电极化张量不一定是单位矩阵的单位矩阵的倍数．不过，由于改变基时矩阵变为相似矩阵，那么如果电极化张量在某个基下的矩阵表示是可对角化的，那么总能找到$\bvec{E}$的一组基，使得电极化张量的矩阵是一个对角矩阵；换句话说，沿着这基向量方向上的电场，引起的电偶极矩密度依然和电场平行，只不过不同的方向上倍数不一样．

\end{example}


\subsection{$k$阶张量}

\subsubsection{$k$-线性函数}

把$2$-线性函数的定义推广，就得到$k$-线性函数的定义．

\begin{exercise}{}
请尝试给出$k$-线性函数的定义．答案就在题目下方．
\end{exercise}

\begin{definition}{$k$-线性函数}

给定域$\mathbb{R}$上的$n$维线性空间$V$，称$f:V^k\rightarrow \mathbb{R}$为$V$到域$\mathbb{R}$上的一个\textbf{$k$-线性函数}，如果$f$对于每一个自变量都是线性的；或者说，当$k$个自变量中只有一个在变化，其它分量不变，那么$f$对于那个变化的自变量是线性函数．特别地，线性函数又被称为$1$-线性函数．

\end{definition}

有了$k$-线性函数的概念，我们就可以定义$k$-阶张量了：

\begin{definition}{张量}
一个$k$-线性函数，被称为一个$k$-阶张量．
\end{definition}

从前面的讨论我们知道，一个二阶张量还可以看成是把一个向量映射到一个向量；向量可以和一阶张量一一对应，因此二阶张量也可以看成是把一个一阶张量映射到一个一阶张量．同样地，一个三阶张量可以看成是把两个线性函数映射到一个一阶张量．

事实上，一个$k$-阶张量$f(\bvec{v}_1, \bvec{v}_2, \cdots,\bvec{v}_k)$是$k$-线性函数，固定住其中几个自变量，比如说$\bvec{v}_1$到$\bvec{v}_m$，那么$f(\bvec{v}_1, \bvec{v}_2,\cdots,\bvec{v}_k)$可以看成是剩下$(k-m)$个自变量的$k-m$-线性函数$g(\bvec{v}_{m+1}, \cdots,\bvec{v}_k)$．也就是说，$f$把前$m$个向量映射成一个$k-m$线性函数．因此，我们也可以说，一个$k$阶张量把$m$个向量映射为一个$(k-m)$-线性函数．

\subsection{小结与拓展}

一个$n$维的$k$阶张量，可以表示为一个$k$维的超立方体矩阵，其中每条边包含了$n$个实数．

我们在之前的定义中都把线性空间看成是定义在实数域$\mathbb{R}$上的，而线性函数是线性空间映射到实数．实际上，这一点可以推广到，把线性空间定义在任何域$\mathbb{F}$上，线性函数就是线性空间映射到这个域$\mathbb{F}$上．这样也可以讨论线性空间上的张量．

目前物理学中常用到的域，是实数域$\mathbb{R}$和复数域$\mathbb{C}$．

像转动惯量这样的物理量，为什么可以看成线性映射呢？这是因为物理量彼此不是独立存在的，多个物理量之间相互作用而影响了其它的物理量，本身就是一种多个物理量映射到其它物理量或它们的性质的过程，就像牛顿第二定律可以看成是把加速度映射到力上，而质量的作用就是作为映射的函数．因此，物理量本身也可以看成是映射的过程，而在许多物理理论中，我们都可以线性地描述这些映射关系，因此常常可以把物理量看成是线性映射．







