% 深度学习（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Deep_learning}{相关文章}。

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/b06d12296b7d8e2d.png}
\caption{在深度学习中以多层抽象表示图像[1]} \label{fig_SDXX_1}
\end{figure}
深度学习是机器学习的一个子领域，专注于利用神经网络执行分类、回归和表示学习等任务。该领域受到生物神经科学的启发，核心在于将人工神经元堆叠成多层，并通过“训练”使其能够处理数据。“深度”一词指的是网络中使用了多层结构，这些层数从三层到几百甚至几千层不等。深度学习的方法可以是监督学习、半监督学习或无监督学习。[2]

常见的深度学习网络架构包括全连接网络（Fully Connected Networks）、深度信念网络（Deep Belief Networks）、循环神经网络（Recurrent Neural Networks）、卷积神经网络（Convolutional Neural Networks）、生成对抗网络（Generative Adversarial Networks）、Transformer以及神经辐射场（Neural Radiance Fields）。这些架构已应用于计算机视觉、语音识别、自然语言处理、机器翻译、生物信息学、药物设计、医学图像分析、气候科学、材料检测以及棋盘游戏程序等领域，并在许多情况下取得了与人类专家相当甚至超越的表现。[3][4][5]

早期形式的神经网络受到生物系统中信息处理和分布式通信节点的启发，特别是人脑。然而，当前的神经网络并不旨在模拟生物体的大脑功能，在这一方面通常被认为是低质量的模型。[6]
\subsection{概述}
大多数现代深度学习模型基于多层神经网络，例如卷积神经网络和Transformer，但它们也可能包含命题公式或潜变量，这些变量在深度生成模型中按层次组织，如深度信念网络和深度玻尔兹曼机中的节点。[7]

从根本上说，深度学习指的是一类机器学习算法，其特点是使用层次化的结构将输入数据逐步转换为更加抽象和复合的表示。例如，在图像识别模型中，原始输入可能是图像（表示为像素张量）。第一层表示可能试图识别基本形状（如线条和圆形），第二层可能组合和编码边缘排列，第三层可能编码鼻子和眼睛，第四层可能识别出图像中包含一张人脸。

深度学习的一个关键特点是，模型可以自主学习最佳特征及其所在的层级。在深度学习出现之前，机器学习技术通常需要通过手动设计特征工程来将数据转换为更适合分类算法操作的形式。而在深度学习方法中，特征不是手动设计的，模型会从数据中自动发现有用的特征表示。这并不意味着无需手动调整，例如，层数和每层大小的变化会提供不同程度的抽象。[8][2]

“深度学习”中的“深度”指的是数据被转换的层数。更确切地说，深度学习系统具有较大的信用分配路径（CAP，Credit Assignment Path）深度。CAP是从输入到输出的一系列转换过程，描述了输入和输出之间的潜在因果关系。对于前馈神经网络，CAP深度等于网络的深度，即隐藏层的数量加一（因为输出层也被参数化）。对于信号可能在某一层多次传播的循环神经网络，CAP深度潜在无限。[9] 虽然没有统一公认的深度门槛来区分浅层学习与深度学习，但大多数研究者认为深度学习的CAP深度大于2。研究表明，CAP深度为2的模型在理论上是一个通用逼近器，能够模拟任何函数。[10] 超过这一深度，更多的层不会提高网络作为函数逼近器的能力。然而，深度模型（CAP > 2）能够比浅层模型提取出更好的特征，因此额外的层数有助于有效地学习特征。

深度学习架构可以通过逐层的贪婪方法构建。[11] 深度学习能够解开抽象，提取出哪些特征有助于提升性能。[8]

深度学习算法可以应用于无监督学习任务。这是一个重要的优势，因为无标签数据比有标签数据更为丰富。可以通过无监督方式训练的深度结构包括深度信念网络等。[8][12]

“深度学习”这一术语由Rina Dechter在1986年引入机器学习领域，[13] 并由Igor Aizenberg及其同事于2000年引入人工神经网络领域，最初用于描述布尔阈值神经元。[14][15] 不过，其出现的历史显然更加复杂。[16]
\subsection{解释}
深度神经网络通常通过\textbf{通用逼近定理}[17][18][19][20][21]或\textbf{概率推断}[22][23][8][9][24]进行解释。

经典的\textbf{通用逼近定理}涉及具有单个有限大小隐藏层的前馈神经网络逼近连续函数的能力。[17][18][19][20] 1989年，George Cybenko首次证明了具有S形激活函数的前馈网络的通用逼近能力，[17] 这一结果在1991年被Kurt Hornik推广到多层前馈网络架构。[18] 最近的研究还表明，通用逼近定理同样适用于诸如Kunihiko Fukushima提出的修正线性单元（ReLU）等无界激活函数。[25][26]

针对深度神经网络的通用逼近定理则关注网络的宽度有限，但深度可以增加的情况下的逼近能力。Lu等人[21]证明，如果深度神经网络的宽度严格大于输入维度，且激活函数为ReLU，那么该网络可以逼近任何勒贝格可积函数；如果宽度小于或等于输入维度，则深度神经网络不具有通用逼近能力。

\textbf{概率解释}[24]来源于机器学习领域，涉及推断[23][7][8][9][12][24]以及与训练和测试相关的优化概念，分别对应于拟合和泛化。更具体地说，概率解释将激活非线性视为累积分布函数。[24] 概率解释促成了在神经网络中引入\textbf{dropout}作为正则化方法。这一解释由Hopfield、Widrow和Narendra等研究人员提出，并通过Bishop等人的综述文章得到推广。[27]
\subsection{历史}
\subsubsection{1980年之前}
人工神经网络（ANN）分为两种类型：\textbf{前馈神经网络（FNN）}或\textbf{多层感知机（MLP）}和\textbf{循环神经网络（RNN）}。RNN的连接结构中包含循环，而FNN则没有。

1920年代，\textbf{威廉·伦茨（Wilhelm Lenz）}和\textbf{恩斯特·伊辛（Ernst Ising）}提出了\textbf{伊辛模型（Ising model）}，[28][29] 其本质上是一种非学习型RNN架构，由类神经元的阈值元素组成。1972年，\textbf{天利俊一（Shun'ichi Amari）}将这一架构改造成自适应的。[30][31] 他的学习型RNN在1982年由\textbf{约翰·霍普菲尔德（John Hopfield）}重新发表。[32] 其他早期的循环神经网络包括\textbf{中野薰（Kaoru Nakano）}在1971年发表的模型。[33][34] 早在1948年，\textbf{艾伦·图灵（Alan Turing）}就已经在未发表的论文《智能机器》（Intelligent Machinery）中提出了与人工进化和学习型RNN相关的思想。[35][31]

1958年，\textbf{弗兰克·罗森布拉特（Frank Rosenblatt）}提出了感知机（Perceptron），这是一个包含三层的MLP：输入层、具有随机权重的隐藏层（不参与学习）和输出层。他在1962年的书中进一步介绍了感知机的变体和计算机实验，包括一种四层感知机的版本，其“终端前网络”（preterminal networks）具有自适应权重。[37] 书中引用了\textbf{R.D. Joseph}于1960年提出的一个类似系统[38]，但Joseph的学习算法并不实用，最终被遗忘。

第一个可用的深度学习算法是1965年\textbf{亚历克谢·伊瓦赫宁科（Alexey Ivakhnenko）}和Lapa提出的\textbf{数据处理组方法（Group Method of Data Handling, GMDH）}，它可以训练任意深度的神经网络。[39] 1971年的一篇论文描述了用这种方法训练的八层深度网络。[41] 该方法通过逐层回归分析进行训练，并使用单独的验证集剪枝多余的隐藏单元。节点的激活函数是柯尔莫哥洛夫-加博尔多项式（Kolmogorov-Gabor polynomials），因此这些是最早的带有乘性单元或“门”的深度网络。[31]

第一个使用\textbf{随机梯度下降（Stochastic Gradient Descent, SGD）}训练的多层感知机（MLP）由\textbf{天利俊一}于1967年提出。[43] 他的学生Saito的计算机实验显示，一个五层MLP通过两个可调节层学会了非线性可分模式的内部表示。[31] 随着硬件和超参数调节的发展，端到端的随机梯度下降已成为当前主流的训练技术。

1969年，\textbf{福岛邦彦（Kunihiko Fukushima）}引入了\textbf{修正线性单元（ReLU, Rectified Linear Unit）}激活函数。[25][31] ReLU已成为深度学习中最流行的激活函数。[44]

卷积神经网络（CNN）的深度学习架构，包含卷积层和下采样层，起源于1979年\textbf{福岛邦彦}提出的新认知机（Neocognitron），尽管当时未使用反向传播进行训练。[45][46]

\textbf{反向传播（Backpropagation）}是一种高效应用由\textbf{戈特弗里德·威廉·莱布尼茨（Gottfried Wilhelm Leibniz）}于1673年提出的链式法则的技术，用于可微节点组成的网络。术语“误差反向传播”（Back-propagating errors）由\textbf{罗森布拉特}在1962年引入，[37] 但他并未实现该方法。\textbf{Henry J. Kelley}在1960年提出了控制理论背景下的连续版本反向传播。[48] 现代形式的反向传播首次由\textbf{Seppo Linnainmaa}在1970年的硕士论文中发表。[49][50][31] \textbf{G.M. Ostrovski}等人在1971年重新发表了该方法。[51][52] \textbf{Paul Werbos}在1982年将反向传播应用于神经网络，[53] 但他的1974年博士论文中尚未描述该算法。[52] 1986年，\textbf{David E. Rumelhart}等人推广了反向传播，但未引用最初的研究。[55][56]
\subsubsection{1980年代至2000年代}
时延神经网络（TDNN）：1987年，Alex Waibel 引入了时延神经网络（TDNN），将卷积神经网络（CNN）应用于音素识别。它使用了卷积、权重共享和反向传播。[57][58]CNN早期应用：1988年，Wei Zhang将反向传播训练的CNN用于字母识别。[59] 1989年，Yann LeCun等人开发了LeNet，用于识别邮件上的手写邮政编码，训练耗时3天。[60]1990年，Wei Zhang在光学计算硬件上实现了CNN。[61] 1991年，CNN被应用于医学图像分割[62]和乳腺癌检测。[63] 1998年，LeNet-5是一个7层CNN，用于分类手写数字，并被多家银行用于识别支票上的手写数字。[64]  

RNN在1980年代得到了进一步发展，主要用于序列处理。展开后的RNN在数学上类似于深度前馈层。[28][30] 两个早期的关键RNN模型：Jordan网络（1986）[65]和Elman网络（1990）[66]，被用于认知心理学问题的研究。

1980年代，反向传播在长信用分配路径上的表现不佳。1991年，Jürgen Schmidhuber提出了一种分层RNN，通过自监督学习逐层预训练，每层RNN尝试预测自身的下一个输入。[67][68]  1993年，这种神经历史压缩器解决了一个“非常深度学习”任务，涉及1000多层展开的RNN。[69]  

1991年，Sepp Hochreiter在毕业论文中提出了LSTM，并分析了梯度消失问题。[70][71] 1995年，LSTM被正式发表。[72] LSTM能够处理需要数千离散时间步的长期记忆任务。现代LSTM架构于1999年引入“遗忘门”，成为标准RNN架构。[73]  

生成对抗网络的雏形：1991年，Schmidhuber提出了“对抗神经网络”，其中两个网络以零和博弈形式竞争。[74][75] 这一原理在2014年被用于开发生成对抗网络（GANs）。[76]  

无监督学习模型：受统计力学的启发，Terry Sejnowski、Peter Dayan 和 Geoffrey Hinton等人开发了玻尔兹曼机、受限玻尔兹曼机、赫姆霍兹机及其“唤醒-睡眠”算法。[77][78][79][80] 这些模型设计用于深度生成模型的无监督学习，但与反向传播相比计算成本更高。 生物信息学的早期应用：1988年的网络成为蛋白质结构预测的最新成果，这是深度学习在生物信息学中的早期应用。[82]  

ANN的探索：ANN（包括RNN）在语音识别中的浅层和深层学习已被研究多年。[83][84][85] 然而，早期方法未能超越基于生成模型的GMM-HMM技术。[86] 主要困难包括梯度消失[70]和神经预测模型中时间相关性较弱。[87][88]  

例外进展：1990年代末，SRI International在美国NSA和DARPA资助下，研究语音和说话人识别。1998年，Larry Heck领导的团队在NIST说话人识别基准中取得了显著成功，并在Nuance Verifier中部署，这是深度学习的首次重大工业应用。[89][91]  原始特征的探索：1990年代末，深度自编码器首次在“原始”语谱图或线性滤波特征上表现出优越性，超越了传统的梅尔倒谱特征。[90] 随后，基于原始语音波形的深度学习在大规模任务中取得了优异表现。[92]  
\subsubsection{2000年代}  
在1990年代和2000年代，神经网络进入了一段低潮期。由于人工神经网络的计算成本较高以及对生物网络连接方式缺乏理解，研究更倾向于采用任务特定的手工设计特征（如Gabor滤波器）和支持向量机（SVM）等更简单的模型。[citation needed]  

2003年，LSTM在某些任务上开始与传统语音识别器竞争。[93] 2006年，Alex Graves、Santiago Fernández、Faustino Gomez和Schmidhuber将LSTM与\textbf{连接时间分类（CTC）}结合在一起，形成了LSTM堆栈。[94][95] 2009年，LSTM成为首个在模式识别比赛中获胜的RNN，用于连接手写识别任务。[96][9]  

2006年，Geoff Hinton、Ruslan Salakhutdinov、Osindero和Teh发表了关于\textbf{深度信念网络（DBN）}的论文。[97][98] 该模型被开发用于生成式建模。训练过程包括先训练一个受限玻尔兹曼机（RBM），然后冻结这一层，再在其上训练另一层，以此类推，最后可选地使用监督反向传播进行微调。[99] 深度信念网络可以建模高维概率分布，例如MNIST图像的分布，但收敛速度较慢。[100][101][102]  

根据Yann LeCun的说法，2000年代早期，CNN已经处理了估计10\%至20\%的美国手写支票。[103] 深度学习在大规模语音识别中的工业应用始于2010年前后。  

2009年，NIPS举办了“深度学习与语音识别”研讨会，旨在解决基于深度生成模型的语音识别的局限性，并探索在更强硬件和大规模数据集支持下神经网络的实用性。当时认为，通过深度信念网络（DBN）对深度神经网络（DNN）进行生成式预训练可以克服神经网络的主要困难。然而，研究发现，若使用包含大量上下文相关输出层的大型DNN，仅通过大规模训练数据和直接反向传播即可显著降低错误率，相较于当时的高斯混合模型（GMM）/隐马尔可夫模型（HMM）和其他生成式模型系统，这种方法效果更佳。[104]两种系统产生的识别错误性质截然不同，[105] 为将深度学习集成到已有高效语音解码系统提供了技术见解。[23][106][107] 2009年至2010年左右的分析对比了基于GMM和其他生成式语音模型与DNN模型的表现，激发了工业界对深度学习在语音识别中投资的兴趣。[105] 这一分析表明，判别式DNN与生成式模型之间的误差率差距小于1.5\%。[104][105][108]2010年，研究人员将深度学习从TIMIT扩展到大词汇量语音识别。他们通过基于决策树构建的上下文相关HMM状态，采用DNN的大型输出层，从而实现这一突破。[109][110][111][106]  
\subsubsection{深度学习革命}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/19d3951b7fd96363.png}
\caption{深度学习是机器学习的一个子集，而机器学习是人工智能（AI）的一个子集。} \label{fig_SDXX_2}
\end{figure}
深度学习革命始于基于卷积神经网络（CNN）和GPU的计算机视觉。

尽管通过反向传播训练的CNN已经存在了几十年，且基于GPU实现的神经网络（包括CNN）也已应用多年，[112][113] 但要在计算机视觉领域取得进展，还需要更快的GPU上的CNN实现。后来，随着深度学习的普及，专门用于深度学习的硬件以及算法优化得以发展。[114]

推动深度学习革命的一项关键进步是硬件的发展，特别是GPU。早期的一些工作可以追溯到2004年。[112][113] 在2009年，Raina、Madhavan 和 Andrew Ng 报告了一个由30块Nvidia GeForce GTX 280 GPU组成的集群训练的1亿参数深度信念网络，这是基于GPU的深度学习的早期演示。他们报告称训练速度提高了多达70倍。[115]

2011年，Dan Ciresan、Ueli Meier、Jonathan Masci、Luca Maria Gambardella 和 Jürgen Schmidhuber 开发的名为 DanNet 的卷积神经网络（CNN）[116][117] 首次在视觉模式识别竞赛中达到了超越人类的表现，其性能比传统方法高出3倍。[9] 随后，它还赢得了更多的竞赛。[118][119] 他们还展示了基于GPU的最大池化CNN如何显著提升性能。[3]

2012年，Andrew Ng 和 Jeff Dean 开发了一种前馈神经网络（FNN），能够仅通过观看从YouTube视频中提取的无标签图像学习识别更高层次的概念，例如猫。[120]

2012年10月，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 开发的 AlexNet[4] 在大规模 ImageNet 竞赛中以显著优势击败了浅层机器学习方法。这之后，增量改进包括由 Karen Simonyan 和 Andrew Zisserman 开发的 VGG-16 网络[121] 以及 Google 的 Inceptionv3。[122]

图像分类的成功随后扩展到更具挑战性的任务，即为图像生成描述（字幕），这通常是 CNN 和 LSTM 的结合使用。[123][124][125]

到2014年，深度学习的最新进展是训练“非常深的神经网络”，网络深度达到20至30层。[126] 但堆叠过多的层数会导致训练准确率的急剧下降，[127] 这被称为“退化问题”。[128] 在2015年，为训练非常深的网络开发了两种技术：2015年5月发布的 Highway Network，以及2015年12月发布的残差神经网络（ResNet）。[129] ResNet 的行为类似于一个开放门控的 Highway 网络。

大约在同一时间，深度学习开始对艺术领域产生影响。早期的例子包括 Google DeepDream（2015）和神经风格迁移（2015），[130] 这两者都基于预训练的图像分类神经网络，例如 VGG-19。

生成对抗网络（GAN）由 Ian Goodfellow 等人于2014年提出，[131]（基于 Jürgen Schmidhuber 的人工好奇原则[74][76]），在2014至2018年期间成为生成建模的最新技术。Nvidia 的 StyleGAN（2018）基于 Tero Karras 等人提出的 Progressive GAN，[133] 实现了卓越的图像质量。在该方法中，GAN 的生成器以金字塔方式从小规模逐步扩展到大规模。GAN 的图像生成取得了广泛的成功，并引发了关于深度伪造（deepfakes）的讨论。[134] 自那时以来，扩散模型（2015）[135] 在生成建模中超越了 GAN，例如 DALL·E 2（2022）和 Stable Diffusion（2022）等系统。

2015年，Google 的语音识别通过基于 LSTM 的模型提高了49\%，并通过智能手机上的 Google Voice Search 提供了这一功能。[136][137]

深度学习已经成为多个学科中最先进系统的一部分，尤其是在计算机视觉和自动语音识别（ASR）领域。诸如 TIMIT（ASR）和 MNIST（图像分类）等常用评估集的结果，以及一系列大词汇量语音识别任务的结果，均稳步提升。[104][138] 在语音识别方面，卷积神经网络（CNN）被 LSTM 超越，[137][139][140][141] 但在计算机视觉领域，CNN 更为成功。

Yoshua Bengio、Geoffrey Hinton 和 Yann LeCun 因“在概念和工程上的突破，使深度神经网络成为计算的关键组成部分”而被授予2018年图灵奖。[142]
\subsection{神经网络}
人工神经网络（ANN）或连接主义系统是受动物大脑中生物神经网络启发的计算系统。这类系统通过学习示例来完成任务，并逐步提高能力，通常无需特定于任务的编程。例如，在图像识别中，它们可以通过分析手动标记为“猫”或“非猫”的示例图像来学习识别包含猫的图像，并利用分析结果来识别其他图像中的猫。它们主要被用于那些难以用基于规则的传统计算机算法表达的应用场景。

人工神经网络基于一组称为人工神经元的连接单元（类似于生物大脑中的神经元）。神经元之间的每个连接（突触）都可以将信号传递到另一个神经元。接收信号的（突触后）神经元可以处理信号，然后向与其连接的下游神经元发送信号。神经元可能具有状态，通常用介于0到1之间的实数表示。神经元和突触还可能具有权重，这些权重在学习过程中会发生变化，从而增强或减弱其向下游发送信号的强度。

通常，神经元以层的形式组织。不同的层可能对输入进行不同种类的变换。信号从第一层（输入层）开始，经过多层传递，最终到达最后一层（输出层），有时可能会多次遍历这些层。

神经网络方法的最初目标是以与人脑相同的方式解决问题。随着时间的推移，研究重点转向匹配特定的心理能力，导致了一些偏离生物学的创新，如反向传播（backpropagation），即反向传递信息并调整网络以反映该信息。

神经网络已被用于多种任务，包括计算机视觉、语音识别、机器翻译、社交网络过滤、棋类和电子游戏，以及医疗诊断。

截至2017年，神经网络通常包含几千到几百万个单元以及数百万个连接。尽管这些数字比人类大脑中的神经元数量低几个数量级，这些网络在许多任务上的表现已经超越人类水平，例如面部识别或围棋游戏[144]。