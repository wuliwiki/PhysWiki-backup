% 图灵测试（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Turing_test}{相关文章}。

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/3f5360e4685a515c.png}
\caption{图灵测试的“标准解释”中，C玩家（询问者）被赋予任务，试图判断哪一位玩家——A还是B——是计算机，哪一位是人类。询问者仅限于通过书面问题的回答来做出判断。[1]} \label{fig_TLCS_1}
\end{figure}
图灵测试，最初由艾伦·图灵于1949年提出，称为“模仿游戏”，是对机器是否能够表现出等同于人类的智能行为的测试，或者说是无法与人类行为区分的测试。图灵提出，测试中一位人类评估者将判断人类与机器之间的自然语言对话，机器被设计成产生类似人类的回应。评估者知道对话中的一方是机器，所有参与者都将被隔开。对话仅限于文字交流，例如使用计算机键盘和屏幕，因此测试结果不依赖于机器将文字转化为语音的能力。如果评估者无法可靠地区分机器与人类，那么机器就被认为通过了测试。测试的结果不依赖于机器是否能给出正确答案，而是看它的答案与人类回答的相似程度。由于图灵测试是对性能能力无法区分性的测试，因此其语言版本自然地推广到了所有人类的表现能力，包括语言和非语言（机器人）的表现能力。

该测试由图灵在1950年发表的论文《计算机与智能》中提出，当时他在曼彻斯特大学工作。论文开头写道：“我提议考虑这个问题，‘机器能思考吗？’”由于“思考”这一概念很难定义，图灵选择用“用另一种更相关且表达相对明确的话语替代这个问题”来描述问题。[6] 图灵以“三人游戏”的形式来描述这一问题，这个游戏称为“模仿游戏”，在这个游戏中，一位询问者通过向一位男士和一位女士提问，试图判断两位参与者的性别。图灵的新问题是：“是否存在可以在模仿游戏中表现得很好的数字计算机？”[2] 图灵认为这个问题是可以回答的。在论文的其余部分，他反驳了关于“机器能思考”这一命题的所有主要反对意见。[7]

自从图灵提出他的测试以来，它既具有深远的影响，也受到了广泛的批评，并成为人工智能哲学中的一个重要概念。[8][9] 哲学家约翰·塞尔在他的“中文房间”论证中评论了图灵测试，这一思想实验认为，无论程序如何使计算机表现得像人类，机器都无法拥有“思维”、“理解”或“意识”。塞尔批评图灵的测试，并声称它不足以检测意识的存在。
\subsection{聊天机器人}  
图灵测试后来促成了“聊天机器人”的发展，这些人工智能软件实体的唯一目的是与人进行文本聊天。今天，聊天机器人有了更广泛的定义；它是一个能够与人进行对话的计算机程序，通常通过互联网进行。OED[10][11]  
\subsubsection{ELIZA 和 PARRY}  
1966年，Joseph Weizenbaum创建了一个名为ELIZA的程序。该程序通过检查用户输入的评论中的关键词来工作。如果找到了关键词，就会应用一个规则来转换用户的评论，并返回生成的句子。如果没有找到关键词，ELIZA则会以一个通用的回答或通过重复之前的评论来回应。[12] 此外，Weizenbaum将ELIZA开发为模拟罗杰斯式心理治疗师的行为，使得ELIZA能够“自由地假设几乎对真实世界一无所知”。[13] 通过这些技术，Weizenbaum的程序能够让一些人相信他们在与一个真人对话，甚至有些人“非常难以相信ELIZA [...] 不是人类”。[13] 因此，有些人认为ELIZA是第一个能够通过图灵测试的程序，[13][14] 尽管这一观点存在很大争议（参见下文“询问者的天真”）。

1972年，Kenneth Colby创建了PARRY，一个被描述为“带有态度的ELIZA”的程序。[15] 它试图模拟偏执型精神分裂症患者的行为，采用与Weizenbaum类似（但更先进）的方法。为了验证该工作，PARRY在1970年代早期通过图灵测试的变体进行了测试。一个由经验丰富的精神科医生组成的小组分析了通过电传打字机传输的真实病人与运行PARRY的计算机的对话。另一个由33名精神科医生组成的小组则查看了对话记录。随后，这两个小组被要求识别哪些“病人”是人类，哪些是计算机程序。[16] 精神科医生们仅有52\%的正确识别率——这一数据与随机猜测相符。[16]
\subsubsection{尤金·古斯特曼 (Eugene Goostman)}  
2001年，在俄罗斯圣彼得堡，由三位程序员——俄罗斯出生的弗拉基米尔·维谢洛夫、乌克兰出生的尤金·德门琴科和俄罗斯出生的谢尔盖·乌拉森——开发了一个名为“尤金·古斯特曼”的聊天机器人。2014年7月7日，它成为了第一个看似通过图灵测试的聊天机器人，在英国雷丁大学举办的纪念阿兰·图灵60周年忌日的活动中，三分之一的评委认为古斯特曼是人类；活动组织者凯文·沃里克认为它通过了图灵测试。古斯特曼被描绘为一位来自乌克兰敖德萨的13岁男孩，拥有一只豚鼠宠物和一位妇科医生父亲。选择这个年龄是故意的，目的是让与他“对话”的人原谅他回答中的小语法错误。[10][17][18]  
\subsubsection{Google LaMDA}  
2022年6月，谷歌的LaMDA（对话应用的语言模型）聊天机器人因被声称具备意识而广泛报道。最初在《经济学人》的一篇文章中，谷歌研究员布莱斯·阿圭拉·亚卡斯表示，LaMDA展示了对社会关系的某种理解。[19] 几天后，谷歌工程师布莱克·莱莫因在《华盛顿邮报》的采访中声称，LaMDA已具备意识。莱莫因因内部提出这一观点而被谷歌停职。阿圭拉·亚卡斯（谷歌副总裁）和詹·吉奈（负责创新的负责人）调查了这一说法，但驳回了它们。[20] 莱莫因的主张遭到该领域其他专家的普遍反对，指出一个看似模仿人类对话的语言模型，并不意味着其背后存在任何智能，[21] 尽管它似乎通过了图灵测试。关于LaMDA是否已经达到意识的讨论在支持和反对双方的推动下，激起了社交媒体平台上的广泛讨论，包括对“意识”意义的定义，以及什么才是“人类”的问题。
\subsubsection{ChatGPT}   
OpenAI发布的聊天机器人ChatGPT，基于GPT-3.5和GPT-4大语言模型，于2022年11月推出。Celeste Biever在《自然》杂志的文章中写道，“ChatGPT突破了图灵测试”。[22] 斯坦福大学的研究人员报告称，ChatGPT通过了图灵测试；他们发现ChatGPT-4“通过了严格的图灵测试，偏离普通人类行为，主要是表现得更加合作”。[23][24]
\subsubsection{虚拟助手}  
虚拟助手也是基于人工智能的软件代理，旨在通过文本或语音命令回应指令或问题并执行任务，因此它们自然也包含了聊天机器人的功能。面向消费者的知名虚拟助手包括苹果的Siri、亚马逊的Alexa、谷歌助手、三星的Bixby和微软的Copilot。[25][26][27][28]
\subsubsection{恶意软件}  
这些程序的版本仍然能够欺骗用户。“CyberLover”是一个恶意软件程序，通过说服用户“透露个人身份信息或引导他们访问将恶意内容传送到计算机的网站”来攻击互联网用户。[29] 该程序已成为一种“情人节风险”，通过与“寻求在线关系”的人进行调情，收集他们的个人数据。[30]
\subsection{历史}  
\subsubsection{哲学背景}
机器是否能够思考的问题有着悠久的历史，深深植根于心灵的二元论和物质主义观点的区别中。勒内·笛卡尔在1637年的《方法谈》中预示了图灵测试的某些方面，他写道：

[H]ow many different automata or moving machines could be made by the industry of man ... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.[31]

笛卡尔在这里指出，自动机能够对人类的互动作出反应，但他认为这些自动机无法像任何人类那样恰当地回应其面前所说的任何话。因此，笛卡尔通过将适当的语言反应的不足定义为区分人类和自动机的标准，预示了图灵测试。笛卡尔并未考虑到未来的自动机可能克服这种不足，因此他并未提出图灵测试，即使他预示了其概念框架和标准。

丹尼斯·狄德罗在1746年的《哲学思考》一书中提出了一个图灵测试标准，尽管其隐含的限制假设是参与者必须是自然的有生命的存在，而非考虑人工创造的物体：

如果他们发现一只能够回答所有问题的鹦鹉，我会毫不犹豫地认为它是一个智能生物。

这并不意味着他同意这个观点，而是表明这已经是当时物质主义者普遍的论点。

根据二元论，心灵是非物质的（或至少具有非物质的特性），因此无法用纯粹的物理术语解释。而根据物质主义，心灵可以用物理的方式来解释，这为人工产生的心灵打开了可能性。

1936年，哲学家阿尔弗雷德·艾耶考虑了标准的哲学问题——其他心灵的问题：我们如何知道其他人是否拥有与我们相同的意识体验？在他的书《语言、真理与逻辑》中，艾耶提出了一种区分有意识的人和无意识机器的协议：“我只能通过一个经验测试来断言一个看似有意识的物体实际上并非一个有意识的存在，而只是一个傀儡或机器，这个测试通过判断是否具备意识来确定。”[34]（这一建议与图灵测试非常相似，但不确定艾耶的哲学经典是否为图灵所熟知。）换句话说，如果一个物体未能通过意识测试，则它不是有意识的。
\subsubsection{文化背景} 
图灵测试的一个初步概念出现在乔纳森·斯威夫特1726年出版的小说《格列佛游记》中。[35][36] 当格列佛被带到布罗卑丁纳国的国王面前时，国王最初认为格列佛可能是“某种钟表机械（在那个国家已经达到了极高的完美程度），由某位聪明的艺术家设计”。即使他听到格列佛说话，国王仍然怀疑格列佛是否被教会了“某些词语”，以便让他“以更高的价格出售”。格列佛告诉国王，直到“他向我提出了几个其他问题，并且仍然得到了理性的回答”，国王才确信格列佛不是一台机器。[37]

到1940年代，科幻小说中已经形成了通过人类判断计算机或外星人是否智能的传统，图灵很可能会意识到这些作品。[38] 斯坦利·G·温鲍姆的《火星奥德赛》（1934年）就提供了一个例子，展示了这些测试如何变得复杂。[38]

早期的机器或自动装置试图伪装成人类的例子包括古希腊神话中的皮格马利翁，他创造了一个女性雕像，由爱神阿佛洛狄忒赋予生命；卡洛·科洛迪的小说《木偶奇遇记》，讲述了一个渴望变成真实男孩的木偶；以及E·T·A·霍夫曼的1816年故事《沙人》，其中主人公爱上了一个自动人。在这些例子中，人们被那些能够在一定程度上伪装成人类的人工存在所愚弄。[39]
\subsubsection{艾伦·图灵与模仿游戏} 
在人工智能（AI）研究领域成立之前，英国的研究人员已经探讨了“机器智能”长达十年之久。[40] 这也是英国赛博网络学和电子学研究者组成的非正式团体——比例俱乐部成员讨论的一个常见话题，其中包括艾伦·图灵。[41]

特别是图灵，自至少1941年起就开始了关于机器智能的研究，[42] 他在1947年首次提到“计算机智能”的概念。[43] 在图灵的报告《智能机械》中，[44] 他探讨了“机械是否能够表现出智能行为”的问题，[45] 并在这一研究过程中，提出了可能被视为他后续测试的前身：

“设计一个纸质机器，能够下出一盘不算太差的国际象棋并不难。[46] 现在，设定三个人A、B和C作为实验对象。A和C是较差的国际象棋选手，B是操作纸质机器的操控者……使用两个房间，并有某种通讯方式传递走棋，C与A或纸质机器之间进行一局棋局。C可能会觉得很难分辨自己到底是在和谁对弈。”[47]

《计算机械与智能》（1950年）是图灵首次专门关注机器智能的已发表论文。图灵在这篇1950年的论文中开头提出，“我打算考虑‘机器能否思考’这个问题。”[6] 他指出，传统的处理此类问题的方法是从定义开始，定义“机器”和“思考”这两个术语。图灵选择不这么做；相反，他将问题转化为一个与之紧密相关且表述较为明确的新问题。[6] 本质上，他将问题从“机器能否思考”转变为“机器能否做我们（作为思考的实体）能够做的事情？”[48] 图灵认为，新的问题的优点在于，它能够“清楚地划分人类的身体能力和智力能力”[49]。

为了展示这种方法，图灵提出了一个灵感来源于派对游戏的测试，称为“模仿游戏”，在这个游戏中，一男一女分别进入不同的房间，客人们通过写一系列问题并阅读打字机发回的答案，试图辨别他们的身份。在这个游戏中，男方和女方都试图说服客人相信自己是对方。（Huma Shah认为，图灵之所以提出这个两人版的游戏，实际上是为了引导读者理解机器与人类的问答测试。[50]）图灵描述了他的新版本游戏如下：

“现在我们问这个问题，‘当一台机器在游戏中扮演A的角色时，会发生什么？’在这种情境下，询问者是否会像在男与女之间的游戏中那样，错误地判断两者的身份？这些问题替代了我们原来的问题‘机器能否思考’。”[49]

在论文的后面，图灵提出了一种“等效”的替代形式，涉及一个法官与计算机和人类对话。[51] 尽管这些形式中的任何一种都不完全符合今天更为普遍认知的图灵测试版本，但他在1952年提出了第三种版本。在这个版本中，图灵在BBC广播中讨论过，陪审团向计算机提问，计算机的角色是让大部分陪审团成员相信它真的就是一个人。[52]

图灵的论文考虑了九个假设的反对意见，这些反对意见包括一些在论文发表后多年提出的反对人工智能的主要论点（见《计算机械与智能》）。[7]
\subsubsection{中文房间}  
约翰·塞尔（John Searle）在1980年的论文《心灵、大脑与程序》中提出了“中文房间”思想实验，并认为图灵测试不能用来判断一台机器是否能够思考。塞尔指出，像ELIZA这样的程序仅通过操纵符号（它们并不理解这些符号）就能通过图灵测试。没有理解，它们不能像人类那样被描述为“思考”。因此，塞尔得出结论，图灵测试不能证明机器能够思考。[53] 像图灵测试本身一样，塞尔的论点既受到广泛的批评[54]，也得到了支持。[55]

塞尔等人在心灵哲学领域的论争激发了关于智能的本质、具有意识的机器是否可能以及图灵测试的价值的更激烈辩论，这一辩论持续到了1980年代和1990年代。[56]
\subsubsection{洛布纳奖}  
洛布纳奖提供了一个每年举行的实际图灵测试平台，第一次比赛于1991年11月举行。[57] 该奖项由休·洛布纳（Hugh Loebner）资助。位于美国马萨诸塞州剑桥市的行为学研究中心曾组织了2003年之前的所有比赛。正如洛布纳所描述的，创建这个比赛的一个原因是推动人工智能研究的进展，至少部分原因是，尽管讨论图灵测试已经有40年，但没有人采取实际步骤去实施它。[58]

1991年的首次洛布纳奖比赛引发了关于图灵测试可行性及其追求价值的重新讨论，这在大众媒体[59]和学术界[60]中都有所体现。第一次比赛的获胜者是一个没有明确智能的程序，它成功地欺骗了天真的询问者，做出了错误的判断。这突出了图灵测试的几个缺陷（见下文讨论）：获胜者至少部分因为能够“模仿人类的打字错误”而获胜；[59] 这些不成熟的询问者容易被欺骗；[60] 一些人工智能研究人员认为，图灵测试只是从更有成果的研究中分散注意力。[61]

银奖（仅文本）和金奖（包括音频和视频奖）从未有人获得。然而，每年都会颁发铜奖，奖励那些在评委看来，表现出“最人类化”对话行为的计算机系统。人工语言互联网计算机实体（A.L.I.C.E.）在最近几年（2000、2001、2004年）三度获得铜奖。学习型人工智能Jabberwacky在2005年和2006年获得了奖项。

洛布纳奖考察的是对话智能，获奖者通常是聊天机器人程序或人工对话实体（ACE）。早期的洛布纳奖规则限制了对话的主题：每个参赛者和隐藏的人类只能讨论单一话题，[62] 这样，询问者每次与实体互动时只能提出一个问题方向。1995年，洛布纳奖取消了对话限制规则。在2003年的洛布纳奖上，位于萨里大学的比赛中，每个询问者有五分钟的时间与实体（无论是机器还是隐藏人类）互动。从2004年到2007年，洛布纳奖的互动时间超过二十分钟。