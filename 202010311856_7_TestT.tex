% TestTensor

\pentry{Matrix\upref{Mat}}

The word "tensor" was first translated in Structural Mechanics, used to describe the properties of stress, therefore translated as “张力”. The term continues in use today, except the translation now becomes “张量”. Today we realize that the stress tensor is only a particular case for a more general mathematical object, tenosr. This object appears everywhere in physics frequently. We will define the concept of tensor from the fundamental sense, which is the contemporary perspective. We will also demonstrate how a tensor is represented by a matrix or a high dimensional matrix. 

This section is a step-by-step guiding textbook, rather than a list of concepts and theorems. To better understand this section, it is recommended that the reader have solid understanding of linear algebra. 

The definition we use in this section is the most fundamental version. No additional structures or constraints are put onto it; also, Einstein's Summation Agreement is not applied yet, so as to be friendly for beginners. For the further concepts like covariant or contravariant tensors, please continue to the subsequent sections. 



\subsection{Introduction}

Students in physics often ask a question: "what is a tensor?" Usually the students are actually asking "why the moment of intertia, the mass, the stress, the electrical polarization are tensors? What do they have in common? " To understand the essential common property of all these seemingly different quantaties, we have to clarify the very fundamental definition of a tensor. That is, to answer the real question, we have to answer the literal question. So let's take some time to focus on the mathematical term itself, before we natrually demonstrate why those things are all tensors. 

A tensor is a multi-linear map. Particularly, a second-order tensor can be interpreted as a map from two vectors to a scalor, or a map from a vector to another vector. The scalor here refers to an element in the field that defines the vector space; in physics, the field is usually the set of real numbers or complex numbers. 

Many physical quantities can be interpreted as linear maps. As an example, mass is a second-order tensor, which maps a force vector to an acceleration vector; the moment of inertia is a second-order tensor, which maps an angular velocity vector to an angular momentum vector. Tensors of different orders may behave differently, but they all share one property: the linearity. When you encounter tensors in the future, I suggest you take some time and think "what quantities does this tensor map from and to"; it helps a deeper understanding. 



\subsection{1-linear function, 2-linear function and their representation}

It is known that given 2 vector spaces $V$ and $W$, we can define a linear map from $V$ to $W$. In this case, we have only 1 independent and 1 dependent variables. A multi-linear map, say, a k-linear map, maps a series of vector spaces $V_1, V_2, \cdots, V_k$ to $W$, so that if only the element in one $V_i$ varies, the map varies in the way a linear map does. 

The simplest case is when $W$ is a 1-dimensional space. In this case, the vector space $W$ is also the field (the real numbers). If the image of a map is some set of numbers, then we have a particular term for such a map, \textbf{function}. This is the same function that you may have seen in high school, as the maps involved in high school are mostly from real numbers to real numbers. Now, since we make $W$ the set of real numbers, we just call the map from $V_1, V_2, \cdots, V_k$ to $W$ a \textbf{linear function}. Now, we are going to introduce the concepts of multi-linear maps, tensors and their representations, step by step, starting with linear functions. 

\subsubsection{Representing linear functions with vectors}

\begin{definition}{Linear functions}
Given the n-dimensional vector space $V$ on $\mathbb{R}$, we call $f:V\rightarrow \mathbb{R}$ a \textbf{linear function} from $V$ to $\mathbb{R}$, if and only if $f$ satisfies the linearity: for any vectors $\bvec{v}_1, \bvec{v}_2\in V$ and scalors $a_1, a_2\in\mathbb{R}$, we have: $a_1f(\bvec{v}_1)+a_2f(\bvec{v}_2)=f(a_1\bvec{v}_1+a_2\bvec{v}_2)$．

\end{definition}


If we think of $\mathbb{R}$ as a 1-dimensional vector space, then $f$ is a linear map from $V$ onto this vector space. Therefore, if we take any base of $V$ and determine what real numbers $f$ maps the base vectors to, then we can calculate $f(\bvec{v})$ for any vector $\bvec{v}\in V$. 

Now suppose a base of $V$ is $\{\bvec{e}_i\}^n_{i=1}$. If the base vector $\bvec{e}_i$ is mapped to $f(\bvec{e}_i)=m_i\in\mathbb{R}$, then for any vector $\bvec{v}=a_1\bvec{e}_1+a_2\bvec{e}_2+\cdots+a_n\bvec{e}_n$, we can calculate by linearity: 

\begin{equation}
\begin{aligned}
f(a_1\bvec{e}_1+a_2\bvec{e}_2+\cdots+a_n\bvec{e}_n)&=f(a_1\bvec{e}_1)+f(a_2\bvec{e}_2)+\cdots+f(a_n\bvec{e}_n)\\&=m_1a_1+m_2a_2+\cdots+m_na_n
\end{aligned}
\end{equation}


$m_1a_1+m_2a_2+\cdots+m_na_n$can be interpreted as the inner product of $\bvec{m}$ and $\bvec{v}$, where$\bvec{m}=m_1\bvec{e}_1+m_2\bvec{e}_2+\cdots+m_n\bvec{e}_n$．

That is, every linear function $f$ corresponds to a vector $\bvec{m}$, so that $f(\bvec{v})=\bvec{m}\cdot\bvec{v}$．The coordinate of the vector $\bvec{m}$, in the base $\{\bvec{e}_i\}_{i=1}^n$, is $(m_1, m_2, \cdots, m_n)^T$. If we define another base, the coordinate usually changes, but the vector is still the same. 

Since a linear function is a map from a vector space to real numbers, we also call it a 1-linear function. It is to distinguish it from other multi-linear functions, like the 2-linear function we are going to focus on now. 

\subsubsection{Representing 2-linear funcions with matices}

Let's stick to the n-dimensional vector space $V$, except now we take 2 copies of $V$ to construct the map $f:V\times V\rightarrow\mathbb{R}$．

\begin{definition}{2-linear function}
We call $f:V\times V\rightarrow\mathbb{R}$ a 2-linear function, if and only if for any fixed vector $\bvec{v}_0$, both $f(\bvec{v}_0, \bvec{v})$ and $f(\bvec{v}, \bvec{v}_0)$ are 1-linear functions for $\bvec{v}$. In particular, we call $f$ a 2-linear function from $(V, V)$ onto $\mathbb{R}$. 
\end{definition}

An explicit expression for a 2-linear function is as follows: 

\begin{equation}\label{TestT_eq2}
\begin{aligned}
f(a_1\bvec{v}_1+a_2\bvec{v}_2, b_1\bvec{u}_1+b_2\bvec{u}_2)&=a_1f(\bvec{v}_1, b_1\bvec{u}_1+b_2\bvec{u}_2)+a_2f(\bvec{v}_2, b_1\bvec{u}_1+b_2\bvec{u}_2)\\&=a_1b_1f(\bvec{v}_1, \bvec{u}_1)+a_2b_1f(\bvec{v}_2, \bvec{u}_1)+a_1b_2f(\bvec{v}_1, \bvec{u}_2)+a_2b_2f(\bvec{v}_2, \bvec{u}_2)
\end{aligned}
\end{equation}


In order to represent this map, we have to determine the bases for the two copies of $V$. They don't have to be the same base. Let's assume the base for the first $V$ is $\{\bvec{e}_i\}^n_{i=1}$, and for the second is $\{\bvec{e}_i'\}^n_{i=1}$. Now vectors in the two vector spaces can be represented by column matrices, and the 2-linear function by a square matrix: (example with a 2-dimesional $V$)
\begin{equation}\label{TestT_eq1}
\begin{aligned}
&\bvec{v}=a_1\bvec{e}_1+a_2\bvec{e}_2\rightarrow  \bvec c_v=\pmat{a_1\\a_2}\\
&\bvec{u}=b_1\bvec{e}'_1+b_2\bvec{e}'_2\rightarrow  \bvec c_u=\pmat{b_1\\b_2}\\
&f\rightarrow \bvec{M}=\pmat{f(\bvec{e}_1, \bvec{e}'_1)&f(\bvec{e}_1, \bvec{e}'_2)\\f(\bvec{e}_2, \bvec{e}'_1)&f(\bvec{e}_2, \bvec{e}'_2)}\\
\end{aligned}
\end{equation}


That way, we have $f(\bvec{v}, \bvec{u})= \bvec c_v^T\bvec{M} \bvec c_u$．Note that $\bvec c_v^T$ means the transpose of the matix $\bvec c_v$\footnote{If we use other bases, then the representations of $\bvec{v}$, $\bvec{u}$ and $f$ may be different matrices, but the calculation of $\bvec c_v^T\bvec{M} \bvec c_u$ doesn't change}．

\begin{exercise}{}
Think of the coordinate of a vector as a column matrix, then use to the laws of matrix calculation and substitute \autoref{TestT_eq1} into \autoref{TestT_eq2} to verify that $f(\bvec{v}, \bvec{u})= \bvec c_v^T\bvec{M} \bvec c_u$．
\end{exercise}


The example we just talked about is for 2-dimensional $V$. For n-dimensional $V$, $\bvec{M}$ becomes a $n\times n$ matrix, where the element in the ith row and jth column is $m_{ij}=f(\bvec{e}_i, \bvec{e}'_j)$．

Let's take a look at a simple 2-linear function. 

\begin{example}{Power}
The possible forces exerted on a point mass in the 3-dimensional space, make a 3-dimensional real vector space $V$. The possible velocities of this point mass alsk make a $V$\footnote{Recall: 2 vector spaces are isomorphic (equivalent) if and only if they are defined over a same field and have the same dimensions. }．Represent the force on the point by $\bvec{F}$, the velocity of the point by $\bvec{v}$, then the power of the force on the point is $P$, a 2-linear function: $P=\bvec{F}\cdot\bvec{v}$．If we determine the bases for the force space and the velocity space, we can also represent $\bvec{F}$ and $\bvec{v}$ as column matrices $c_F$ and $c_v$, respectively. Then, there is a matrix $\bvec{P}$ so that $P=c^T_F\bvec{P}c_v$. Here, $\bvec{P}$ is a 2-linear function that maps a force and a velocity to a power. If we choose the bases in the way: the base forces are along the positive directions of $x$, $y$ and $z$ with magnitude $1N$, and the base velocities are along the positive directions of $x$, $y$ and $z$ with magnitude $1\opn{m/s}$, then for these two bases, $\bvec{P}=\pmat{1&0&0\\0&1&0\\0&0&1}$；If we choose the base forces to be along the positive directions of $y$, $x$ and $z$, then $\bvec{P}$ becomes $\pmat{0&1&0\\1&0&0\\0&0&1}$. If, \textbf{again}, we change the base velocities to be along the positive directions of $x$, $z$ and $y$, then $\bvec{P}$ becomes $\pmat{0&0&1\\1&0&0\\0&1&0}$．
\end{example}



We can see that functiuon of power, $P$, maps the two vectors, force and velocity, to a real number, power. If we choose different base for the space of forces and the space of velocities, the matrix representation of $P$ also differs. These matrices all represent the properties of $P$ in corresponding bases. Compare this fact to the linear map that you learned from linear algebra. The matrix representation of the same linear map may vary as the bases for the vector spaces vary; similarly, the tensor $P$ doesn't change as the bases change, but the matrix representation of it does. 

\subsubsection{Review on matrix calculations}

In order to conveniently generalize the concept of 2-linear functions, we require the concept of matrix whose elements are vectors. We will briefly review the matrix calculations before proceeding with vector matrices. 

Matrix calculation is just an arrangement of addition and multiplication. Although we usually deal with matrices whose elements are numbers, we can apply the same arrangement to any set whose elements can "add" and "multiply". 

\begin{example}{Examples for vector matrices}

\begin{itemize}
%
\item We can multiply a vector with a number (the scalor product). If $\bvec{v}_i$ is a vector, $a_i$ is a number, then $\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}$ is a row matrix of 3 vectors，$\pmat{a_1\\a_2\\a_3}$ is a column matrix of 3 numbers. Use the scalor product as the multiplication, we have: $\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}\pmat{a_1\\a_2\\a_3}=(a_1\bvec{v}_1+a_2\bvec{v}_2+a_3\bvec{v}_3)$ is a vector, and $\pmat{a_1\\a_2\\a_3}\pmat{\bvec{v}_1&\bvec{v}_2&\bvec{v}_3}=\pmat{a_1\bvec{v_1}&a_1\bvec{v_2}&a_1\bvec{v_3}\\a_2\bvec{v_1}&a_2\bvec{v_2}&a_2\bvec{v_3}\\a_3\bvec{v_1}&a_3\bvec{v_2}&a_3\bvec{v_3}}$ is a square matrix of 9 vectors. 
\item We can multiply two vectors with inner product. Using this multiplication, $\pmat{\bvec{v}_1&\bvec{v}_2\\\bvec{v}_3&\bvec{v}_4}\cdot\pmat{\bvec{v}_5\\\bvec{v}_6}=\pmat{\bvec{v}_1\cdot\bvec{v}_5+\bvec{v}_2\cdot\bvec{v}_6\\\bvec{v}_3\cdot\bvec{v}_5+\bvec{v}_4\cdot\bvec{v}_6}$ is a column matrix of 2 numbers. 
\item We can multiply 3-dimensional vectors with cross product. Using this multiplication, $\pmat{\bvec{v}_1&\bvec{v}_2\\\bvec{v}_3&\bvec{v}_4}\times\pmat{\bvec{v}_5\\\bvec{v}_6}=\pmat{\bvec{v}_1\times\bvec{v}_5+\bvec{v}_2\times\bvec{v}_6\\\bvec{v}_3\times\bvec{v}_5+\bvec{v}_4\times\bvec{v}_6}$ is a column matrix of 2 vectors. 
%
\end{itemize}
\end{example}

\subsubsection{Representing 2-linear functions with vector matrices}

Suppose we have a 2-linear function $f:V^2\rightarrow\mathbb{R}$. It has a matrix representation under the base $\{\bvec{e}_i\}_{i=1}^n$: (for convenience, suppose the 2 copies of $V$ share this base)
\begin{equation}
\pmat{m_{ij}}=\pmat{m_{11}&m_{12}&\cdots&m_{1n}\\ m_{21}&m_{22}&\cdots&m_{2n}\\ \vdots&\vdots&\ddots&\vdots\\ m_{n1}&m_{n2}&\cdots&m_{nn}}
\end{equation}
If we take every row as a row matrix, namely $\bvec{m}_i^T=\pmat{m_{i1}&m_{i2}&\cdots&m_{in}}$, then $f$ can also be expressed as "a row matrix of row matrices"" 

\begin{equation}
\bvec{M}=\pmat{\bvec{m}_1^T&\bvec{m}_2^T&\cdots&\bvec{m}_n^T}
\end{equation}

For any vectors $\bvec{v}, \bvec{u}\in V$ if their coordinates under the base are $c_v$ and $c_u$, then $\bvec{M}c_v$ is a row matrix, and $\bvec{M}c_vc_u$ is a number in $\mathbb{R}$. 

Hence, the row matrix of row matrices, $\bvec{M}$, represents the 2-linear function $f$. We can express this representation explicitly: $f(\bvec{v}, \bvec{u})=\bvec{M}\bvec{v}\bvec{u}$\footnote{Here, we treat $\bvec{m}_1^T$ as an element from $\bvec{M}$，and vector $\bvec{v}, \bvec{u}$ as column matrices, then apply the matrix multiplication defined by scalor product. }．

You may have noticed that the expression starts to look complicated. Good news is that in subsequent sections we will introduce Einstein's Summation Agreement, which simplifies the expressions by a lot. 

\subsection{Tensors are linear functions}

A 2-linear function is also named a \textbf{second-order tensor}. Similarly, a 1-linear function is a \textbf{first-order tensor}. Further more, a real number (or anything from the field) is a \textbf{zero-order tensor}. This is one of the definition of tensors in modern mathematics: a tensor of order k is a k-linear function. 

But a tensor does NOT only map things to numbers; that is, tensors are more than linear functions; they are linear maps, as we are discussing as follows. 

\subsection{Reflections on 2-linear functions}
Let's observe a 2-linear function once again. We define a 2-linear function as a map from 2 vectors to 1 number, but in the example of "representing a 2-linear function with a vector matrix", we also realize that a 2-linear function can also map 1 vector to 1 linear function. In the case of the example, $f$ maps $c_v$ onto $\bvec{M}c_v$上; if we think of $\bvec{M}c_v$ as a whole, we notice that it is a 1-linear function of $c_u$. Now that a 1-linear funtion is interpreted as a vector, we can also interpret a 2-linear function as a map from 1 vector to 1 vector. 

To wrap up, a 2-linear function is a map \textbf{from 2 vectors to 1 number}, \textbf{from 1 vector to a 1-linear function}, or \textbf{from 1 vector to 1 vector}. Does the last interpretation look familiar? Yes, in this context, a 2-linear function is a "linear transformation" or a "linear map from one vector space to another", as we have studied a lot in linear algebra. 

With the new interpretations, it is possible to make more examples for 2-linear maps. 

\begin{example}{Moment of inertia}


The angular velocity is defined as a vector, with the angular speed as its magnitude and the direction defined with the righ-hand-rule. Given a reference point, the angular momentum of a particle is defined as $\bvec{r}\times m\bvec{v}$, where $\bvec{r}$ is the displacement vector from the reference point to the location of the particle, $m$ is the mass of the particle, and $\bvec{v}$is the velocity of the particle. 一个质点系统的角动量，定义为所有质点的角动量之和．

对于一个质量均匀分布的球体，由于它高度对称，如果它的角速度是$\bvec{\omega}$，那么它的角动量也沿着$\bvec{\omega}$的方向，因此可以表示为$\bvec{L}=I\bvec{\omega}$，其中$\bvec{L}$是球体的角动量，$I$被称为转动惯量．

但是，如果一个刚体是$x$轴上一对对称质点构成的，它们的质量都是$m$，到原点的距离都是$r$，那么当这个刚体的角速度在$z$方向上时，角动量非零，且也在$z$方向上；当角速度在$x$方向上时，角动量为零．也就是说，转动惯量一般没法简单表示为一个实数$I$，而应该用一个$2$-线性函数来表示．在这个例子中，在给定的直角坐标系上，转动惯量表示为一个矩阵
\begin{equation}
\pmat{mr^2&0&0\\ 0&0&0\\ 0&0&0}
\end{equation}

拿出你的手机，把它近似看成一个长方体．取一个直角坐标系，使得三个坐标轴分别垂直于手机的正面、侧面和顶面，那么你的手机的转动惯量就可以表示成
\begin{equation}
\pmat{a&0&0\\ 0&b&0\\ 0&0&c}
\end{equation}
其中通常有$a>b>c$，因为你的手机的厚度从薄到厚依次是正面、侧面和顶面对应的方向．如果选用其它坐标系，这个转动惯量通常就是另一个矩阵的样子，但不管选什么坐标系表示手机的转动惯量矩阵，它们都是相似的．

事实上，转动惯量都应该是双线性函数而非一个数，即使对于前面所说的球体，也应该把它的转动惯量写为
\begin{equation}
\pmat{I&0&0\\ 0&I&0\\ 0&0&I}
\end{equation}
\end{example}








