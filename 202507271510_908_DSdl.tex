% 大数定律（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Law_of_large_numbers}{相关文章}。

在概率论中，大数定律是一条数学规律，它指出：在大量相互独立的随机样本中获得的结果的平均值将收敛于真实值（如果这个真实值存在）【1】。更正式地说，大数定律表明：对于一组独立同分布的样本，其样本均值将趋于真实的数学期望。

大数定律的重要性在于，它为某些随机事件的平均值提供了长期稳定的保证【1】【2】。例如，一个赌场在某次轮盘赌中可能会亏损，但在大量旋转后，其收益将趋近于一个可预测的百分比。任何玩家的连胜最终也会被游戏设定所“拉回”。需要注意的是，大数定律（顾名思义）只在观测次数足够大时才适用。并没有任何原则表明少数几次的观测结果就会接近期望值，或者说某种结果的连发会立即被其他结果“平衡”（参见赌徒谬误）。

大数定律仅适用于重复试验结果的平均值，并声称这个平均值会收敛到期望值；它并不意味着随着试验次数 $n$ 的增加，结果的总和一定会接近 $n$ 倍的期望值。

在其发展历程中，许多数学家对大数定律进行了不断完善。如今，大数定律被广泛应用于统计学、概率论、经济学以及保险学等多个领域【3】。
\subsection{示例}
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/614919ff753b49e7.png}
\caption{用一次掷单个骰子的特定实验来说明大数定律。随着该实验中掷骰次数的增加，所有结果数值的平均值逐渐趋近于 3.5。尽管在掷骰次数较少时（左侧）每次实验的曲线形状会有明显差异，但当掷骰次数增多时（右侧），这些曲线的形状将会极为相似。} \label{fig_DSdl_1}
\end{figure}
例如，掷一次六面骰子会得到 1、2、3、4、5 或 6 中的一个结果，每个数字出现的概率相等。因此，这次掷骰子的**期望值**是：
$$
\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5~
$$
根据大数定律，如果掷出大量的六面骰子，这些点数的平均值（有时称为样本均值）将趋近于 3.5，且随着掷骰数量的增加，结果的精确度也会提高。

大数定律还意味着，在一系列伯努利试验中，成功的经验概率会收敛于理论概率。对于伯努利随机变量，其期望值就是成功的理论概率，而 $n$ 个这样的变量的平均值（假设它们是独立同分布（i.i.d.）的）恰好就是相对频率。
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/35cb06686a7cb2ff.png}
\caption{这张图说明了相对频率向理论概率收敛的过程。假设从一个袋子中抽取红球的概率是 0.4，抽取黑球的概率是 0.6。左侧图显示的是抽到黑球的相对频率，右侧图显示的是抽到红球的相对频率，二者均基于 10,000 次试验。随着试验次数的增加，相对频率逐渐趋近于各自的理论概率，从而体现了大数定律的作用。} \label{fig_DSdl_2}
\end{figure}
例如，掷一枚公平硬币就是一次伯努利试验。当公平硬币被掷一次时，结果为正面的理论概率是 $\frac{1}{2}$。因此，根据大数定律，在进行“大量”掷硬币试验时，正面朝上的比例“应当”大约为 $\frac{1}{2}$。特别地，随着掷币次数 $n$ 趋近于无穷，正面朝上的比例将几乎必然地收敛到 $\frac{1}{2}$。

尽管正反面比例趋近于 $\frac{1}{2}$，正反面次数的绝对差值却几乎必然会随着试验次数增加而变大。也就是说，这个差值保持在一个很小的数值的概率会趋近于零。不过，这个差值与试验总次数的比例几乎必然趋近于零。从直觉上说，差值确实在增长，但它的增长速度比总次数慢。

另一个很好地体现大数定律的例子是蒙特卡洛方法。这是一大类依赖重复随机抽样以获得数值结果的计算算法。重复次数越多，得到的近似结果通常也越准确。该方法的重要性主要在于，有时候使用其他方法求解是非常困难甚至不可能的【4】。
\subsection{局限性}
在某些情况下，从大量试验中获得的结果的平均值可能不会收敛。例如，从柯西分布或某些 帕累托分布（Pareto distribution，α<1）中抽取的 $n$ 个结果的平均值不会随着 $n$ 增大而收敛，其原因在于这些分布具有重尾特性。[5] 柯西分布是一个没有数学期望的分布[6]，而当帕累托分布的参数 α 小于 1 时，其期望值为无穷大。[7]生成一个柯西分布的例子的方法之一是：令随机数等于一个在区间 $-90^\circ$ 到 $+90^\circ$ 内均匀分布的角度的正切值。[8] 这个分布的中位数为 0，但其数学期望不存在，并且 $n$ 个此类变量的平均值具有与其中任意一个变量相同的分布。因此，即使 $n$ 趋于无穷大，这个平均值在概率意义上不会收敛到 0 或任何其他值。

此外，如果试验中存在选择性偏差——例如人类的经济或理性行为中常见的偏差——即使增加试验次数，大数定律也无法消除这种偏差，偏差仍然会存在。
\subsection{历史}
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/2e2b2dc3ee5caed4.png}
\caption{扩散现象是大数法则的一个实例。一开始，容器中隔板（品红色线）左侧存在溶质分子，而右侧则没有。移除隔板后，溶质开始扩散，最终填满整个容器。顶部：当只有一个分子时，其运动看起来是完全随机的。中部：当分子数量增加时，虽然仍然存在随机波动，但可以明显看出溶质趋向于更加均匀地填满容器。底部：当溶质分子的数量极其庞大（多到无法看见）时，随机性基本消失：溶质似乎平滑而有规律地从高浓度区域流向低浓度区域。在现实情况下，尽管扩散的本质是随机的，化学家仍可以将其视为一种确定性的宏观现象（参见菲克定律）。} \label{fig_DSdl_3}
\end{figure}
意大利数学家杰罗拉莫·卡尔达诺（Gerolamo Cardano，1501–1576）曾在未加证明的情况下指出，经验统计的准确性随着试验次数的增加而提高。[9][3] 后来，这一观点被正式表述为“大数法则”。大数法则的一种特殊形式（适用于二元随机变量）最早由雅各布·伯努利证明。[10][3] 他花费了超过 20 年时间，才发展出足够严谨的数学证明，并于 1713 年在其著作《概率术》中发表。他将其称为“金色定理”，但后世更常称之为“伯努利定理”。这一定理不应与“伯努利原理”混淆，后者是以雅各布的侄子丹尼尔·伯努利的名字命名的。1837 年，西梅翁·德尼·泊松以“la loi des grands nombres”（法语，意为“大数法则”）的名称进一步描述了这一原理。[11][12][3] 此后，这一法则在文献中以这两个名称交替出现，但“law of large numbers”（大数法则）是最常用的名称。

在伯努利和泊松发表其成果之后，其他数学家也对大数法则进行了改进与拓展，其中包括切比雪夫（、马尔可夫、博雷尔、坎特利、柯尔莫哥洛夫和欣钦等人。[3] 马尔可夫证明了在某些较弱的条件下，大数法则仍适用于方差不存在的随机变量；1929 年，欣钦进一步证明，如果一组独立同分布的随机变量具有期望值，那么弱大数法则就成立。[14][15] 这些后续研究发展出了大数法则的两个主要形式：一种称为“弱大数法则”，另一种称为“强大数法则”，分别对应于样本均值趋近于期望值的两种不同收敛方式；特别地，如下文所解释的，强大数法则蕴含弱大数法则。[14]
\subsection{形式}
大数法则有两个不同的版本，称为强大数法则和弱大数法则【16】【1】。以下是它们在如下情形下的表述：设 $X_1, X_2, \dots$ 是一个无限序列的独立同分布（i.i.d.）的勒贝格可积随机变量，其期望满足：
$$
\mathbb{E}(X_1) = \mathbb{E}(X_2) = \cdots = \mu~
$$
这两个版本都断言样本平均值：
$$
\overline{X}_n = \frac{1}{n}(X_1 + \cdots + X_n)~
$$
收敛于期望值：
$$
\overline{X}_n \to \mu \quad \text{当 } n \to \infty~
$$
（其中“勒贝格可积”意味着 $\mathbb{E}(X_j)$ 按照勒贝格积分存在且有限；这并不意味着相关的概率测度对勒贝格测度绝对连续。）

入门级的概率教材通常还额外假设每个随机变量具有相同的有限方差，即：
$$
\operatorname{Var}(X_i) = \sigma^2 \quad \text{对所有 } i~
$$
并且各随机变量之间不相关。在这种情况下，$n$个随机变量的样本均值的方差为：
$$
\operatorname{Var}(\overline{X}_n) = \operatorname{Var}\left(\frac{1}{n}(X_1 + \cdots + X_n)\right) = \frac{1}{n^2} \operatorname{Var}(X_1 + \cdots + X_n) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}~
$$
这个公式可用于简化和缩短相关证明。但这种有限方差的假设并非必要。即使存在较大或无限的方差，只是会导致收敛速度变慢，大数法则依然成立【17】。

随机变量的相互独立性也可以被两两独立性【18】或可交换性【19】所取代，两种形式的大数法则仍然适用。

强大数法则和弱大数法则之间的区别在于所主张的收敛方式不同。关于这些收敛方式的解释，可参见“随机变量的收敛性”相关内容。
\subsubsection{弱大数法则}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/d7530671329bae27.png}
\caption{模拟演示大数法则：每一帧中都会抛掷一枚硬币，其一面为红色，另一面为蓝色，并在对应的柱状图列中添加一个点。饼图显示当前红色与蓝色的比例。可以看到，虽然在初期比例波动较大，但随着试验次数的增加，比例逐渐趋近于 50\%。} \label{fig_DSdl_4}
\end{figure}
弱大数法则（也称为欣钦法则，Khinchin's law）指出：如果一组样本是从具有有限期望值的随机变量中独立同分布（i.i.d.）地抽取的，那么其样本均值将按概率收敛于该期望值：
$$
\overline{X}_n \xrightarrow{P} \mu \quad \text{当 } n \to \infty~
$$
也就是说，对于任意正数 ε，有：
$$
\lim_{n \to \infty} \Pr\left(|\overline{X}_n - \mu| < \varepsilon\right) = 1~
$$
解释这个结果：弱大数法则意味着，对于任意非零的容差（$\varepsilon$），无论多小，只要样本量足够大，样本均值以极高的概率落在该容差范围内，也就是说，会非常接近于期望值。

正如前面提到的，弱大数法则适用于 i.i.d. 随机变量的情形，但它也适用于其他一些情况。例如：序列中的每个随机变量的方差可以不同，只要它们的期望值保持不变。如果这些方差是有界的，那么该法则依然成立，这一点早在 1867 年就被切比雪夫证明过。

（如果期望值在序列中发生变化，我们可以将法则应用于每个变量与其期望值之间的平均偏差。此时法则表明，这一偏差按概率收敛于零。）事实上，只要前 $n$ 项的样本均值的方差在 $n \to \infty$ 时趋于零，切比雪夫的证明就成立【15】。例如，假设该序列中的每个随机变量都服从均值为 0、方差为 $2n/\log(n+1)$ 的高斯分布（正态分布）。虽然方差不是有界的，但每一步的样本均值仍服从正态分布（因为是正态变量的平均值）。此时，总和的方差是各项方差之和，渐近等于 $n^2 / \log n$，因此样本均值的方差渐近等于 $1 / \log n$，并趋于零。还有一些例子表明，即使期望值不存在，弱大数法则也可能适用。
