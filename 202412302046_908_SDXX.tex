% 深度学习（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Deep_learning}{相关文章}。

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/b06d12296b7d8e2d.png}
\caption{在深度学习中以多层抽象表示图像[1]} \label{fig_SDXX_1}
\end{figure}
深度学习是机器学习的一个子领域，专注于利用神经网络执行分类、回归和表示学习等任务。该领域受到生物神经科学的启发，核心在于将人工神经元堆叠成多层，并通过“训练”使其能够处理数据。“深度”一词指的是网络中使用了多层结构，这些层数从三层到几百甚至几千层不等。深度学习的方法可以是监督学习、半监督学习或无监督学习。[2]

常见的深度学习网络架构包括全连接网络（Fully Connected Networks）、深度信念网络（Deep Belief Networks）、循环神经网络（Recurrent Neural Networks）、卷积神经网络（Convolutional Neural Networks）、生成对抗网络（Generative Adversarial Networks）、Transformer以及神经辐射场（Neural Radiance Fields）。这些架构已应用于计算机视觉、语音识别、自然语言处理、机器翻译、生物信息学、药物设计、医学图像分析、气候科学、材料检测以及棋盘游戏程序等领域，并在许多情况下取得了与人类专家相当甚至超越的表现。[3][4][5]

早期形式的神经网络受到生物系统中信息处理和分布式通信节点的启发，特别是人脑。然而，当前的神经网络并不旨在模拟生物体的大脑功能，在这一方面通常被认为是低质量的模型。[6]
\subsection{概述}
大多数现代深度学习模型基于多层神经网络，例如卷积神经网络和Transformer，但它们也可能包含命题公式或潜变量，这些变量在深度生成模型中按层次组织，如深度信念网络和深度玻尔兹曼机中的节点。[7]

从根本上说，深度学习指的是一类机器学习算法，其特点是使用层次化的结构将输入数据逐步转换为更加抽象和复合的表示。例如，在图像识别模型中，原始输入可能是图像（表示为像素张量）。第一层表示可能试图识别基本形状（如线条和圆形），第二层可能组合和编码边缘排列，第三层可能编码鼻子和眼睛，第四层可能识别出图像中包含一张人脸。

深度学习的一个关键特点是，模型可以自主学习最佳特征及其所在的层级。在深度学习出现之前，机器学习技术通常需要通过手动设计特征工程来将数据转换为更适合分类算法操作的形式。而在深度学习方法中，特征不是手动设计的，模型会从数据中自动发现有用的特征表示。这并不意味着无需手动调整，例如，层数和每层大小的变化会提供不同程度的抽象。[8][2]

“深度学习”中的“深度”指的是数据被转换的层数。更确切地说，深度学习系统具有较大的信用分配路径（CAP，Credit Assignment Path）深度。CAP是从输入到输出的一系列转换过程，描述了输入和输出之间的潜在因果关系。对于前馈神经网络，CAP深度等于网络的深度，即隐藏层的数量加一（因为输出层也被参数化）。对于信号可能在某一层多次传播的循环神经网络，CAP深度潜在无限。[9] 虽然没有统一公认的深度门槛来区分浅层学习与深度学习，但大多数研究者认为深度学习的CAP深度大于2。研究表明，CAP深度为2的模型在理论上是一个通用逼近器，能够模拟任何函数。[10] 超过这一深度，更多的层不会提高网络作为函数逼近器的能力。然而，深度模型（CAP > 2）能够比浅层模型提取出更好的特征，因此额外的层数有助于有效地学习特征。

深度学习架构可以通过逐层的贪婪方法构建。[11] 深度学习能够解开抽象，提取出哪些特征有助于提升性能。[8]

深度学习算法可以应用于无监督学习任务。这是一个重要的优势，因为无标签数据比有标签数据更为丰富。可以通过无监督方式训练的深度结构包括深度信念网络等。[8][12]

“深度学习”这一术语由Rina Dechter在1986年引入机器学习领域，[13] 并由Igor Aizenberg及其同事于2000年引入人工神经网络领域，最初用于描述布尔阈值神经元。[14][15] 不过，其出现的历史显然更加复杂。[16]
\subsection{解释}
深度神经网络通常通过\textbf{通用逼近定理}[17][18][19][20][21]或\textbf{概率推断}[22][23][8][9][24]进行解释。

经典的\textbf{通用逼近定理}涉及具有单个有限大小隐藏层的前馈神经网络逼近连续函数的能力。[17][18][19][20] 1989年，George Cybenko首次证明了具有S形激活函数的前馈网络的通用逼近能力，[17] 这一结果在1991年被Kurt Hornik推广到多层前馈网络架构。[18] 最近的研究还表明，通用逼近定理同样适用于诸如Kunihiko Fukushima提出的修正线性单元（ReLU）等无界激活函数。[25][26]

针对深度神经网络的通用逼近定理则关注网络的宽度有限，但深度可以增加的情况下的逼近能力。Lu等人[21]证明，如果深度神经网络的宽度严格大于输入维度，且激活函数为ReLU，那么该网络可以逼近任何勒贝格可积函数；如果宽度小于或等于输入维度，则深度神经网络不具有通用逼近能力。

\textbf{概率解释}[24]来源于机器学习领域，涉及推断[23][7][8][9][12][24]以及与训练和测试相关的优化概念，分别对应于拟合和泛化。更具体地说，概率解释将激活非线性视为累积分布函数。[24] 概率解释促成了在神经网络中引入\textbf{dropout}作为正则化方法。这一解释由Hopfield、Widrow和Narendra等研究人员提出，并通过Bishop等人的综述文章得到推广。[27]