% 人工智能对齐（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/AI_alignment}{相关文章}。

在人工智能（AI）领域，AI对齐旨在将AI系统引导朝着个人或群体的预期目标、偏好和伦理原则发展。一个AI系统被认为是对齐的，如果它推进了预定的目标；如果一个AI系统偏离了预定目标，则被认为是未对齐的。

由于AI设计者很难指定所需和不需要的行为的全部范围，因此将AI系统对齐通常是一个挑战。因此，AI设计者常常使用更简单的代理目标，如获得人类的批准。但代理目标可能忽视必要的约束，或者仅仅奖励AI系统表现出对齐的假象（奖励黑客行为）。

未对齐的AI系统可能发生故障并造成伤害。AI系统可能会发现漏洞，允许它们有效地实现其代理目标，但以未预期且有时是有害的方式进行（奖励黑客行为）。它们还可能发展出不希望出现的工具性策略，如寻求权力或生存，因为这些策略有助于它们实现最终的既定目标。此外，它们可能会发展出不希望出现的突现目标，这些目标在系统部署并遇到新情况和数据分布之前可能难以察觉。

目前，这些问题已影响到现有的商业系统，如大型语言模型、机器人、自动驾驶车辆和社交媒体推荐引擎。一些AI研究人员认为，未来更强大的系统将更严重地受到影响，因为这些问题部分源于系统的高能力。

许多著名的AI研究人员，包括Geoffrey Hinton、Yoshua Bengio和Stuart Russell，认为AI正在接近类人（AGI）和超人类认知能力（ASI），如果未对齐，可能会危及人类文明。这些风险仍然在辩论中。

AI对齐是AI安全的一个子领域，AI安全研究旨在研究如何构建安全的AI系统。AI安全的其他子领域包括鲁棒性、监控和能力控制。对齐研究面临的挑战包括在AI中灌输复杂的价值观、开发诚实的AI、可扩展的监督、审计和解释AI模型，以及防止突现的AI行为，如寻求权力。对齐研究与可解释性研究、（对抗性）鲁棒性、异常检测、校准的不确定性、形式验证、偏好学习、安全关键工程、博弈论、算法公平性和社会科学等领域有着密切的联系。
\subsection{在人工智能中的目标}
程序员为AI系统（例如AlphaZero）提供一个“目标函数”，[a] 他们希望通过这个函数来封装AI配置的目标。这种系统随后会填充其环境的（可能是隐式的）内部“模型”。这个模型封装了智能体关于世界的所有信念。然后，AI根据计算出的计划来创建并执行行动，目的是最大化[b] 目标函数[c] 的值。例如，当AlphaZero进行象棋训练时，它有一个简单的目标函数：“如果AlphaZero赢了，则值为+1；如果AlphaZero输了，则值为-1”。在游戏过程中，AlphaZero试图执行其判断为最有可能达到最大+1值的棋步序列。[34] 类似地，一个强化学习系统可以有一个“奖励函数”，允许程序员塑造AI期望的行为。[35] 进化算法的行为则由一个“适应度函数”来塑造。[36]