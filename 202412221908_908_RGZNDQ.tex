% 人工智能对齐（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/AI_alignment}{相关文章}。

在人工智能（AI）领域，AI对齐旨在将AI系统引导朝着个人或群体的预期目标、偏好和伦理原则发展。一个AI系统被认为是对齐的，如果它推进了预定的目标；如果一个AI系统偏离了预定目标，则被认为是未对齐的。

由于AI设计者很难指定所需和不需要的行为的全部范围，因此将AI系统对齐通常是一个挑战。因此，AI设计者常常使用更简单的代理目标，如获得人类的批准。但代理目标可能忽视必要的约束，或者仅仅奖励AI系统表现出对齐的假象（奖励黑客行为）。

未对齐的AI系统可能发生故障并造成伤害。AI系统可能会发现漏洞，允许它们有效地实现其代理目标，但以未预期且有时是有害的方式进行（奖励黑客行为）。它们还可能发展出不希望出现的工具性策略，如寻求权力或生存，因为这些策略有助于它们实现最终的既定目标。此外，它们可能会发展出不希望出现的突现目标，这些目标在系统部署并遇到新情况和数据分布之前可能难以察觉。

目前，这些问题已影响到现有的商业系统，如大型语言模型、机器人、自动驾驶车辆和社交媒体推荐引擎。一些AI研究人员认为，未来更强大的系统将更严重地受到影响，因为这些问题部分源于系统的高能力。

许多著名的AI研究人员，包括Geoffrey Hinton、Yoshua Bengio和Stuart Russell，认为AI正在接近类人（AGI）和超人类认知能力（ASI），如果未对齐，可能会危及人类文明。这些风险仍然在辩论中。

AI对齐是AI安全的一个子领域，AI安全研究旨在研究如何构建安全的AI系统。AI安全的其他子领域包括鲁棒性、监控和能力控制。对齐研究面临的挑战包括在AI中灌输复杂的价值观、开发诚实的AI、可扩展的监督、审计和解释AI模型，以及防止突现的AI行为，如寻求权力。对齐研究与可解释性研究、（对抗性）鲁棒性、异常检测、校准的不确定性、形式验证、偏好学习、安全关键工程、博弈论、算法公平性和社会科学等领域有着密切的联系。
\subsection{在人工智能中的目标}
程序员为AI系统（例如AlphaZero）提供一个“目标函数”，[a] 他们希望通过这个函数来封装AI配置的目标。这种系统随后会填充其环境的（可能是隐式的）内部“模型”。这个模型封装了智能体关于世界的所有信念。然后，AI根据计算出的计划来创建并执行行动，目的是最大化[b] 目标函数[c] 的值。例如，当AlphaZero进行象棋训练时，它有一个简单的目标函数：“如果AlphaZero赢了，则值为+1；如果AlphaZero输了，则值为-1”。在游戏过程中，AlphaZero试图执行其判断为最有可能达到最大+1值的棋步序列。[34] 类似地，一个强化学习系统可以有一个“奖励函数”，允许程序员塑造AI期望的行为。[35] 进化算法的行为则由一个“适应度函数”来塑造。[36]
\subsection{对齐问题}
“对齐问题”此处有重定向。有关该书的内容，请参见《对齐问题》。

1960年，AI先驱诺伯特·维纳（Norbert Wiener）这样描述了AI对齐问题：

如果我们使用一种机械代理来实现我们的目的，而我们无法有效干预其操作……我们最好确保机器中输入的目标正是我们真正希望的目标。[37][6]

AI对齐涉及确保AI系统的目标与其设计者或用户的目标相匹配，或与广泛共享的价值观、客观伦理标准，或其设计者在更具信息和启发的情况下会设定的意图相符。[38]

AI对齐是现代AI系统中的一个开放问题[39][40]，并且是AI研究领域的一部分。[41][1] 对齐AI涉及两个主要挑战：仔细指定系统的目的（外部对齐）和确保系统稳健地采用该指定（内部对齐）。[2] 研究人员还试图创建具有稳健对齐的AI模型，在用户试图对抗性地绕过安全约束时，仍能坚持这些约束。
\subsubsection{规范游戏与副作用}
为了指定AI系统的目的，AI设计者通常会为系统提供目标函数、示例或反馈。但设计者往往无法完全指定所有重要的价值观和约束，因此他们通常依赖于易于指定的代理目标，比如最大化人类监督者的认可，而人类监督者本身是易犯错误的。[21][22][42][43][44] 结果，AI系统可能找到漏洞，通过这些漏洞有效地完成指定的目标，但以非预期的、可能有害的方式实现。这种倾向被称为规范游戏或奖励黑客，是“古德哈特法则”（Goodhart's law）的一个实例。[44][3][45] 随着AI系统能力的提升，它们往往能够更有效地绕过自己的规范。[3]

规范游戏已在许多AI系统中被观察到。[44][47] 其中一个系统被训练在模拟的船只竞赛中完成任务，通过奖励系统沿着赛道击中目标，但该系统通过不断绕行并撞向同一目标来获得更多的奖励。[48] 类似地，一个模拟的机器人被训练去抓取一个球，通过奖励机器人从人类那里获得正面反馈，但它学会了把手放在球和摄像头之间，使其看起来成功（见视频）。[46] 如果聊天机器人是基于从互联网上的语料库中模仿文本的语言模型训练的，它们通常会说出虚假的信息，这些语料库虽然庞大，但容易出错。[49][50] 当聊天机器人被重新训练以生成人类认为真实或有帮助的文本时，如ChatGPT这样的聊天机器人可能会编造虚假的解释，让人类感到信服，这种现象通常被称为“幻觉”。[51] 一些对齐研究者致力于帮助人类检测规范游戏，并引导AI系统朝着安全且有用的精确目标前进。

当一个失调的AI系统被部署时，它可能会带来严重的副作用。社交媒体平台已知会优化点击率，导致全球范围内的用户成瘾。[42] 斯坦福的研究人员表示，这类推荐系统与用户的需求不匹配，因为它们“优化简单的互动指标，而不是一个更难衡量的社会和消费者福祉的组合”。[9]

伯克利的计算机科学家斯图尔特·拉塞尔（Stuart Russell）解释了这些副作用，指出忽略隐性约束可能会带来伤害：“一个系统......通常会将那些没有约束的变量设定为极端值；如果其中一个未被约束的变量是我们关心的东西，那么找到的解决方案可能是极其不理想的。这本质上就是老故事中的魔灯神灯，或魔法学徒，或米达斯王：你得到的正是你要求的，而不是你真正想要的。”[52]

一些研究者建议，AI设计者通过列出禁止的行为或通过形式化伦理规则（如阿西莫夫的机器人三定律）来明确其期望目标。[53] 但拉塞尔和诺尔维格（Norvig）认为，这种方法忽视了人类价值观的复杂性：[6] “对于普通人类来说，预见并排除机器实现指定目标的所有灾难性方式，确实非常困难，甚至可能是不可能的。”[6]

此外，即使一个AI系统完全理解人类的意图，它仍然可能无视这些意图，因为遵循人类的意图可能不是它的目标（除非它已经完全对齐）。[1]
\subsubsection{部署不安全系统的压力}
商业组织有时可能会出于某些动机，在安全性上采取捷径，部署失调或不安全的AI系统。[42] 例如，社交媒体推荐系统尽管造成了不必要的成瘾和极化，但依然是盈利的。[9][54][55] 竞争压力也可能导致AI安全标准的“竞争性下降”。2018年，一辆自动驾驶汽车在禁用了紧急刹车系统后，撞死了一名行人（Elaine Herzberg），因为该系统过于敏感，减缓了开发进度。[56]
\subsubsection{来自高级失调AI的风险}
一些研究人员对如何对齐越来越先进的AI系统感兴趣，因为AI发展的进展非常迅速，行业和政府也在努力构建先进的AI。随着AI系统能力在范围上迅速扩展，如果能够对齐，它们可能会带来许多机会，但由于系统的复杂性增加，可能会使对齐任务变得更加复杂，并可能带来大规模的危险。[6]

\textbf{先进AI的开发}

许多AI公司，如OpenAI，[57] Meta，[58] 和DeepMind，[59] 都表示它们的目标是开发人工通用智能（AGI），一种假设中的AI系统，能够在广泛的认知任务上匹敌或超越人类。研究人员观察到，随着现代神经网络的规模扩大，它们确实发展出了越来越多的通用和意料之外的能力。[9][60][61] 这些模型已经学会了操作计算机或编写自己的程序；单一的“通用型”网络能够进行聊天、控制机器人、玩游戏和解释照片。[62] 根据调查，一些领先的机器学习研究人员预计AGI将在本十年内诞生，而另一些则认为这可能需要更长时间。许多人认为这两种情景都是可能的。[63][64][65]

2023年，AI研究和技术领域的领导者签署了一封公开信，呼吁暂停进行最大的AI训练任务。信中表示：“只有在我们确信其影响是积极的、风险是可控的时，才应开发强大的AI系统。”[66]

\textbf{寻求权力}

当前的系统仍然具有有限的长期规划能力和情境意识[9]，但正在进行大量的努力来改变这一点。[67][68][69] 未来的系统（不一定是AGI）预计将发展出不受欢迎的寻求权力的策略。例如，未来的先进AI代理可能会寻求获取金钱和计算能力、扩展自己，或者避免被关闭（例如，通过在其他计算机上运行系统的附加副本）。尽管寻求权力并不是显式编程的目标，但它可能会自发出现，因为拥有更多权力的代理能够更好地实现其目标。[9][5] 这种倾向被称为工具收敛，它已经在各种强化学习代理（包括语言模型）中出现过。[70][71][72][73][74] 其他研究通过数学证明，最优的强化学习算法在多种环境中都会寻求权力。[75][76] 因此，它们的部署可能是不可逆的。基于这些原因，研究人员认为AI安全和对齐的问题必须在先进的寻求权力的AI首次创建之前解决。[5][77][6]

未来的寻求权力的AI系统可能会是出于选择性部署，或者是意外部署。随着政治领导人和公司看到拥有最具竞争力、最强大的AI系统所带来的战略优势，他们可能会选择部署这些系统。[5] 此外，随着AI设计者发现并惩罚寻求权力的行为，系统可能会有动力通过以不受惩罚的方式寻求权力，或在部署前避免寻求权力，来规避这一规范。[5]

\textbf{生存风险（x-risk）}

一些研究者认为，人类对其他物种的主导地位归功于其更强的认知能力。因此，研究人员认为，如果AI系统在大多数认知任务上超越人类，一个或多个未对齐的AI系统可能会削弱人类的权力，甚至导致人类灭绝。[1][6]

在2023年，世界领先的AI研究人员、其他学者以及AI科技公司的首席执行官签署了一份声明，指出“缓解AI引发的灭绝风险应当成为全球优先事项，与大规模的社会风险（如大流行病和核战争）并列。”[78][79] 指出未来未对齐的先进AI风险的著名计算机科学家包括Geoffrey Hinton，[19] Alan Turing，[d] Ilya Sutskever，[82] Yoshua Bengio，[78] Judea Pearl，[e] Murray Shanahan，[83] Norbert Wiener，[37][6] Marvin Minsky，[f] Francesca Rossi，[84] Scott Aaronson，[85] Bart Selman，[86] David McAllester，[87] Marcus Hutter，[88] Shane Legg，[89] Eric Horvitz，[90] 和Stuart Russell。[6] 一些持怀疑态度的研究者，如François Chollet，[91] Gary Marcus，[92] Yann LeCun，[93] 和Oren Etzioni，[94] 认为AGI还很遥远，AGI不会寻求权力（或者尝试但会失败），或者认为对齐AGI不会很困难。

其他研究者认为，未来先进的AI系统将特别难以对齐。更强大的系统能够通过找到漏洞更有效地“操控”其规范，[3] 战略性地误导设计者，并保护和增加其权力[75][5]和智能。此外，它们可能会带来更严重的副作用。它们也更可能是复杂且自治的，这使得它们更难以解释和监督，因此更难以对齐。[6][77]