% 自然语言处理（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Natural_language_processing}{相关文章}。

自然语言处理（NLP）是计算机科学的一个子领域，特别是人工智能领域。它主要关注赋予计算机处理自然语言编码的数据的能力，因此与信息检索、知识表示和计算语言学（语言学的一个子领域）密切相关。通常，数据通过文本语料库收集，并使用基于规则、统计方法或基于神经网络的机器学习和深度学习方法进行处理。

自然语言处理的主要任务包括语音识别、文本分类、自然语言理解和自然语言生成。
\subsection{历史} 
更多信息：自然语言处理的历史  
自然语言处理的根源可以追溯到20世纪50年代。[1] 早在1950年，阿兰·图灵就发表了一篇名为《计算机器与智能》的文章，提出了现在被称为图灵测试的智能标准，尽管当时这并没有被表述为一个与人工智能分开的问题。该测试提议包括一个任务，涉及自动化地解释和生成自然语言。
\subsubsection{符号化自然语言处理（1950年代 – 1990年代初）}  
符号化自然语言处理的前提可以通过约翰·塞尔的“中文房间”实验来简明总结：给定一组规则（例如，一本中文短语手册，包含问题及其对应的答案），计算机通过应用这些规则来模拟自然语言理解（或其他自然语言处理任务），从而应对其所遇到的数据。
\begin{itemize}
\item 1950年代：1954年的乔治城实验涉及将超过60个俄语句子完全自动翻译成英语。研究者声称，机器翻译将在三到五年内解决。[2] 然而，真正的进展要慢得多，在1966年发布的ALPAC报告之后，报告指出十年的研究未能实现预期目标，机器翻译的资金大幅减少。美国几乎没有再进行机器翻译的进一步研究（尽管其他地方如日本和欧洲仍有一些研究[3]），直到1980年代末，首个统计机器翻译系统的出现。
\item 1960年代：1960年代出现了一些成功的自然语言处理系统，其中包括SHRDLU，这是一种在受限“积木世界”中工作的自然语言系统，具有受限的词汇表；以及ELIZA，这是由约瑟夫·魏岑鲍姆于1964到1966年间编写的罗杰式心理治疗师模拟系统。ELIZA几乎不涉及人类思维或情感的信息，但有时能提供惊人的人类互动。当“病人”的问题超出了非常小的知识库时，ELIZA可能会提供一个通用的回答，例如，当被问到“我的头很痛”时，它可能会回答“你为什么说你的头痛？”罗斯·奎利安（Ross Quillian）在自然语言方面的成功工作证明了仅用20个词汇就能开发出有效的系统，因为那时计算机的内存只能容纳这么多。[4]
\item 1970年代：1970年代，许多程序员开始编写“概念本体”，将现实世界的信息结构化为计算机可理解的数据。例子包括MARGIE（Schank, 1975）、SAM（Cullingford, 1978）、PAM（Wilensky, 1978）、TaleSpin（Meehan, 1976）、QUALM（Lehnert, 1977）、Politics（Carbonell, 1979）和Plot Units（Lehnert, 1981）。在这一时期，第一批聊天机器人被编写出来（例如，PARRY）。
\item 1980年代：1980年代和1990年代初是符号方法在自然语言处理中的黄金时代。当时的研究重点包括基于规则的解析（例如，HPSG作为生成语法的计算性实现）、形态学（例如，两级形态学[5]）、语义学（例如，Lesk算法）、指代（例如，在中心理论[6]中）以及自然语言理解的其他领域（例如，在修辞结构理论中）。其他研究方向也得到了延续，例如使用Racter和Jabberwacky开发聊天机器人。一个重要的进展（最终导致了1990年代统计方法的转向）是这一时期量化评估的重要性逐渐上升。[7]
\end{itemize}
\subsubsection{统计自然语言处理（1990年代–2010年代）}  
直到1980年代，大多数自然语言处理系统都基于复杂的手写规则集。然而，自1980年代末开始，随着引入机器学习算法进行语言处理，自然语言处理领域发生了一场革命。这一变化既得益于计算能力的稳步提升（参见摩尔定律），也得益于乔姆斯基语言学理论（例如转化语法）的主导地位逐渐减弱，这些理论的理论基础阻碍了以语料库语言学为基础的机器学习方法在语言处理中的应用。[8]
\begin{itemize}
\item 1990年代：统计方法在自然语言处理中的一些显著早期成功，尤其是在机器翻译领域，得益于IBM研究的工作，如IBM对齐模型。这些系统能够利用现有的多语言文本语料库，这些语料库是由加拿大议会和欧盟根据相关法律产生的，这些法律要求将所有政府事务翻译成相应政府系统的所有官方语言。然而，大多数其他系统依赖于专门为这些系统实现的任务而开发的语料库，而这通常是这些系统成功的主要限制。因此，很多研究都致力于如何更有效地从有限的数据中学习。
\item 2000年代：随着互联网的增长，自1990年代中期以来，越来越多的原始（未经标注的）语言数据变得可用。因此，研究逐渐集中在无监督和半监督学习算法上。这些算法可以从没有手动标注的、没有预定答案的数据中学习，或者从标注数据与未标注数据的组合中学习。一般来说，这一任务比有监督学习要困难得多，通常在给定输入数据的情况下会产生较不准确的结果。然而，存在大量的未标注数据（包括，除了其他内容外，整个万维网的内容），如果所用算法的时间复杂度足够低，这通常可以弥补较差的结果，从而使得其在实际应用中仍具有可行性。
\end{itemize}
\subsubsection{神经网络自然语言处理（现状）}  
2003年，Yoshua Bengio及其合著者提出的多层感知器（具有单一隐层和上下文长度为数个词的语言模型，并在14百万词语的训练数据上使用CPU集群）超越了当时最好的统计算法——词n-gram模型。[9]

2010年，Tomáš Mikolov（当时是布尔诺理工大学的博士生）及其合著者将一个简单的单隐层递归神经网络应用于语言建模，[10] 之后几年他又发展出了Word2vec。进入2010年代，表示学习和深度神经网络风格（具有多个隐层）的机器学习方法在自然语言处理领域变得广泛应用。其流行部分是因为一系列结果表明，这些技术[11][12]能够在许多自然语言任务中取得最先进的成绩，例如语言建模[13]和句法分析[14][15]。这一技术的应用在医学和医疗领域尤为重要，尤其是在帮助分析电子健康记录中的笔记和文本方面，这些内容本来难以进行研究，但通过NLP技术可以提供支持，从而改善医疗护理[16]或保护患者隐私[17]。
\subsection{方法：符号方法、统计方法、神经网络方法}  
符号方法，即手工编写一组符号操作规则，并结合字典查找，历史上是人工智能（AI）和自然语言处理（NLP）最早使用的方法之一：[18][19] 例如，通过编写语法或设计启发式规则来进行词干提取。

另一方面，机器学习方法，包括统计方法和神经网络方法，相较于符号方法有许多优势：
\begin{itemize}
\item 统计和神经网络方法能够更专注于从文本语料库中提取的最常见情况，而基于规则的方法需要为稀有情况和常见情况提供相同数量的规则。
\item 由统计或神经网络方法生成的语言模型在面对陌生（例如包含未见过的单词或结构）和错误输入（例如拼写错误或意外遗漏的单词）时，表现得更为健壮，相比之下，基于规则的系统更容易受到这些问题的影响，且生产成本较高。
\item 这种（概率）语言模型的规模越大，其准确性越高，而基于规则的系统只能通过增加规则的数量和复杂性来提高准确性，这也会导致不可处理的问题。
\end{itemize}
尽管符号方法仍在2020年使用，但随着大型语言模型（LLMs）在2023年的发展，符号方法已变得基本过时。

在此之前，它们常常用于：
\begin{itemize}
\item 当训练数据不足以成功应用机器学习方法时，例如用于低资源语言的机器翻译（如Apertium系统提供的翻译），
\item 用于NLP管道中的预处理，例如分词，或者
\item 用于NLP管道输出的后处理和转换，例如从句法分析中提取知识。
\end{itemize}
\subsubsection{统计方法}  
在1980年代末到1990年代中期，统计方法结束了由基于规则方法低效性引发的AI冬天。[20][21]

最早的决策树，生成硬性的“如果-那么”规则系统，仍然与旧的基于规则的方法非常相似。只有隐马尔可夫模型的引入，应用于词性标注，才宣告了旧的基于规则方法的结束。
\subsubsection{神经网络}  
更多信息：人工神经网络  
统计方法的一个主要缺点是它们需要复杂的特征工程。自2015年起，[22] 统计方法已被神经网络方法取代，神经网络方法使用语义网络[23] 和词嵌入来捕捉单词的语义属性。

中间任务（例如词性标注和依存解析）不再是必需的。

基于新发明的序列到序列转换的神经机器翻译，淘汰了之前在统计机器翻译中必需的中间步骤，例如词对齐。
\subsection{常见的自然语言处理任务}  
以下是一些最常研究的自然语言处理任务的列表。部分任务具有直接的现实应用，而其他任务则通常作为子任务，帮助解决更大规模的任务。

尽管自然语言处理任务密切相关，但为了方便起见，可以将其细分为几个类别。粗略的分类如下。
\subsubsection{文本与语音处理} 
\textbf{光学字符识别（OCR）}  

给定一张表示印刷文本的图像，确定其对应的文本内容。

\textbf{语音识别}  

给定一段人物或多人讲话的音频剪辑，确定其文本表示。这是与文本转语音相反的过程，是一种被俗称为“AI完备”的极其困难的问题（见上文）。在自然语言中，单词之间几乎没有停顿，因此语音分割是语音识别的必要子任务（见下文）。在大多数口语语言中，表示连续字母的语音会相互融合，这一过程称为“共振”，因此将模拟信号转化为离散字符可能是一个非常困难的过程。此外，由于同一语言中的单词由不同口音的人发音，语音识别软件必须能够识别这些不同的输入，并将其与文本等价物视为相同。

\textbf{语音分割}  

给定一段人物或多人讲话的音频剪辑，将其分割成单个单词。这是语音识别的一个子任务，通常与语音识别一起处理。

\textbf{文本转语音}  
给定一段文本，将其转换成语音表示。文本转语音可以用于帮助视力障碍人士。[24]

\textbf{词语分割（标记化）}  

标记化是文本分析中用来将文本划分为单个单词或单词片段的过程。该技术产生了两个关键组件：单词索引和标记化文本。单词索引是一个将唯一单词映射到特定数字标识符的列表，标记化文本则用相应的数字标记替代每个单词。然后，这些数字标记被用在各种深度学习方法中。[25]

对于像英语这样的语言，这个过程相对简单，因为单词通常由空格分隔。然而，像中文、日语和泰语等一些书面语言并没有以这种方式标记单词边界，因此在这些语言中，文本分割是一个重要的任务，需要了解语言的词汇和形态学。有时，这一过程也用于像数据挖掘中的“词袋模型”（BOW）创建等情况。
\subsubsection{形态学分析}  
\textbf{词形还原}  

去除词尾变化，只返回词语的基础字典形式，也称为词根（lemma）。词形还原是另一种将词语还原为规范形式的技术，但与之不同的是，它实际上使用字典将单词映射到其实际形式。[26]

\textbf{形态学分割}  

将单词拆分为独立的语素，并识别语素的类别。此任务的难度很大程度上取决于所考虑语言的形态学复杂性（即，单词的结构）。英语的形态学相对简单，尤其是屈折形态，因此通常可以完全忽略这项任务，并简单地将一个单词的所有可能形式（例如，“open, opens, opened, opening”）视为独立的单词。然而，在像土耳其语或梅提语这样的语言中，这种方法是不可行的，因为每个字典条目都有成千上万种可能的单词形式。[27]

\textbf{词性标注}  

给定一个句子，为每个单词确定其词性（POS）。许多单词，尤其是常见的单词，可以充当多种词性。例如，“book”可以是名词（“the book on the table”）或动词（“to book a flight”）；“set”可以是名词、动词或形容词；而“out”至少可以是五种不同的词性之一。

\textbf{词干提取}  

将屈折（或有时派生）词还原为基本形式的过程（例如，“close”是“closed”、“closing”、“close”、“closer”等词的词干）。词干提取与词形还原产生类似的结果，但它是基于规则进行的，而不是基于字典。

\subsubsection{句法分析}
\textbf{语法归纳[28]}  

生成描述语言语法的形式化语法。

\textbf{句子分割（也称为“句子边界歧义消解”）} 

给定一段文本，找出句子边界。句子边界通常由句点或其他标点符号标示，但这些符号也可以用于其他目的（例如，表示缩写）。

\textbf{句法分析}  

确定给定句子的句法树（语法分析）。自然语言的语法是模糊的，典型的句子有多种可能的分析方式：令人惊讶的是，对于一个典型的句子，可能有成千上万种潜在的分析（其中大多数对人类来说完全没有意义）。句法分析主要有两种类型：依存句法分析和成分句法分析。依存句法分析侧重于句子中单词之间的关系（标记主语、谓语等），而成分句法分析则侧重于使用概率上下文无关语法（PCFG）构建句法树（参见随机语法）。
\subsubsection{词汇语义学（上下文中的单个词的语义）}
\textbf{词汇语义学}  

计算机如何理解上下文中单个词的意义？

\textbf{分布式语义学}  

我们如何从数据中学习语义表示？

\textbf{命名实体识别（NER）} 

给定一段文本，确定文本中哪些项映射到专有名词，如人物或地点，并确定每个专有名词的类型（如人物、地点、组织）。虽然在英语等语言中，大写字母有助于识别命名实体，但它不能帮助确定命名实体的类型，而且在很多情况下，大写字母信息不准确或不足。例如，句子的第一个字母也会大写，命名实体通常跨越多个词，其中只有部分单词大写。此外，许多使用非西方字母的语言（如中文或阿拉伯语）根本没有大写字母，即使是有大写字母的语言，也不一定一致地使用它来区分名称。例如，德语中所有名词都大写，而无论它们是否为专有名词，法语和西班牙语中用作形容词的专有名词通常不大写。此任务的另一个名称是“标记分类”（Token Classification）。

\textbf{情感分析（见“多模态情感分析”）}  

情感分析是一种计算方法，用于识别和分类文本中的情感意图。该技术通过分析文本，判断表达的情感是积极的、消极的还是中立的。情感分类模型通常利用词汇n-gram、词频-逆文档频率（TF-IDF）特征、手工生成的特征，或者使用深度学习模型来识别文本序列中的长期和短期依赖关系。情感分析的应用非常广泛，涉及诸如对各种在线平台上的客户评论进行分类等任务。

\textbf{术语提取}  

术语提取的目标是从给定的语料库中自动提取相关的术语。

\textbf{词义消歧（WSD）}  

许多词有多个意思；我们需要选择在特定上下文中最符合语义的意义。对于这个问题，通常会给定一个词列表及其关联的词义，例如来自词典或在线资源（如WordNet）。

\textbf{实体链接}  

许多词——通常是专有名词——指代命名实体；在这种情况下，我们需要选择上下文中指代的实体（例如知名人物、地点、公司等）。
\subsubsection{关系语义学（单个句子的语义）}
\textbf{关系提取}  

给定一段文本，识别命名实体之间的关系（例如，谁和谁结婚了）。

\textbf{语义解析}  

给定一段文本（通常是一个句子），生成其语义的正式表示，可以是图形（例如，AMR解析）或按照逻辑形式化的方式（例如，DRT解析）。这个任务通常涉及语义学中的几个基础NLP任务（例如，语义角色标注、词义消歧），并且可以扩展到包括完整的语篇分析（例如，语篇分析、指代消解；见“自然语言理解”部分）。

\textbf{语义角色标注（见“隐式语义角色标注”）}  

给定一个句子，识别并消歧义语义谓词（例如，动词框架），然后识别并分类框架元素（语义角色）。
\subsubsection{语篇（超越单个句子的语义）}
\textbf{指代消解}  

给定一个句子或更大段的文本，确定哪些词（“提及”）指代相同的对象（“实体”）。代词消解是这个任务的一个特定例子，特别关注将代词与其指代的名词或名称进行匹配。更一般的指代消解任务还包括识别所谓的“桥接关系”，这些关系涉及指代表达。例如，在句子“他通过前门进入了约翰的房子”中，“前门”是一个指代表达，所要识别的桥接关系是所指的门是约翰房子的前门（而不是某个可能被提及的其他结构的门）。

\textbf{语篇分析}  

这一范畴包含几个相关任务。一个任务是语篇解析，即识别一段连贯文本的语篇结构，即句子之间的语篇关系（例如，扩展、解释、对比）。另一个可能的任务是识别和分类文本段落中的言语行为（例如，是非问题、内容问题、陈述、断言等）。
\subsubsection{隐式语义角色标注}  
给定一个句子，识别并消歧义语义谓词（例如，动词框架）及其在当前句子中的显式语义角色（参见上文的语义角色标注）。然后，识别当前句子中未显式实现的语义角色，将其分类为在文本其他地方显式实现的论元和未指定的论元，并将前者与本地文本中的内容进行关联。一个紧密相关的任务是零代词消解，即将指代消解扩展到省略主语的语言（pro-drop languages）。

\textbf{文本蕴含识别}  

给定两个文本片段，判断一个是否为真能推导出另一个为真，是否推导出另一个的否定，或是否允许另一个为真或为假。[30]

\textbf{主题分割与识别}  

给定一段文本，将其分割为若干主题段落，并识别每个段落的主题。

\textbf{论证挖掘}

论证挖掘的目标是通过计算机程序自动从自然语言文本中提取和识别论证结构。[31] 此类论证结构包括前提、结论、论证方案以及主论点与附属论点、或主论点与反论点之间的关系。[32][33]
\subsubsection{更高层次的 NLP 应用}
\textbf{自动摘要（文本摘要）} 

生成一段文本的可读摘要。通常用于提供已知类型文本的摘要，例如研究论文、报纸财经版的文章。

\textbf{语法错误修正}  

语法错误检测与修正涉及语言分析的各个层面（语音/正字法、形态学、句法、语义、语用）。语法错误修正具有重要影响，因为它涉及到数亿使用或学习英语作为第二语言的人。因此，自2011年以来，它一直是多个共享任务的研究对象。[34][35][36] 就正字法、形态学、句法及语义的某些方面而言，随着强大神经语言模型（如 GPT-2）的发展，至2019年，这一问题已经可以视为大部分解决，并且在各种商业应用中得到了推广。

\textbf{逻辑翻译}  

将文本从自然语言翻译为形式逻辑。

\textbf{机器翻译（MT）}  

自动将文本从一种人类语言翻译为另一种语言。这是最困难的问题之一，属于被称为“AI 完备”的问题类，即解决这一问题需要人类所具备的所有不同类型的知识（语法、语义、现实世界的事实等）。

\textbf{自然语言理解（NLU）}  

将文本块转换为更为正式的表示形式，如一阶逻辑结构，这些结构更容易被计算机程序处理。自然语言理解涉及从多种可能的语义中识别出预期的语义，这些语义通常以组织良好的自然语言概念符号的形式表达。语言元模型和本体的引入与创建是有效的经验性解决方案。为了构建语义形式化的基础，预期对自然语言语义进行明确的形式化，不应与隐性假设（如封闭世界假设（CWA）与开放世界假设、或主观的“是/否”与客观的“真/假”）混淆。[37]

\textbf{自然语言生成（NLG）}  

将计算机数据库或语义意图转换为可读的人类语言。

\textbf{书籍生成} 

这不是传统的 NLP 任务，但自然语言生成及其他 NLP 任务的延伸是生成完整的书籍。第一本机器生成的书籍是由基于规则的系统于1984年创建的（《警察的胡子是半完成的》，Racter）。[38] 第一部由神经网络发布的作品《1 the Road》于2018年发布，作为小说市场销售，包含六千万字。两者基本上都是精心设计但没有语义内容（无语义的）语言模型。第一本机器生成的科学书籍于2019年发布（《Beta Writer，锂离子电池》，Springer，Cham）。[39] 与《Racter》和《1 the Road》不同，这本书基于事实知识并以文本摘要为基础。

\textbf{文档 AI}  

文档 AI 平台依托于 NLP 技术，使没有人工智能、机器学习或 NLP 经验的用户也能快速训练计算机，从不同类型的文档中提取他们所需的特定数据。基于 NLP 的文档 AI 使非技术团队能够快速访问隐藏在文档中的信息，例如律师、商业分析师和会计师。[40]

\textbf{对话管理}  

旨在与人类对话的计算机系统。

\textbf{问答系统}  

给定一个人类语言的问题，确定其答案。典型问题有一个特定的正确答案（如“加拿大的首都是哪里？”），但有时也考虑开放性问题（如“生命的意义是什么？”）。

\textbf{文本到图像生成} 

给定一张图像的描述，生成一张符合描述的图像。[41]

\textbf{文本到场景生成}  

给定一段场景描述，生成该场景的 3D 模型。[42][43]

\textbf{文本到视频}  

给定一个视频的描述，生成符合该描述的视频。[44][45]
\subsection{一般趋势和（可能的）未来方向}
根据该领域长期存在的趋势，可以推测出 NLP 未来的发展方向。截至 2020年，在 CoNLL共享任务长期系列的主题中，可以观察到以下三种趋势：[46]
\begin{itemize}
\item 对自然语言中日益抽象的“认知”方面的兴趣 1999–2001年：浅层解析 2002–03年：命名实体识别 2006–09年/2017–18年：依赖句法 2004–05年/2008–09年：语义角色标注  2011–12年：共指消解 015–16年：话语解析 2019年：语义解析
\item 对多语言性以及潜在的多模态性（多种感知形式的结合）日益关注 英语自1999年起； 西班牙语、荷兰语自2002年起；德语自2003年起；保加利亚语、丹麦语、日语、葡萄牙语、斯洛文尼亚语、瑞典语、土耳其语自2006年起；巴斯克语、加泰罗尼亚语、中文、希腊语、匈牙利语、意大利语、土耳其语自2007年起；捷克语自2009年起；阿拉伯语自2012年起；2017年：40多种语言；2018年：60多种/100多种语言
\item 去除符号表示法 从基于规则的监督学习方法转向弱监督方法、表示学习和端到端系统
\end{itemize}