% 自然语言处理（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Natural_language_processing}{相关文章}。

自然语言处理（NLP）是计算机科学的一个子领域，特别是人工智能领域。它主要关注赋予计算机处理自然语言编码的数据的能力，因此与信息检索、知识表示和计算语言学（语言学的一个子领域）密切相关。通常，数据通过文本语料库收集，并使用基于规则、统计方法或基于神经网络的机器学习和深度学习方法进行处理。

自然语言处理的主要任务包括语音识别、文本分类、自然语言理解和自然语言生成。
\subsection{历史} 
更多信息：自然语言处理的历史  
自然语言处理的根源可以追溯到20世纪50年代。[1] 早在1950年，阿兰·图灵就发表了一篇名为《计算机器与智能》的文章，提出了现在被称为图灵测试的智能标准，尽管当时这并没有被表述为一个与人工智能分开的问题。该测试提议包括一个任务，涉及自动化地解释和生成自然语言。
\subsubsection{符号化自然语言处理（1950年代 – 1990年代初）}  
符号化自然语言处理的前提可以通过约翰·塞尔的“中文房间”实验来简明总结：给定一组规则（例如，一本中文短语手册，包含问题及其对应的答案），计算机通过应用这些规则来模拟自然语言理解（或其他自然语言处理任务），从而应对其所遇到的数据。
\begin{itemize}
\item 1950年代：1954年的乔治城实验涉及将超过60个俄语句子完全自动翻译成英语。研究者声称，机器翻译将在三到五年内解决。[2] 然而，真正的进展要慢得多，在1966年发布的ALPAC报告之后，报告指出十年的研究未能实现预期目标，机器翻译的资金大幅减少。美国几乎没有再进行机器翻译的进一步研究（尽管其他地方如日本和欧洲仍有一些研究[3]），直到1980年代末，首个统计机器翻译系统的出现。
\item 1960年代：1960年代出现了一些成功的自然语言处理系统，其中包括SHRDLU，这是一种在受限“积木世界”中工作的自然语言系统，具有受限的词汇表；以及ELIZA，这是由约瑟夫·魏岑鲍姆于1964到1966年间编写的罗杰式心理治疗师模拟系统。ELIZA几乎不涉及人类思维或情感的信息，但有时能提供惊人的人类互动。当“病人”的问题超出了非常小的知识库时，ELIZA可能会提供一个通用的回答，例如，当被问到“我的头很痛”时，它可能会回答“你为什么说你的头痛？”罗斯·奎利安（Ross Quillian）在自然语言方面的成功工作证明了仅用20个词汇就能开发出有效的系统，因为那时计算机的内存只能容纳这么多。[4]
\item 1970年代：1970年代，许多程序员开始编写“概念本体”，将现实世界的信息结构化为计算机可理解的数据。例子包括MARGIE（Schank, 1975）、SAM（Cullingford, 1978）、PAM（Wilensky, 1978）、TaleSpin（Meehan, 1976）、QUALM（Lehnert, 1977）、Politics（Carbonell, 1979）和Plot Units（Lehnert, 1981）。在这一时期，第一批聊天机器人被编写出来（例如，PARRY）。
\item 1980年代：1980年代和1990年代初是符号方法在自然语言处理中的黄金时代。当时的研究重点包括基于规则的解析（例如，HPSG作为生成语法的计算性实现）、形态学（例如，两级形态学[5]）、语义学（例如，Lesk算法）、指代（例如，在中心理论[6]中）以及自然语言理解的其他领域（例如，在修辞结构理论中）。其他研究方向也得到了延续，例如使用Racter和Jabberwacky开发聊天机器人。一个重要的进展（最终导致了1990年代统计方法的转向）是这一时期量化评估的重要性逐渐上升。[7]
\end{itemize}
\subsubsection{统计自然语言处理（1990年代–2010年代）}  
直到1980年代，大多数自然语言处理系统都基于复杂的手写规则集。然而，自1980年代末开始，随着引入机器学习算法进行语言处理，自然语言处理领域发生了一场革命。这一变化既得益于计算能力的稳步提升（参见摩尔定律），也得益于乔姆斯基语言学理论（例如转化语法）的主导地位逐渐减弱，这些理论的理论基础阻碍了以语料库语言学为基础的机器学习方法在语言处理中的应用。[8]
\begin{itemize}
\item 1990年代：统计方法在自然语言处理中的一些显著早期成功，尤其是在机器翻译领域，得益于IBM研究的工作，如IBM对齐模型。这些系统能够利用现有的多语言文本语料库，这些语料库是由加拿大议会和欧盟根据相关法律产生的，这些法律要求将所有政府事务翻译成相应政府系统的所有官方语言。然而，大多数其他系统依赖于专门为这些系统实现的任务而开发的语料库，而这通常是这些系统成功的主要限制。因此，很多研究都致力于如何更有效地从有限的数据中学习。
\item 2000年代：随着互联网的增长，自1990年代中期以来，越来越多的原始（未经标注的）语言数据变得可用。因此，研究逐渐集中在无监督和半监督学习算法上。这些算法可以从没有手动标注的、没有预定答案的数据中学习，或者从标注数据与未标注数据的组合中学习。一般来说，这一任务比有监督学习要困难得多，通常在给定输入数据的情况下会产生较不准确的结果。然而，存在大量的未标注数据（包括，除了其他内容外，整个万维网的内容），如果所用算法的时间复杂度足够低，这通常可以弥补较差的结果，从而使得其在实际应用中仍具有可行性。
\end{itemize}