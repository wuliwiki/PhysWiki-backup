% Softmax 函数（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Softmax_function}{相关文章}。

Softmax 函数，也称为 softargmax1: 184  或归一化指数函数2: 198 ，能够将一个长度为 K 的实数向量转换为 K 个可能结果的概率分布。它是逻辑函数在多维空间的推广形式，常用于多项式逻辑回归中。Softmax 函数通常作为神经网络中最后一层的激活函数，用于将网络输出归一化为对各个预测类别的概率分布。
\subsection{定义}
Softmax 函数以一个长度为 $K$ 的实数向量 $\mathbf{z}$ 作为输入，并将其归一化为一个概率分布：该分布由 $K$ 个概率值组成，每个概率值与输入中对应元素的指数成正比。也就是说，在应用 Softmax 之前，向量中的某些分量可能为负，或大于 1，且它们的和不一定为 1；但在应用 Softmax 之后，每个分量都将位于区间 $(0, 1)$ 之内，并且所有分量之和为 1，因此可以将它们解释为概率。此外，输入值越大的分量，对应的概率也越大。

标准（单位）Softmax 函数$\sigma: \mathbb{R}^K \to (0,1)^K$,其中 $K > 1$，它接收一个向量$\mathbf{z} = (z_1, \dotsc, z_K) \in \mathbb{R}^K$,并计算输出向量$\sigma(\mathbf{z}) \in (0,1)^K$的每个分量，定义为：
$$
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}.~
$$
换句话说，Softmax 对输入向量 $\mathbf{z}$ 中的每个元素 $z_i$ 应用标准指数函数（即 $e^{z_i}$），然后将所有指数值归一化——即每个指数值除以所有指数值的总和。这个归一化操作保证了输出向量 $\sigma(\mathbf{z})$ 所有分量的和为 1，从而可以被解释为概率分布。

“Softmax”一词来源于指数函数对输入向量中最大值的放大作用。例如，对向量 $(1, 2, 8)$ 进行标准 Softmax 运算，其结果大约为$(0.001, 0.002, 0.997)$,也就是说，几乎所有的权重都被分配给了最大值 8 所在的位置。

一般情况下，Softmax 函数中不一定非要使用自然底数 $e$，可以使用任意大于 0 的底数 $b$。和之前一样：如果 $b > 1$，那么输入向量中较大的分量会对应较大的输出概率；并且当 $b$ 增大时，所得的概率分布将更加集中在最大值所在的位置；相反，如果 $0 < b < 1$，那么输入中较小的分量反而会对应较大的输出概率；随着 $b$ 的减小，概率分布将更多地集中在最小值所在的位置。我们可以写成如下形式：$b = e^{\beta}$，或$b = e^{-\beta}$，其中 $\beta$ 为实数。这将导致 Softmax 函数有如下表达式：
$$
\sigma(\mathbf{z})_i = \frac{e^{\beta z_i}}{\sum_{j=1}^{K} e^{\beta z_j}} \quad \text{或} \quad \sigma(\mathbf{z})_i = \frac{e^{-\beta z_i}}{\sum_{j=1}^{K} e^{-\beta z_j}}, \quad \text{其中 } i = 1, \dotsc, K~
$$
其中与 $\beta$ 的倒数成正比的值，有时被称为温度：$\beta = 1/kT$这里的 $k$ 通常取 1 或玻尔兹曼常数，$T$ 是“温度”。较高的温度（较小的 $\beta$）会使输出分布更均匀（即熵更高，更“随机”）；较低的温度（较大的 $\beta$）则会使分布更尖锐，即一个值占主导地位。

在某些领域中，底数 $b$ 是固定的，对应于某种固定的尺度；而在另一些领域中，会改变参数 $\beta$ 或 $T$ 来调整分布的形状。
\subsection{解释}
\subsubsection{平滑的 arg max}
Softmax 函数是 arg max 函数（即返回向量中最大元素索引的函数）的一个平滑近似。尽管如此，“softmax”这个名称可能具有误导性：它并不是最大值函数的平滑近似，而只是arg max的平滑版本。“softmax”一词有时也被用来指代与之密切相关的 LogSumExp 函数，而后者确实是最大值函数的平滑近似。因此，为了更准确地表达其本质，一些人更倾向于使用“softargmax”这一术语，尽管在机器学习领域，“softmax”已经是习惯用法。为避免混淆，本节中使用“softargmax”这一更清晰的表述。

与其将 arg max 看作一个输出为类别索引（如 $1, 2, \dots, n$）的函数，我们可以将其视为一个输出为独热编码的函数（假设最大值是唯一的）：
$$
\operatorname{arg,max}(z_1, \dots, z_n) = (y_1, \dots, y_n) = (0, \dots,0,1,0,\dots, 0)~
$$
当且仅当索引 $i$ 是向量$(z_1, \dots, z_n)$的最大值所在位置时，输出坐标 $y_i = 1$，也就是说，$z_i$是该向量的**唯一最大值**。例如，在这种编码下：$\operatorname{arg\,max}(1, 5, 10) = (0, 0, 1)$因为第三个元素是最大值。

这一表示可以推广到存在多个最大值（即多个 $z_i$ 相等且为最大值）的情况。此时，可以将值 1 平均分配给所有最大值所在的位置；形式上，对应位置取值为 $1/k$，其中 $k$ 是最大值的个数。例如：$\operatorname{arg\,max}(1, 5, 5) = (0, 1/2, 1/2)$,因为第二和第三个元素都是最大值。如果所有元素都相等，例如：$\operatorname{arg\,max}(z, \dots, z) = \left( 1/n, \dots, 1/n \right)$表示每个位置都等可能是最大值。具有多个最大值的点 $\mathbf{z}$ 被称为奇异点（singular points或 singularities），它们构成所谓的奇异集——这些是 arg max 函数不连续的点（存在跳跃不连续）；而只有一个最大值的点则称为非奇异点或常规点（non-singular或 regular points）。

根据引言中的最后一个表达式，softargmax 是 arg max函数的一个平滑近似：当$\beta \to \infty$时，softargmax 逐点收敛于 arg max。也就是说，对于任意固定的输入向量 $\mathbf{z}$，当 $\beta \to \infty$ 时，有：$\sigma_{\beta}(\mathbf{z}) \to \operatorname{arg\,max}(\mathbf{z})$。然而，softargmax 不以一致方式收敛于 arg max。这意味着不同的输入点收敛速度不同，甚至可能非常缓慢。实际上，softargmax 是连续的，而 arg max 在 奇异集（即两个或多个坐标相等的位置）上是不连续的。由于连续函数的一致极限也是连续函数，而 arg max 不连续，因此 softargmax 不可能以一致方式收敛于它。其不一致收敛的原因在于：当输入中两个坐标接近相等（其中一个略大于另一个）时，arg max 的输出会发生剧烈跳跃（从一个位置变到另一个）。例如：
$\sigma_\beta(1, 1.0001) \to (0, 1)$，$\sigma_\beta(1, 0.9999) \to (1, 0)$，而对于完全相等的输入 $(1, 1)$，无论 $\beta$ 为多少，都有：$\sigma_\beta(1, 1) = (1/2, 1/2)$。这说明：越接近奇异点 $(x, x)$，收敛速度越慢。尽管如此，在非奇异点集（即最大值唯一）上，softargmax 会在紧集上收敛。这是一种更强的逐点收敛形式，适用于没有不连续跳变的区域。

相反地，当$\beta \to -\infty$时，softargmax 会以相同的方式收敛到 arg min，这时的奇异集是具有两个或多个最小值的点。在热带分析的语言中，softmax 被视为对 arg max 和 arg min 的一种变形或“量化”。具体而言，这种变形是将 max-plus 半环（对应 arg max）或 min-plus 半环（对应 arg min）替换为 对数半环。通过取极限来恢复 arg max 或 arg min 的过程，被称为“热带化”或“去量化”。

同样地，对于任意固定的 $\beta$，如果某个输入 $z_i$ 相对于温度$T = 1/\beta$来说远大于其他输入分量，那么 Softmax 的输出就会近似于 arg max。例如，当温度为 1 时，输入间的差值为 10 被认为是“很大”的：
$$
\sigma(0, 10) := \sigma_1(0, 10) = \left( 1/(1 + e^{10}), e^{10}/(1 + e^{10})\right) \approx (0.00005, 0.99995)~
$$
在这种情况下，输出几乎完全集中在最大值对应的位置上。

但如果输入差值相对于温度来说较小，则输出就不会接近 arg max。例如，当温度为 100 时，差值 10 相对较小：
$$
\sigma_{1/100}(0, 10) = \left( 1/(1 + e^{1/10}), e^{1/10}/(1 + e^{1/10})\right) \approx (0.475, 0.525)~
$$
此时两个分量的概率接近平均，远不如高温差下那么“明确”。随着$\beta \to \infty$，意味着温度$T = 1/\beta \to 0$，此时即便是很小的输入差异，相对于趋近于零的温度也变得“巨大”，这为 Softmax 在极限情形下趋于 arg max 提供了另一种解释。
\subsubsection{统计力学}
在统计力学中，softargmax 函数被称为玻尔兹曼分布，也称为吉布斯分布：\(^\text{[5]: 7}\)索引集合 $\{1, \dots, k\}$ 表示系统的微观状态；输入值 $z_i$ 表示对应状态的能量；分母称为配分函数，通常记为 $Z$；因子 $\beta$ 被称为冷度，也称为热力学贝塔或逆温度。
\subsection{应用}
Softmax 函数被广泛应用于多类分类任务中，例如多项逻辑回归（又称 softmax 回归）\(^\text{[2]: 206–209 [6]}\)、多类线性判别分析、朴素贝叶斯分类器以及人工神经网络中的输出层\(^\text{[7]}\)。具体来说，在多项逻辑回归和线性判别分析中，Softmax 函数的输入是由 $K$ 个不同线性函数的输出组成，其对某个样本向量 $\mathbf{x}$ 和权重向量 $\mathbf{w}$ 给出的第 $j$ 类的预测概率为：
$$
P(y = j \mid \mathbf{x}) = \frac{e^{\mathbf{x}^\mathsf{T} \mathbf{w}_j}}{\sum_{k=1}^{K} e^{\mathbf{x}^\mathsf{T} \mathbf{w}_k}}~
$$
这个表达式可以看作是以下两个步骤的组合：应用 $K$ 个线性函数：$\mathbf{x} \mapsto \mathbf{x}^\mathsf{T} \mathbf{w}_1, \dotsc, \mathbf{x} \mapsto \mathbf{x}^\mathsf{T} \mathbf{w}_K$（其中 $\mathbf{x}^\mathsf{T} \mathbf{w}$ 表示 $\mathbf{x}$ 和 $\mathbf{w}$ 的内积）；将上述结果输入到 Softmax 函数中。这个过程等价于将一个线性算子（由权重向量 $\mathbf{w}$ 定义）应用于输入向量 $\mathbf{x}$，从而将原始的（可能是高维的）输入转换为一个位于 $\mathbb{R}^K$ 空间中的向量。这个向量可以理解为输入属于每个类别的“激活度”，再通过 Softmax 转换为概率分布。
\subsubsection{神经网络}
标准的 softmax 函数常被用作基于神经网络的分类器的最后一层激活函数。在这种设置下，网络通常在对数损失函数或交叉熵框架下进行训练，从而形成一种非线性的多项逻辑回归模型。

由于 softmax 函数的输出依赖于输入向量和特定索引 $i$，因此在计算导数时需要考虑该索引的影响：
$$
\frac{\partial}{\partial q_k} \sigma(\mathbf{q}, i) = \sigma(\mathbf{q}, i) \left( \delta_{ik} - \sigma(\mathbf{q}, k) \right)~
$$
这个表达式在索引 $i$ 和 $k$ 上是对称的，因此也可以等价地写为：
$$
\frac{\partial}{\partial q_k} \sigma(\mathbf{q}, i) = \sigma(\mathbf{q}, k) \left( \delta_{ik} - \sigma(\mathbf{q}, i) \right)~
$$
其中，$\delta_{ik}$ 是克罗内克$\delta$符号，用于简化表达式（类似于 sigmoid 函数导数通过函数自身表示的方式）。

这些导数形式在反向传播中非常重要，用于计算 softmax 层中各个神经元的梯度。

为确保数值计算的稳定性，通常会从输入向量中减去其最大值。这种做法在理论上不会改变 Softmax 的输出或导数，但在实际计算中可以显著提升稳定性，因为它有效控制了所计算指数项的最大值，避免了指数溢出的问题。

如果 Softmax 函数中引入了缩放参数 $\beta$，则上述导数表达式需要乘以该参数 $\beta$。

关于使用 Softmax 激活函数的概率模型，可参考多项逻辑模型。
\subsubsection{强化学习}
在强化学习领域，Softmax 函数可用于将动作值转换为各动作的概率。常用的形式如下：\(^\text{[8]}\)
$$
P_t(a) = \frac{\exp(q_t(a) / \tau)}{\sum_{i=1}^{n} \exp(q_t(i) / \tau)}~
$$
其中：$q_t(a)$ 表示在时间 $t$ 选择动作 $a$ 的期望奖励；$\tau$ 被称为温度参数，借用了统计力学中的术语。当温度较高（即 $\tau \to \infty$）时，所有动作的概率几乎相同；而温度越低，动作的期望奖励对概率的影响越大。当温度趋近于零（即 $\tau \to 0^+$）时，具有最高期望奖励的动作的概率趋近于 1，其他动作的概率趋近于 0。此机制常用于在探索（高温）与利用（低温）之间做平衡。
\subsection{计算复杂度与解决方案}
在神经网络应用中，可能结果的数量 $K$ 通常非常大，例如在神经语言模型中，需要从一个可能包含数百万词汇的词表中预测最可能的输出单词。\(^\text{[9]}\)这会使 softmax 层的计算（即先进行矩阵乘法以得到各个 $z_i$，再应用 softmax 函数本身）变得非常昂贵。\(^\text{[9][10]}\)更进一步地，在使用梯度下降的反向传播方法训练此类神经网络时，每一个训练样本都需要计算一次 softmax，而训练样本的数量往往也很庞大。因此，Softmax 的计算开销成为了构建更大规模神经语言模型的主要限制因素，这推动了各种用于缩短训练时间的解决方案的发展。\(^\text{[9][10]}\)

重构 Softmax 层以提高计算效率的方法包括：层次化 softmax和 分化 softmax。[9]其中，层次化 softmax（由 Morin 和 Bengio 于 2005 年提出）采用二叉树结构，将所有可能输出（如词汇表中的单词）作为**叶节点**，而**中间节点**则是适当选择的一些“类别”，作为**潜在变量（latent variables）**。[10][11]

对于某个叶节点（即输出结果）的 softmax 概率，可以通过**从根节点到该叶节点路径上所有节点概率的乘积**来计算。[10]在理想情况下，如果该树是**平衡树**，其计算复杂度可由$O(K)$降低至$O(\log_2 K)$。[11]  实际效果则依赖于**如何将输出结果合理地聚类成类别**。[10][11]  
例如，Google 于 2013 年推出的 **word2vec 模型** 中就使用了 **Huffman 树** 来实现这一结构，以提高可扩展性。[9]

---

第二类解决方案是在训练过程中，**通过改写损失函数来近似计算 softmax**，从而避免完整归一化因子的计算。[9]  
此类方法包括仅在一个**输出样本子集**上进行归一化的策略，例如：  
- **重要性采样（Importance Sampling）**  
- **目标采样（Target Sampling）**。[9][10]  

这些方法有效地减少了计算开销，尤其适用于处理具有超大类别数的输出空间。
$$
